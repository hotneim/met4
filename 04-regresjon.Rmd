
# Regresjon

I forrige modul fokuserte vi på *binære* spørsmål av typen "Er det en forskjell mellom disse to populasjonene, eller ikke?", "Er disse kjennetegnene uavhengige, eller ikke?", og så videre. I denne modulen skal vi prøve å gå et steg lenger og tillate mer interessante spørsmål. I stedet for bare å spørre om en eller annen effekt er til stede (eller ikke), så vil vi heller finne ut hvor stor denne effekten er, hvilken retning den går, og kanskje om vi kan bruke kunnskapen vi får om statistiske sammenhenger til å si noe fornuftig om hva som vil skje for noe som vi enda ikke har observert. Da er det regresjon som gjelder, og mer spesifikt for vår del: *lineær regresjon*. 

Regresjon er et hovedtema i MET4. Vi innfører en *statistisk modell* som i sin enkleste form sier at en *forklaringsvariabel* $X$ henger sammen med en responsvariabel $Y$ på en helt bestemt måte, nemlig gjennom ligningen

$$Y = \beta_0 + \beta_1 X + \epsilon.$$

Ligningen over sier at det er en *lineær* sammenheng mellom $X$ og $Y$, men at det i tillegg kommer en uforutsigbar støyvariabel $\epsilon$ som gjør at vi ikke vil kunne observere den lineære sammenhengen direkte. Det vi derimot kan gjøre, er å bruke de observerte $X$er og $Y$er til å finne ut hvilke verdier av $\beta_0$ og $\beta_1$ som passer *best*. Til det bruker vi minste kvadraters metode, som beskrevet i videoforelesningene i denne modulen.

Vi deler arbeidet med regresjon inn i tre deler. I den første (og største) delen går vi grundig gjennom ulike sider vi den enkle lineære regresjonsmodellen over. I den andre delen ser vi på *multippel regresjon* som er en utvidelse av enkel regresjon der vi tillater flere forklaringsvariabler på høyre side av likhetstegnet, og i den tredje delen ser vi på ulike praktiske aspekter ved regresjonsmodellering og modellbygging.

I videoforelesningene går vi gjennom noen slides, og vi skriver et R-skript. Du kan laste disse ned ved å klikke på lenkene under:

[Slides til "Regresjon"](script-slides/regresjon/regresjon-slides.html)

[R-script til "Regresjon"](script-slides/regresjon/regresjon-script.R)

**TIPS:** Hvis du ønsker å laste ned lysbildene som PDF trykker du på linken over, velger "Skriv ut", og så skriver du ut som PDF. Før du gjør det bør du scrolle gjennom alle sidene slik at ligningene vises korrekt.

## Enkel regresjon

### Videoforelesninger

<div style='padding:75% 0 0 0;position:relative;'><iframe src='https://vimeo.com/showcase/7781447/embed' allowfullscreen frameborder='0' style='position:absolute;top:0;left:0;width:100%;height:100%;'></iframe></div>

### Kommentarer

Vi har sett på en del figurer som illustrerer noen pedagogiske poenger, og lærebokens kapittel **16** går detaljert til verks når de beskriver de ulike læringsmomentene:

I kapittel **16.1** kan vi lese mer om den statistiske modellen som vi kaller enkel regresjon. I kapittel **16.2** introduseres minste kvadraters metode for å estimere regresjonskoeffisientene ved hjelp av data. De viser til og med hvordan det kan gjøres manuelt ved hjelp av bildatasettet, men det er selvsagt kun for å illustrere hvodan formlene ser ut. Vi estimerer ved hjelp av R, og vi har sett i videoforelesningen hvordan vi gjør det ved hjelp av `lm()`-funksjonen.

Det som gjør regresjon til et *statistisk* problem er feilleddet $\epsilon$. Vi tenker oss at for en gitt verdi av $X$, så vil «naturen» regne ut verdien av $Y$ ved å regne ut den lineære sammenhengen $Y = \beta_0 + \beta_1 X$, og så legge til støyvariabelen $\epsilon$ som *trekkes* fra en sannsynlighetsfordeling. Vi kan ikke observere direkte hvilke $\epsilon$ som «naturen» har «trukket» (for da ville vi med en gang kunne regnet oss frem til verdiene av $\beta_0$ og $\beta_1$). For gitte estimater av regresjonskoeffisientene $\widehat \beta_0$ og $\widehat \beta_1$ (som vi kan finne f.eks. ved hjelp av minste kvadraters metode), så kan vi regne ut de *observerte residualene* 

$$\widehat\epsilon_i = Y_i - \widehat Y_i = Y_i - (\widehat \beta_0 + \widehat \beta_1 X_i).$$

Ved å analysere residualene kan vi si mer om f.eks

1. Er det egentlig en lineær sammenheng mellom $X$ og $Y$? Hvis det er mønstre og sammenhenger i de observerte residualene, tyder det på at den enkle lineære modellen *ikke* fanger opp hele sammenhengen mellom $X$ og $Y$.
2. Vi kan gå mer spesifikt til verks: nøyaktig *hvilke* antakelser om residualene er ser ut til å være brutt? I senere økonometrikurs vil dere kunne lære mer om hvordan vi håndterer de ulike problemene. 
3. Hvor stor er variansen til $\epsilon$? Det brukes videre til å sette opp den viktige signifikanstesten for om stigningstallet i regresjonen er forskjellig fra null.

Alt dette behandles grudig i bokens kapittel **16.3--16.6**. Her bør teksten leses godt. Kode til bileksempelet finnes i scriptet som følger med videoforelesningene.

Når det gjelder enkel regresjon kan du sjekke om du har fått med deg det vesentligste ved å diskutere følgende spørsmål:

1. Hva er responsvariabelen og hva er forklaringsvariabelen i enkel regresjon?
1. Hva er fortolkningen av de to regresjonskoeffisientene?
1. Hvilket prinsipp er det vi legger til grunn når vi skal bestemme (estimere) verdien av koeffisientene ved hjelp av data?
1. Skriv opp formlene for koeffisientestimatene. Kan du gi en intuitiv fortolkning av disse? Er de rimelige? 
1. Kan du ved hjelp av formelen for $\widehat\beta_1$ utlede sammenhengen mellom *stigningstallet* $\beta_1$ og korrelasjonskoeffisienten* mellom $X$ og $Y$?
1. Hvilken rolle spiller feilleddet ($\epsilon$)?
1. Skriv opp de 4 + 1 forutsetningene. Når må den siste være oppfylt? Når kan vi klare oss uten?
1. Hva er testobservatoren når vi tester H$_0: \beta_1 = 0$?
1. Kan du holde styr på de fire standardavvikene vi har jobbet med i denne forelesningen?
1. Hva mener vi med å diagnostisere en regresjonsmodell?
1. Hva er $R^2$, og hva måler den?
1. Hva sier $R^2$ *ikke* noe om?

Her er noen grunnleggende ferdigheter fra kapittel 16. Klarer du dette? 

1. Bruke \texttt{R} til å tilpasse en enkel regresjonsmodell for et datasett?
1. Bruke \texttt{R} til å skrive ut oversiktlige regresjonstabeller?
1. Tolke en regresjonsutskrift?
1. Hente ut relevant informasjon etter en slik tilpasning?
1. Bruke informasjon fra regresjonsutskriften til å regne ut antall stjerner for hånd?
1. Lage diagnoseplott i \texttt{R}?
1. Diagnistisere en modell?
1. Identifisere innflytelsesrike observasjoner?

## Multippel regresjon

### Videoforelesninger

<div style='padding:75.14% 0 0 0;position:relative;'><iframe src='https://vimeo.com/showcase/7793207/embed' allowfullscreen frameborder='0' style='position:absolute;top:0;left:0;width:100%;height:100%;'></iframe></div>

### Kommentarer

I kapittel **17** utvides regresjonsbegrepet til *multippel* regresjon, som i prasis betyr at vi kan ha flere enn en forklaringsvariable:

$$Y = \beta_0 + \beta_1X_1 + \cdots \beta_kX_k + \epsilon,$$
men utover dette er alle detaljene vi har snakket om de samme. For eksempel:

- Tolkningen av regresjonskoeffisienten: En endring på en enhet i forklaringsvariabelen $X_j$ henger sammen med $\beta_j$ enhets endring i responsvariabelen $Y$ (merk at jeg ikke brukker begrepet "fører til", vi kan ikke uten videre fortolke sammenhengen som kausal!).
- Analysen av residualene $\widehat \epsilon_i = Y_i - \widehat Y_i$ er den samme og har samme formål 1--3 som over.
- $R^2$ har samme fortolkning.
- R-kommandoen er den samme, vi bare sette pluss mellom forklaringsvariablene, f.eks `reg <- lm(Y ~ X1 + ... + Xk, data = x)`

I tillegg innfører vi noen nye begreper:

**Justert** $R^2$: Vi viste i forelesningen at vi vil alltid klare å øke $R^2$ ved å legge til forklaringsvariable, selv om de ikke har noe med problemet å gjøre. Derfor innførte vi en *justert* $R^2$ som tar høyde for nettopp dette, ved å bli større bare dersom den aktuelle forklaringsvariebelen faktisk forklarer en reell mengde av variasjonen i responsvariabelen. Se avsnitt **17-2f** i læreboken.

**Multikolinearitet**: Dersom en forklaringsvariabel er sterkt korrelert med en eller flere andre forklaringsvariabler har vi multikolinearitet. Det blir naturlig nok et problem å skille effekter fra hverandre når de i realiteten er helt eller nesten like. Ekstremtilfellet er *perfekt multikolinearitet* der en variabel er en eksakt lineær funksjon av en eller flere andre variable. Det typiske tilfellet er at vi har to kolonner der vi måler det samme fenomenet, men med to ulike enheter, f.eks. **cm** og **m**. Selvsagt kan vi ikke klare å identifisere en *separat* og *uavhengig* effekt av $X$ på  $Y$ om vi skifter måleenhet, og vi vil få en feilmelding dersom vi prøver på det. Det er ekvivalent med å dele på null (every time you divide by zero, God kills a kitten!). Løsning: fjern en av kolonnene fra regresjonsanalysen.

Verre er det om to variable måler *nesten* det samme, men ikke helt, som i skoledataeksempelet der vi kunne bruke både innbyggertall og antall femteklassinger i kommunen som forklaringsvariabler. De henger tett sammen, men selvsagt ikke eksakt, og det virker rart å kunne knytte separate efekter til disse to variablene. I dette tilfellet får vi likevel ikke feilmeldinger, men konsekvensen kan fort bli at standardavvikene (usikkerheten!) til koeffisientestimatene eksploderer, og at ingen av variablene blir signifikant forskjellige fra null, selv det det faktisk er en sterk sammenheng mellom kommunestørrelse og prøveresultat (husk at testobservatoren: $t = \widehat \beta_k/\sigma_{\beta_k}$ blir liten når nevneren blir stor).

**F-test for multiple sammenligninger**: Dette henger nøye sammen med *variansanalyse (analysis of variance, ANOVA)*, som nå er tatt ut av pensum i kurset. For å forstå dette kan vi sette opp et eksempel, med to forklaringsvariabler:
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2.$$
Etter å ha brukt miste kvadraters metode for å estimere de tre koeffisientene er vi kanskje interessert i å vurdere den statsistiske signifikansene til de to stigningstallene separat. Da tester vi de to nullhypotesene $\beta_1 = 0$ og $\beta_2 = 0$, som vi i praksis gjør ved å se på hvor mange stjerner de får i regresjonsutskriften. Men sett at ingen av koeffisientene er signifikant forskjellige fra null, kan vi da slutte at vi ikke kan forkaste hypotesen $\beta_1 = \beta_2 = 0$, dvs at *begge* koeffisientene er lik null, og at ingen av forklaringsvariablene forklarer variasjon i $Y$? **NEI**, det kan vi ikke. Vi kan for eksempel lett tenke oss at vi på grunn av multikolinearitet ikke får separate forkastninger av de to nullhypotesene, men at ved å fjerne en variabel, så blir den andre signifikant.

For å virkelig forstå dette problemet kan du godt lese starten på kapittel **14.1** samt kapittel **14.2** om multiple sammenligninger (som strengt tatt ikke er pensum), men essensen er altså:

$$\textrm{Å forkaste H}_0: \beta_1 = 0 \textrm{ og H}_0: \beta_2 = 0 \textrm{ er ikke det samme som å forkaste H}_0: \beta_1 = \beta_2 = 0!$$

For å gjennomføre den siste testen må vi sette opp en egen testobservator, som viser seg å være $F$-fordelt. Læreboken lister opp noen detaljer i avsnitt **17-2f**, og essensen er at vi setter opp en brøk på formen
$$F = \frac{\textrm{Variasjon i } Y \textrm{ som fanges opp av regresjonsmodellen med } X_1 \textrm{ og }X_2}{\textrm{Variasjon i } Y \textrm{ som fanges opp av regresjonsmodellen uten } X_1 \textrm{ og }X_2}.$$
Dersom denne brøken viser seg å være stor (som definert av signifikansnivå og frihetsgrader, se lærebok), forkaster vi nullhypotesen om at begge koeffisientene begge kan være lik null. I en generell multippel regresjon med $k$ forklaringsvariable rapporterer R `F-statistic: ` etc, med verdien av $F$-observatoren i testen for
$$H_0: \beta_1 = \cdots = \beta_k = 0,$$
og dersom den oppgitte $p$-verdien er mindre enn f. eks. 5%, kan vi slutte at ikke alle koeffisientene kan være null samtidig (selv om ingen av koeffisientene i seg selv nødvendigvis er signifikant forskjellig fra null).

Som en såkalt *fun fact* kan vi nevne at det er enkelt å teste for signifikansen til *grupper* av variable på denne måten, f.eks hvis det er noen variable som måler lignende ting (si $X_2, X_4$ og $X_5$). I R kan du estimere to modeller, en modell som *inkluderer* variablene (f.eks. `reg_stor`) og en modell der du tar bort de aktuelle variablene (f.eks. `reg_liten`). Du kan da kjøre kommandoen `anova(reg_stor, reg_liten)` for å teste
$$H_0: \beta_2 = \beta_4 = \beta_5 = 0.$$


> **Kritikk av læreboken:** Læreboken har en tabell på s. 701 som viser sammenhengen mellom ulike statistiske størrelser som vi kan regne ut for en regresjonsmodell. $R^2$ kjenner vi som forklaringsgraden, $s_{\epsilon}$ er standardavviket til residualene, $F$ er testobservatoren for modellgyldighet som vi definerte uformelt over, og som er definert formelt nederst på s. 700, mens SSE (Sum of Squares Error) henger nøye sammen med standardavviket, som vi også kan se på s. 700. På disse sidene ser vi mange ligninger som viser hvordan disse størrelsene formelt henger sammen, og i tabellen på s. 701 ser vi blant annet at dersom SSE er liten, er også $s_{\epsilon}$ liten, $R^2$ er nær null, og $F$-observatoren er stor. Det er greit nok, **men** de har en ekstra kolonne som slår fast at regresjonsmodellen er *good*. 

 > Her menes det **ikke** at regresjonsmodellen er *god* i den forstand at vi skal reagere med glede eller lettelse (slik noen gjerne gjør), men at variasjonen i datamaterialet i stor grad lar seg forklare av modellen vår. I et tenkt eksempel der den sanne sammenhengen mellom $Y$ og $X$ er gitt ved $Y = \beta_0 + \beta_1X + \epsilon$, men der $\beta_1$ er forholdsvis liten og $s_{\epsilon}$ er relativt stor, vil f.eks. $R^2$ bli *liten*, selv om den enkle lineære regresjonsmodellen repsesenterer sannheten og av alle tenkende mennesker må sies å være *god*. 

> Det er desverre mange lærebøker som blander disse to fortolkningene, ikke gjør det!


Her er enda noen grunnleggende begreper. Har du fått med deg dette?

1. Hva mener vi med at en observasjon er innflytelsesrik?
1. Hva er grunnen til at vi trenger *justert* $R^2$ med flere forklaringsvariable?
1. Hva er forskjellen på *perfekt* og *tilnærmet* multikolinearitet i lineær regresjon? Hva blir konsekvensen i hvert av tilfellene?
1. Kan du gi en praktisk og intuitiv forklaring på hvorfor multikolinearitet nødvendigvis må være et problem?
1. Hva er forskjellen på statistisk og økonomisk signifikans? Kan du sette opp konkrete eksempler der vi kan estimere statistisk signifikante, men ikke økonomisk signifikante effekter i multippel regresjon? Hva med den motsatte situasjonen, økonomisk signifikant, men ikke statistisk signifikant? 

Grunnleggende ferdigheter: Klarer du dette?

1. Bruke R til å tilpasse en multippel regresjonsmodell for et datasett?
1. Bruke R til å finne særlig innflytelsesrike observasjoner?
1. Tolke en multippel regresjonsutskrift?

## Modellbygging

### Videoforelesninger

<div style='padding:75.21% 0 0 0;position:relative;'><iframe src='https://vimeo.com/showcase/7793253/embed' allowfullscreen frameborder='0' style='position:absolute;top:0;left:0;width:100%;height:100%;'></iframe></div>

### Kommentarer

Kapittel **18** dekker de grunnleggende begrepene innen modellbygging. I kap. **18.1** snakkes det om polynomiske modeller, i kap. **18.2** behandles dummyvariabler. Kapittel **18.3** og **18.4** handler om hvordan vi i praksis kan jobbe for å velge ut variable i en gitt situasjon. Forelesningene dekker i grunn greit det vi skal få med oss her. Som en sjekk om du har fått med deg det vesentlige, kan du svare på følgende spørsmål:

1. Vi har lært tre typer log-transformasjoner. Hva blir fortolkningen av koeffisientene for hver av disse?
1. Kan du nevne tre gode grunner til at log-transformasjoner er nyttige?
1. Hvorfor sier vi at følgende modell er lineær? $Y = \beta_0 + \beta_1X + \beta_1X^2 + \varepsilon$
1. Vil vi ikke få problemer med multikolinearitet i modellen over?
1. Nevn en veldig god grunn til at vi må være ytterst forsiktig med polynomtransformasjoner.
1. Hva er en dummyvariabel? 
1. Hva er fortolkningen av regresjonskoeffisienten til en dummyvariabel?
1. Hva er fortolkningen av regresjonskoeffisienten til et interaksjonsledd mellom målevariabelen $X$ og dummyvariabelen $D$?
1. Et utrolig viktig poeng, men bruk tid til å tenke over og formulere et svar: Hvorfor er det viktig å tenke på *multippel testing* i sammenheng med *variabelutvelgelse*?

Grunnleggende ferdigheter: Klarer du dette?

1. Bruke logtransformasjoner i R?
1. Bruke poynomtransformasjner i R?
1. Sette opp en fornuftig regresjonsmodell ved å ta utgangspunkt i et datasett og et analyseformål, og argumentere godt for dine valg? **Denne ferdigheten har blitt testet på hver eneste hjemmeeksamen i manns minne!**

## Oppgaver

### Regresjon med en forklaringsvariabel

#### Oppgave 1 {-}

Forskere har brukt statistikk til å undersøke om TV-titting er forbundet med overvekt. De har samlet inn data fra 15 10-åringer om antall timer TV-titting per uke og antall kilo overvekt hos barnet (rapportert som differanse fra normalvekt). De innsamlede dataene er oppsummert i tabellen:
```{r, echo=FALSE}
df_tv <- data.frame(
  TV_titting = c(42, 35, 28, 34, 37, 38, 32, 33, 18, 28, 36, 29, 29, 34, 18),
  Overvekt = c(17,  5, -1,  0, 13, 15,  5,  7, -7,  7,  6,  7,  4, 15, -5)
)
knitr::kable(
  df_tv, align = "c", format = "html"
)
```

a. Bruk R til å lage et spredningsplott av resultatene. Hva tror du om forholdet mellom de to variablene utfra figuren?

b. Sett opp regresjonsuttrykket for å undersøke om overvekt er forbundet med TV-titting. La overvekt være responsvariabelen. Hva er betydningen av hver parameter i uttrykket?

c. Bruk R til å estimere parameterne. Hva kan koeffisientene fortelle deg om forholdet mellom overvekt og TV-titting?

d. Beregn et 95 % konfidensintervall for $\beta_1$. Hint:  Bruk 'summary()' på regresjonsmodellen for å finne $S(\hat{\beta}_1)$ som da gitt i kolonnen "Std. Error".

e. Bruk R til å beregne et 95 % prediksjonsintervall for overvekt i kilo for et barn som ser på TV 30 timer i uken. Hva forteller intervallet deg?

f. Bruk R til å beregne et 95 % konfidensintervall for gjennomsnittlig overvekt for barn som ser 30 timer på TV i uken. Hvordan er tolkningen av dette intervallet forskjellig fra det i oppgave d?

g. Hva er forventet overvekt for barn som ser 0 timer på TV i uken utfra modellen? Hva kan være problematisk ved å gjøre denne type analyser av en regresjonsmodell?

<details> <summary>Løsning</summary><div>

a)

```{r}
df_tv <- data.frame(
  TV_titting = c(42, 35, 28, 34, 37, 38, 32, 33, 18, 28, 36, 29, 29, 34, 18),
  Overvekt = c(17,  5, -1,  0, 13, 15,  5,  7, -7,  7,  6,  7,  4, 15, -5))
  
plot(df_tv$TV_titting, df_tv$Overvekt, type = "p", xlab="TV-titting", ylab="Overvekt")
```

>Plottet viser en ganske tydelig trend om at TV-titting og overvekt er relaterte. Dette kan vi undersøke nærmere.

b)

>Vi setter overvekt som responsvariabel og TV-titting som forklaringsvariabel. Uttrykket blir da:

>\begin{equation}
>   \text{Overvekt} = \beta_0 + \beta_1 \text{TV-titting} + \epsilon
\end{equation}

>- Ser vi bort fra dataene ville $\beta_0$ være forventet overvekt for personer som ikke ser på tv. Av spredningsplottet ser det derimot ut som at vi ikke har data ved 0 TV-titting og det er derfor ikke fornuftig med en direkte tolkning av denne parameteren.
>
>- Under antagelsen om at regresjonsmodellen er gyldig forteller $\beta_1$ hvor mye vi forventer at overvekten øker dersom man øker TV-tittingen med 1 time.

c)

>Vi estimerer en regresjonsmodell fra dataene og skriver ut tabellen med resultatene:

```{r, message = F}

reg <- lm(Overvekt ~ TV_titting, df_tv)
summary(reg)

# For en finere utskrift kan du bruke følgende:
# library(stargazer)
# stargazer(reg, type="text")
```

>Det er tydelig sigifikans for at hver av parameterne er ulik null. For $\beta_1$ betyr dette at trenden vi observerte i spredningplottet var signifikant. Videre kan vi tolke det positive fortegnet til $\beta_1$ som at flere timer TV-titting er relatert med økt overvekt.

>Merk at vi *ikke* kan si utfra dataene at mer TV-titting *fører til* overvekt. En mer riktig tolkning er å si at de forekommer samtidig i populasjonen. (Dersom vi ønsket å finne ut av kausaliteten måtte man på et tilfeldig utvalg av barn satt noen til å se mye på TV og noen til å se mindre på TV, og deretter undersøkt om dette resulterte i statistisk signifikant høyere overvekt hos en av gruppene. Dette krysser imidlertid noen etiske grenser om å påføre barn overvekt, dersom hypotesen er sann.)

>d)
> 
> Fra R utskriften ser vi at $S(\hat{\beta}_1)=0.1602$. Videre er $t_{0.025, 13} = 2.16$. Et 95 % konfidensintervall for $\beta_1$ er da gitt ved:

$$\left[\hat{\beta}_1 - t_{\alpha/2, n - 2}S(\hat{\beta}_1),  \hat{\beta}_1 + t_{\alpha/2, n - 2}S(\hat{\beta}_1)\right]\\
=\left[0.8942 - 2.16\times0.1602,  0.8942 + 2.16\times0.1602\right]\\
=\left[0.5481, 1.2402\right]$$
e)

```{r, message = FALSE}

reg.pred <- predict(reg, newdata = data.frame(TV_titting = c(30)), interval = "predict")
reg.pred
# stargazer(reg.pred, type="text")
```

>Dersom vi trekker et nytt barn som ser 30 timer på TV per uke, vil barnet i 95 % av tilfellene være innenfor prediksjonsintervallet dersom vi hadde gjentatt dette mange ganger.

f)

```{r, message = F}
reg.conf <- predict(reg, newdata = data.frame(TV_titting = c(30)), interval = "confidence")
reg.conf

#evt 
# library(stargazer)
# stargazer(reg.conf, type="text")
```

>Konfidensintervallet viser hvor *gjennomsnittet* av målinger på *et nytt sett* med barn som ser 30 timer på TV per uke vil ligge i 95 % av nye eksperimenter.

g)

```{r}
reg.zero <- predict(reg, newdata = data.frame(TV_titting = c(0)), interval = "prediction")
reg.zero
# evt
# library(stargazer) 
# stargazer(reg.zero, type="text")
```

>Beste gjetning er at barn som ser 0 timer på TV i uken er 22.2 kg under normalvekt (!). Et kjapt søk viser at barn på 10 år veier mellom 25.5 og 39 kg. Det høres urimelig ut at veldig lite TV-titting har sammenheng med ekstrem underernæring blant den samme populasjonen av barn som ser rundt 30 timer på TV i uken.
>
>Det er flere ting som kan gå galt når man gjør en slik analyse av en modell:
>
>- Modellen kan være gal. Vi antar et lineær forhold mellom TV-titting og overvekt, noe som ikke trenger å være riktig ved 0 timer TV-titting.
>
>- Datasettet dekker ikke den gruppen barn som ser 0 timer på TV, så med mindre modellen er helt riktig (noe den sjelden er) vil den ikke kunne generalisere så langt utenfor området vi estimerte parameterne på.
>
- Med mindre forholdet mellom TV-titting og overvekt er kausalt, kan det være at sammenhengen som er observert mellom dem i dataene ikke vil være det samme langt utenfor det aktuelle data-området.

</div></details>

#### Oppgave 2 {-}
Vi skal undersøke om alder har sammenheng med hvor lang tid man bruker på et puslespill. Vi har data fra et tilfeldig utvalg av 210 voksne personer om alder og tid brukt på oppgaven. Uttrykk alder som $X$ og tid i minutter som $Y$. Følgende deskriptive statistikker er beregnet for datasettet: $s_{xy}= 8,	s_x^2, = 110,	s_y^2 = 42, \bar{x} = 40,	\bar{y} = 20$ hvor $\bar{x},\bar{y}$ er gjennomsnittene.

a. Sett opp regresjonsuttrykket for sammenhengen mellom $X$, $Y$ og støy $\epsilon$. Hva er antagelsen for sammenhengen mellom X og $\epsilon$?

b. Hva er uttrykket for beste gjetning/prediksjon $\hat{Y}_i$ gitt en verdi av forklaringsvariabelen $X_i$ for regresjonsmodellen du har satt opp? Hva er uttrykket for residualene?

c. Vis at hva uttrykket for regresjonsparameterne blir når de estimeres med minste kvadraters metode. Regn ut minste kvadraters estimat av parameterne.

<!-- d. Gi en tolkning av koeffisientene.

e. Er det nok evidens til at vi kan hevde at det er en sammenheng mellom alder og tid? Bruk 95 % signifikansnivå. 

f. Estimer et 90 % konfidensintervall av gjennomsnittlig tid brukt på oppgaven for en 50-åring. -->

<details><summary>Løsning</summary><div>

a)
Regresjonsuttrykket er
\begin{equation}
  Y = \beta_0 + \beta_1 X + \epsilon
\end{equation}

Vi antar at forklaringsvariabelen $X$ er uavhenging støyen $\epsilon$. Dermed vil også kovariansen være null,
\begin{equation}
  \text{Cov}(X, \epsilon) = 0.
\end{equation}

b)

Beste gjetning er forvetningen betinget på utfallet av forklaringsvariabelen $X$:
\begin{align}
  \hat{Y}_i &= E[Y|X = X_i] = E[\beta_0 + \beta_1 X + \epsilon|X = X_i] \\
  &= \beta_0 + \beta_1 X_i + E[\epsilon|X = X_i] \\
  &= \beta_0 + \beta_1 X_i + E[\epsilon] \\
  &= \beta_0 + \beta_1 X_i
\end{align}

Residualene er forskjellen mellom data og prediksjon 

\begin{equation}
  r_i = \hat{Y}_i - Y_i
\end{equation}

c)

Minste kvadraters metode minimerer summen av residualene kvadrert, så 
\begin{align}
  SSE &= \sum_{i} r_i^2 \\
  &= \sum_{i} (\beta_0 + \beta_1 X_i - Y_i)^2.
\end{align}

Minimér ved å sette den deriverte for hver parameter til null:
\begin{align}
  \frac{dSSE}{d\beta_0} &= \sum_{i} 2 (\beta_0 + \beta_1 X_i - Y_i) \overset{!}{=} 0 \\
  \implies \quad \beta_0 &+ \beta_1 \frac{\sum_{i} X_i}{N} = \frac{\sum_{i} Y_i}{N} \\
  \implies \quad \beta_0 &= \frac{\sum_{i} Y_i}{N} -  \beta_1 \frac{\sum_{i} X_i}{N},
\end{align}
og
\begin{align}
  \frac{dSSE}{d\beta_1} &= \sum_{i} 2 (\beta_0 + \beta_1 X_i - Y_i)X_i \\
  &= 2 \beta_0 \sum_{i} X_i + 2 \beta_1 \sum_{i} X_i^2 - 2\sum_{i} X_i Y_i \overset{!}{=} 0 \\
  &\implies \quad \beta_0 \frac{\sum_{i} X_i}{N}  + \frac{\sum_{i} X_i^2}{N}  = \frac{\sum_{i} X_i Y_i}{N} .
\end{align}

Satt inn for $\beta_0$ blir det

\begin{align}
  \left ( \frac{\sum_{i} Y_i}{N} -  \beta_1 \frac{\sum_{i} X_i}{N} \right ) \frac{\sum_{i} X_i}{N}  + \beta_1\frac{\sum_{i} X_i^2}{N}  &= \frac{\sum_{i} X_i Y_i}{N} \\
  \beta_1 \left ( \frac{\sum_{i} X_i^2}{N} -  \left ( \frac{\sum_{i} X_i}{N} \right )^2 \right ) &= \frac{\sum_{i} X_i Y_i}{N} - \frac{\sum_{i} X_i}{N}\frac{\sum_{i} Y_i}{N} \\
  \beta_1 S_X^2 &= S_{XY}.
\end{align}

Da blir 
```{r}
beta_1 <- 8 / 110
beta_1
```
og 
```{r}
beta_0 <- 20 - beta_1 * 40
beta_0
```

</div></details>


<!-- #### (Oppgave 1) {-}

Begrepet regresjon ble for første gang brukt i 1885 av Sir Francis Galton i analysen av forholdet mellom høyden til barn og deres foreldre. Han formulerte da ‘loven om universell regresjon’, som sier at “each peculiarity in a man is shared by his kinsmen, but on average in a less degree”. Dette kan oversettes til noe slikt som at en ekstrem egenskap hos en person er felles hos slektningen, men vanligvis av mindre ekstrem grad. Tenk deg en uvanlig høy far. Sønnen hans vil som oftest være høy han også, men ikke like høy. På samme måte kan vi tenke oss en uvanlig høy sønn, hvor faren sannsynligvis også er høy, men nærmere gjennomsnittet enn sønnen. Dette er betydningen av «regression towards the mean». 
I 1903 gjorde to statistikere, K. Pearson og A. Lee et tilfeldig utvalg av 1708 far-sønn par for å undersøke Galtons lov. Regresjonsuttrykket ble beregnet til å være følgende:

$$(\text{Sønns høyde}) = 33.73 + 0.516 (\text{Fars høyde})$$
(Pearson og Lee er briter, og har målt høyden i tommer.)

a. Hva er betydningen av koeffisientene? 
b. Hva forteller regresjonsuttrykket om høyden til sønner av høye fedre?
c. Hva forteller regresjonsuttrykket om høyden til sønner av lave fedre?
Husk: koeffisientene tilsvarer tallverdiene i regresjonsuttrykket.  

<details>
<summary>Løsning</summary>

a. Helningskoeffisienten forteller oss at for hver ekstra tomme av far høyde vil sønnens høyde gjennomsnittlig øke med 0.516 tommer. Konstantleddet gir her ingen spesiell innsikt, ettersom vi aldri vil få konstantleddet «alene» (det eksisterer ingen fedre som måler 0 tommer). 

b. Sønnen vil gjennomsnittlig være lavere enn sin far.

c. Sønnen vil gjennomsnittlig være høyere enn sin far. 

</details> -->

#### Oppgave 3 {-}

Figurene nedenfor viser residualene (feilleddene) vi får ut fra et par ulike modeller. Ser plottene ut til å tilfredsstille kravene for enkel regresjon? Hvis ikke, hva ser ut til å være galt for hver av figurene? Hvordan kan man håndtere brudd på betingelsene i de ulike situasjonene?

a. 

```{r resplott, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)

dfreg <- data.frame(
  sigma_lin = seq(100) / 60 + 0.5,
  sigma_sin = 2 * (sin(2 * 3.14 * seq(100) / 100 * 3) + 1.3)
)
dfreg$eps_norm <- rnorm(100)
dfreg$eps_lin <- rnorm(100, sd = dfreg$sigma_lin)
dfreg$eps_sin <- rnorm(100, sd = dfreg$sigma_sin)
eps_ar_base = rnorm(102)
dfreg$eps_ma2 = eps_ar_base[3:102] + 0.9 * eps_ar_base[2:101] + 0.8 * eps_ar_base[1:100]
```


```{r res1, echo = FALSE}
plot(dfreg$eps_norm, ylab = "Residualer", xlab = "Prediksjon")
```
b.
```{r res2, echo = FALSE}
plot(dfreg$eps_lin, ylab = "Residualer", xlab = "Prediksjon")
```
c. 
```{r res3, echo = FALSE}
plot(dfreg$eps_sin, ylab = "Residualer", xlab = "Datapunkter (sortert i tid)")
```
d.
```{r res4, echo = FALSE}
plot(dfreg$eps_ma2, ylab = "Residualer", xlab = "Datapunkter (sortert i tid)")
```




<details>
<summary>Løsning</summary><blockquote>

a. Her ser støyleddet ut som det har konstant varians og forventing lik null. Det betyr at antagelsene er oppfylt.

b. Støyen er symmetrisk om null, men kan se ut som den øker. Altså er det brudd på antagelsen om konstant varians. Økningen ser ut til å være jevn, og en log-transformasjon kunne i dette tilfellet vært til hjelp.

c. Her ser det ut som at vi har perioder med høy varians etterfulgt av perioder med lavere varians. Vi skal ikke se så mye nærmere på hvordan dette kan håndteres i MET4, men man kan komme over det i senere kurs.

d. Her ser det ut som at støyen har en relasjon i tid, og at høye/lave verdier henger sammen med høye/laver verdier. Det er ikke åpenbart at det er dette som foregår, og vi burde undersøkt det nærmere. Senere i kurset skal vi håndtere dette ved bruk av tidsrekkemodeller.

Merk at det ikke alltid er lett å se direkte fra figurene hva som er galt. Riktig bruk av statistiske tester og diagnoseplott kan hjelpe med å oppdage brudd på betingelsene. Figurene i denne oppgaven er generert ved simulering, vi kan derfor vite akkurat hva som er galt. For virkelige data vil kunne være vanskeligere å tolke fordi det ikke nødvendigvis er én ting som er galt om gangen. Man må også være på vakt for overtolkning av det som egentlig er tilfeldigheter.

</blockquote></details>

### Multippel regresjon og modellbygging

#### Oppgave 1 {-}

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(stargazer)
library(tidyverse)
```

Denne oppgaven skal gi et forhold til hvordan teorien funker i praksis. Vi skal simulere data fra konstruerte modeller gjøre analyser på dem. Fordelen med å begynne her er at man får er forhold til hvordan statistikk kan og bør tolkes utfra teorien. Med virkelige data vil antagelsene som ligger til grunn for modellene stort sett være brutt, i større eller mindre grad, og kjernen i god statistisk analyse er å vite hva man kan og ikke kan gjøre likevel. God kjennskap til hvordan det burde funke i teorien er en viktig byggestein for statistisk forståelse.

Vi generer opp data som skal brukes til å estimere en model. `rnorm` trekker tall fra en standard normalfordeling. `mutate()` legger til nye kolonner i dataframe-en basert på allerede eksisterende kolonner. `set.seed()` setter startpunktet for pseudotilfeldige tall, og om du setter samme seed får du de samme tilfeldige tallene som vi har brukt. 
```{r}
library(tidyverse)
set.seed(4)
df_mdl <- data.frame(
  x1 = rnorm(100, sd = 2),
  x2 = rnorm(100, sd = 2),
  eps = rnorm(100) 
) %>% mutate(
  y = 2 * x1 + (-1) * x2 + eps
)
```
a. Hva er de eksakte parameterne i modellen som passer til dataene vi har generert, og hva er fordelingen til hver av forklaringsvariablene og støy-leddet? Er betingelsen om uavhengig feilledd oppfylt?

b. Generér opp dataene selv og estimer parameterne basert på dataene. Tolk estimatene.

c. Lag 95 % konfidens- og prediksjonsintervaller for en prediksjon hvor $X_1=1, X_2=1$ basert på estimatene dine.

c. Hva er eksakt prediksjonsintervall basert på modellen vi har generert data fra? Ser dine prediksjonsintervaller rimelige ut utfra dette? Hvorfor er de ikke eksakt like?

d. Stemmer konfidensintervallet overens med modellen? Tolk konfidensintervallet basert på at vi kan generere opp nye data (med et annet seed).

e. Hva er P-verdien i F-test for at vi har en signifikant sammenheng mellom responsvariabelen og forklaringsvariablene?

<!-- f. Hvor få datapunkter måtte vi ha simulert opp før F-testen ikke kunne sagt at det er en sammenheng? -->

<details><summary>Løsning</summary><blockquote>

a. Fordelingen til forklaringsvariablene er \begin{align}
  X_1, X_2 \sim N(0, 2) 
\end{align} 
fordi vi har trukket dem fra en normalfordeling med forventning 0 og standardavvik 2. Videre er fordelingen til støyleddet en standard normalfordeling $\epsilon\sim N(0,1)$. Betingelsen om uavhengig feilledd er oppfylt siden vi ikke har brukt feilleddet til å generere forklaringsvariablene. Det er responsvariabelen, men den skal altså være avhengig av støyleddet. Modellen tar formen
\begin{align}
  Y = 2 X_1 - X_2 + \epsilon,\quad \epsilon \sim N(0, 1),
\end{align}
hvor parameterne er eksakt fordi vi har laget dataene selv. 

b. 
```{r}
reg <- lm(y ~ x1 + x2, data = df_mdl)
stargazer(reg, type = "text")
```
Vi ser at estimatene av koeffisientene er signifikante, og at sanne verdier er innenfor konfidensintervallet for hver av dem. Det er ikke et konstantledd i modellen vår, noe som stemmer overens med at den ikke er signifikant forskjellig fra null i regresjonsmodellen.

c. 
```{r}
new_df <- data.frame(x1 = 1, x2 = 1)
reg_pred <- predict(reg, newdata = new_df, interval = "predict")
reg_conf <- predict(reg, newdata = new_df, interval = "confidence")
```

```{r, echo = FALSE}
stargazer(reg_pred, type = "text", title = "Prediksjonsintervall")
stargazer(reg_conf, type = "text", title = "Konfidensintervall")
```
d. Eksakt prediksjonsintervall for modellen som genererte dataene er
\begin{align}
  \bar{Y}|X_1,X_2 \pm \alpha_{0.025} \sigma_{\epsilon} &= 2 \cdot 1 - 1 \cdot 1 \pm 1.96 \cdot 1 \\
  &= 1 \pm 1.96 \\
  &= [-0.96, 2.96].
\end{align}
Estimert prediksjonsintervall er bredere enn det estimerte intervallet. Dette kommer av usikkerhet knyttet til estimatene av forventning og standardfeil. Det er benyttet T-test i estimatet av prediksjonsintervallet for å ta hensyn til denne usikkerheten. Man burde ikke få smalere prediksjonsintervaller enn det som faktisk er "sant" med mindre noen av modellantagelsene er usann. Merk at når vi generer opp data selv vet vi sannheten, det gjør vi ikke for virkelige data.

e. Vi burde få at $\bar{Y}|X_1,X_2 = 1$ er innenfor konfidensintervallet, noe ser ut til å være oppfylt. Tolkningen er at dersom vi kjører analysen på mange ulike datasett vil $\bar{Y}|X_1,X_2 = 1$ havne innenfor konfidensintervallet i 95 % av tilfellene. Dette kan du prøve selv, bare pass på at du ikke setter seed-et før du generer opp et nytt datasett.

f. Svaret gis i regresjonstabellen
```{r}
stargazer(reg, type = "text")
```
F-statistikken er angitt som $1094.997$ med frihetsgrader $2, 97$. P-verdien regnes som 
```{r}
pf(1094.997, 2, 97, lower.tail = FALSE)
```
som er veldig liten. Altså bekrefter analysen at det er sammenheng mellom forklaringsvariablene og responsvariabelen. Hva ville du tenkt var galt om vi ikke fikk denne konklusjonen fra F-testen, gitt den modellen vi har brukt til å generere dataene? Hint: Det er faktisk en reell sammenheng her.
</blockquote></details>

#### Oppgave 2 {-}
Denne oppgaven følger samme premiss som oppgave 1, men handler om kolinearitet. Last ned datasettene [1](genererte_datasett/colinearity_1.RData) og [2](genererte_datasett/colinearity_2.RData), hvor y er responsvariabel og forklaringsvariable er navngitt x.

Disse dataene er lagret som .RData filer. For å laste inn dataene i R laster du filene over ned i en mappe, setter mappestien til denne mappen og kjører følgende R-kommandoer:

```{r, eval = FALSE}
load("colinearity_1.RData")
load("colinearity_2.RData")
```


- Finn kolinearitet

- Hva må man passe dersom man skal lage en regresjonsmodell for responsen Y? 

<details>
<summary>Løsning</summary><blockquote>
```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(car)
library(corrplot)
```

```{r, echo = FALSE}
set.seed(4)
df_colin1 <- data.frame(
    x1 = rnorm(100, sd = 2),
    x2 = rnorm(100, sd = 2)
  ) %>%
  mutate(
    x3 = 0.5 * x2 + rnorm(100, sd = 1)
  ) %>%
  mutate(
    y = 1.3 * x1 + (-0.8) * x2 + rnorm(100, sd = 1)
  )
# save(df_colin1, file = "genererte_datasett/colinearity_1.RData")
set.seed(4)
df_colin2 <- data.frame(
    x1 = rnorm(100, sd = 2),
    x2 = rnorm(100, sd = 2),
    x3 = rnorm(100, sd = 2)
  ) %>%
  mutate(
    x4 = x3 + rnorm(100, sd = 2)
  ) %>%
  mutate(
    x5 = x2 + x3 + rnorm(100, sd = 1)
  ) %>%
  mutate(
    y = 2 * x1 + (-1) * x2 + rnorm(100, sd = 1)
  )
# save(df_colin2, file = "genererte_datasett/colinearity_2.RData")
```

1) 

Vi kan begynne med å plotte forholdet mellom de ulike variablene:
```{r}
pairs(y ~ ., data = df_colin1)
```

```{r}
corrplot(cor(df_colin1), method = "number")
```

Her ser det ut som vi har et forhold mellom $(X_2, X_3)$. Videre ser det ut som at alle tre forklaringsvariable har en sammenheng med responsvariablen $Y$.

```{r}
reg_colin1 <- lm(y ~ ., data = df_colin1)
stargazer(reg_colin1, type = "text")
```

$X_1$ og $X_2$ kommer ut som signifikante, og $X_3$ ved lavere signifikansnivå.

Vi kan sjekke variance inflation factor (VIF)
```{r}
vif(reg_colin1)
```
som oppgir at estimeringen i seg selv ikke burde være noe problem.

**Kommentarer:**

Det reelle forholdet mellom variablene i dette eksempelet er 
\begin{align}
  X_1 &\sim N(0, 2) \\  
  X_2 &\sim N(0, 2) \\  
  X_3 &= 0.5 X_2 + \epsilon_3 \quad \epsilon_3 \sim N(0,1)  \\
  Y &= 1.3 X_1 - 0.8 X_2 + \epsilon, \quad \epsilon \sim N(0,1),
\end{align}
og vi kan tenke oss at dette representerer det kausale forholdet mellom variablene. Siden vi har generert dataene kan vi si det.

Det er videre interessant å merke seg i dette eksempelet at $X_3$ kommer ut med en signifikant koeffisient selv om den rent kausalt ikke har noen sammenheng med $Y$. $X_3$ er kun relatert til $X_2$.

Dersom man ønsker å gjøre prediksjon kan det være forklaringskraft i $X_3$ for å gjøre prediksjon av $Y$, men i så tilfelle antas det at forholdet mellom $(X_2, X_3)$ også vedvarer i fremtiden. Dersom man skal gjøre inferens om en tenkt kausal sammenheng er det mulig å gå på en blemme i et tilfelle som dette.

Hvis man ønsker å forklare noe og det er sammenheng mellom to forklaringsvariable kan det fra økonomisk prespektiv være interessant å spørre seg om man bryr seg om $X_3$ i det hele tatt og skal ta den ut. Kanksje $X_3$ er vanskelig å samle inn data om eller den åpenbart ikke har sammenheng med Y. Om den åpenbart ikke skal ha sammenheng med $Y$ kan støyleddet $\epsilon_3$ også skape unødig støy i prediksjonene.

2)

Plott forholdet mellom de ulike variablene:
```{r}
pairs(y ~ ., data = df_colin2)
```

```{r}
corrplot(cor(df_colin2), method = "number")
```


Her ser vi et tydelig forhold mellom parene $(X_3, X_4)$, $(X_4, X_5)$, $(X_3, X_5)$ og $(X_2, X_5)$. Ellers ser det kun ut som det er et tydelig forhold mellom $Y$ og $X_1, X_2$. Med så mange forhold kan det være snakk om multikolinearitet, som ikke er så lett å se utfra 2D-plott.

```{r}
reg_colin2 <- lm(y ~ ., data = df_colin2)
stargazer(reg_colin2, type = "text")
```

Vi ser på VIF for om det kan være problemer med estimering grunnet kolinearitet
```{r}
stargazer(vif(reg_colin2), type = "text", title = "VIF")
```
hvor vi ser at $X_5$ nærmer seg problematisk område for å få en overdreven varians i estimatene.

Dersom målet er å estimere sammenheng mellom alle forklaringsvariable og $Y$ vil dette kunne gå gjennom siden VIF ikke er altfor høy. Hvis det derimot er snakk om modellbygging bør man tenke gjennom om noen av variablene ikke er så viktige og bør tas ut.

**Kommentarer:**

Det reelle forholdet mellom variablene i dette eksempelet er 
\begin{align}
  X_1 &\sim N(0, 2) \\  
  X_2 &\sim N(0, 2) \\  
  X_3 &\sim N(0, 2) \\
  X_4 &= X_3 + \epsilon_4, \quad \epsilon_4 \sim N(0,2) \\
  X_5 &= X_2 + X_3 + \epsilon_5, \quad \epsilon_5 \sim N(0,1) \\
  Y &= 2 X_1 - X_2 + \epsilon, \quad \epsilon \sim N(0,1)
\end{align}

Her kan vi igjen merke oss at variablene $X_3, X_4, X_5$ kommer inn i regresjonsmodellen med noen koeffisienter som er svakt signifikante. I dette tilfellet trenger ikke dette ha så mye å si. Likevel, ettersom det ikke er noen direkte sammenheng mellom $X_3, X_4, X_5$ ville det vært ideelt å ikke ha dem med i regresjonsmodellen for å unngå feilestimering av koeffisientene til $X_1$ og $X_2$. Fra anvendt statistikk perspektiv bør man tenke gjennom om det gir mening at $X_3, X_4, X_5$ forklarer $Y$.

</blockquote></details>



<!-- Generér gjerne opp dataene selv og analyser dem mens du svarer på oppgavene. -->

<!-- a. Er det uavhengige feilledd her? -->

<!-- b. Er det kolinearitet mellom forklaringsvariablene her? Om så, er den enkel eller har vi multikolinearitet? -->

<!-- c. Hva har svarene på oppgaven før å si for estimat av koeffisientene for modellen for Y? -->

<!-- <details> -->
<!-- <summary>Løsning</summary> -->

<!-- </details> -->


<!-- #### Oppgave 3 -->

<!-- I denne oppgaven skal vi se hva som kan gå galt dersom man leter etter signifikans i store datasett. Vi starter med å simulere en responsvariabel og en  -->

<!-- (Trekk tilfeldige tall og analyser det...) -->

<!-- (Dette er kjent som P-hacking. Det er *veldig* dårlig praksis, men har en tendens til å snike seg inn i analyser på mer eller mindre subtile vis. Det finnes store mengder forskning hvor P-hacking forekommer, bevisst eller ubevisst. De mindre subtile måtene dette skjer på heter juks. -->

<!-- Out-of-sample validering og testing  -->
<!-- ) -->

<!-- #### Oppgave 4 -->

<!-- (Legg til i enkel regresjon at man kan komme frem til det samme ved Cov-regning.) -->

<!-- Vi så i oppgave ... at minste kvadraters metode brukes til parameter-estimering. I multivariat regresjon får vi enda flere ligninger enn vi så for enkel regresjon, dermed bruker man lineæralgebra. -->

<!-- Vis at man kan regne minste kvadraters estimat ved kovarians. -->

<!-- En matrise lar seg invertere dersom den er kvadratisk og har lineært uavhengige kolonner. På matrisenotasjon blir kovarians regnet som $X^TX / N$.  -->

<!-- Dersom man har multikolinearitet er det kolonner i kovariansmatrisen som er sterkt lineært avhengige. Hvorfor blir det problematisk å  -->


<!-- <detail><summary>Løsning</summary> -->

<!-- </detail> -->

<!-- ### Case-studie med datasett -->

<!-- Her skal vi gå gjennom en analyse på et ofte brukt datasett: Boston Housing Data. Deloppgavene er satt opp for å gi litt drahjelp, sammenlignet med det som ellers kunne vært utgangspunktet for en hjemmeeksamen. -->




<!-- 
#### (Oppgave 1) {-}

Studenten Rita Stata har som mål å gjennomføre et kurs i statistikk med minst mulig arbeid. Hun ligger foreløpig ikke så bra an, men ønsker å finne ut hva slags karakter hun forvente til slutt. Karakteren er basert på følgende beregning:
Karakter = 20 % (Assignment) + 30 % (Midterm test) + 50 % (Final exam)
Karakteren er et tall hvor 0 er lavest mulig og 100 er høyest mulig. 
Det er bare tre uker til eksamen, og på ‘assignment’ og ‘midterm test’ fikk Rita henholdvis 12/20 og 14/30 poeng. 
Rita samlet inn data over karakterer, og score på ‘midterm test’ og ‘assignment’ hos 30 tidligere studenter. Hun brukte deretter en statistikkprogramvare til å beregne et regresjonsuttrykk. Statistikkprogramvaren leverte følgende oppsummering:
 
![](bilder/4_regresjon/tbl_1.png){width=40em}

a. Hva er regresjonsuttrykket?

b. Hva er standardfeilen til målingen? Hva mener vi med standardfeil?

c. Hva er forklaringsgraden (bestemmelseskoeffisienten)? Hva forteller verdien oss?

d. Test validiteten av modellen. Dette gjøres ved hjelp av F-testen. 

e. Gi en tolkning av koeffisientene i modellen. 

f. Kan Rita anta at score på ‘assignment’ har et lineært forhold med karakter i statistikk?

g. Kan Rita anta at score på ‘midterm test’ har et lineært forhold med karakter i statistikk?

Husk: vi kaller ofte et regresjonsuttrykk for «modell». Dette er fordi regresjonsuttrykket i praksis fungerer som en modell, i dette tilfellet en modell som beregner karakter i statistikk. 

<details>
<summary>Løsning</summary>

</details>

#### (Oppgave 2) {-}
Et stort konsulentselskap i USA prøver å lage et system for å avgjøre hvilke søkere til internship som skal bli innkalt til intervju. Søkerne har bare gått ett år på universitetet, så de ønsker å bruke en modell til å predikere hvilke karakterer de vil få på bachelorgraden. De tror at karakterene fra High School og SAT Score er de viktigste variablene for å avgjøre hvilke karakterer søkeren vil få på universitetet. De tror også at søkere som har deltatt i utenomfaglige aktiviteter har større sannsynlighet for å få gode karakterer. For å undersøke dette velger de et tilfeldig utvalg av 100 ansatte som har hatt suksess i selskapet og finner følgende informasjon om de ansatte:
Karakterer fra universitetet (0-12) 
Karakterer fra High School (0-12)
SAT Score (400-1600)
Antall timer brukt på utenomfaglige aktiviteter per uke 
Under finner du en utskrift av regresjon med disse variablene:

![](bilder/4_regresjon/tbl_2.png){width=40em}
 
a. Hva er regresjonsutrykket til modellen? Hva er den avhengige (Y) og de uavhengige variablene (X)?  

b. Hva er forklaringsgraden til modellen? Tolk verdien. 

c. Test validiteten til modellen ved å beskrive F-testen og resultatet av testen.

d. Bestem hvilke av de uavhengige variablene som er lineære i modellen ved å tolke t-statistikken og p-verdien til hver av koeffisientene. 

e. Hva er den predikerte karakteren på universitetet til en søker som har High School GPA 10, SAT score 1200 og arbeider 2 timer i gjennomsnitt med utenomfaglige aktiviteter i uken. Merk: selv om noen av koeffisientene ikke er statistisk signifikante så kan man gjøre prediksjoner med modellen. Ved prediksjon er vi mest opptatt av at forklaringsgraden er høy.

f. Hva er den predikerte karakteren på universitetet til en søker som har High School GPA 8, SAT score 1100 og arbeider 10 timer i gjennomsnitt med utenomfaglige aktiviteter i uken. 

g. Kausalitet og korrelasjon er et viktig tema i dette faget. Viser denne modellen en kausal sammenheng mellom karakterer på High School og karakterer på universitetet? 

<details>
<summary>Løsning</summary>

</details>

#### (Oppgave 3) {-}

Vi undersøker om forutsetningene for OLS holder for modellen ovenfor. 

- Hvilke forutsetninger kan vi undersøke ved å studere de to diagrammene nedenfor? 

![](bilder/4_regresjon/hist_3.png){width=30em}

![](bilder/4_regresjon/res_vs_pred_3.png){width=30em}

- Vil du si at de aktuelle forutsetningene holder?

<details>
<summary>Løsning</summary>

</details>

#### (Oppgave 4) {-}

- Hvordan tolker du diagrammene nedenfor?

- Holder forutsetningene for OLS?

a. 
![](bilder/4_regresjon/hist_4a.png){width=30em}
b. 
![](bilder/4_regresjon/res_vs_pred_4b.png){width=30em}
c.
![](bilder/4_regresjon/hist_4c.png){width=30em}

![](bilder/4_regresjon/res_vs_pred_4d.png){width=30em}

<details>
<summary>Løsning</summary>

a. Histogrammet er bjelle-formet og det kan se ut som at det er en normalfordeling.

b. Plottet viser tegn til heteroskedastisitet. Forutsetningen er ikke oppfylt. 

c. Histogrammet er skjevfordelt og forutsetningen om normalfordeling holder ikke.

d. Plottet viser tegn til heteroskedastisitet. Forutsetningen er ikke oppfylt.

</details>

#### (Oppgave 5) {-}

a. Hva er konsekvensen av brudd på forutsetningen om konstant varians?

b. Hva er konsekvensen av brudd på forutsetningen om normalfordelte feilledd? 


<details>
<summary>Løsning</summary>



</details>

#### (Oppgave 6) {-}

Et transportselskap ønsker å analysere hvordan ulike variabler påvirker hvor lang tid det tar å laste av lastebilene deres. I modellutskriften under vises resultatet av en modell hvor avhengig variabel er antall minutt det tar å laste av en lastebil. De uavhengige variablene er antall bokser og vekten på lasten. I tillegg ønsker transportselskapet å finne ut om tidspunktet på dagen påvirker hvor lang tid det tar å laste av bilene. De deler døgnet inn i tre: I1 = morgenen, I2 = tidlig ettermiddag og I3 = sen ettermiddag.
 
![](bilder/4_regresjon/tbl_6.png){width=40em}
 
a. Gi en kortfattet fortolkning av regresjonsutskriften over.

b. Det er mistanke om multikollinearitet i modellen. Hvilke to variabler tror du kan samvariere? 

c. Hvorfor er ikke l3 i regresjonsutskriften? 

d. Hva kan vi si om betydningen av tidspunktet på dagen og hvor lang tid det tar å laste av lastebilene? 

e. Tegn regresjonslinjen til modellen. 

<details>
<summary>Løsning</summary>

</details> -->

<!-- ### Nøtter -->


<!-- I tabellen under finner du noen oppgaver som du kan bryne deg på for å sjekke forståelsen din og trene på metodene som vi har gått gjennom i denne modulen. Vi peker også på noen tidligere eksamensoppgaver som er relevante til denne tematikken, du finner oppgavene under seksjon \@ref(skoleeksamen). -->

<!-- Har du en eldre utgave av boken kan du laste ned [dette dokumentet](oppgaver/Recommended excercises.doc) for en oversikt over oppgavenummer tilbake til 7. utgave. -->

<!-- **På Canvas finner du en .zip-fil som inneholder alle datasettene, samt løsningsforslag til oppgavene i læreboken.** -->

<!-- ```{r, echo = FALSE, message = FALSE} -->

<!-- library(dplyr) -->
<!-- options(knitr.kable.NA = '') -->

<!-- "oppgaver/oppgaver-regresjon.xlsx" %>%  -->
<!--     readxl::read_excel() %>%  -->
<!--     kableExtra::kbl() %>%  -->
<!--     kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed")) -->

<!-- ``` -->

## Relevante R-kommandoer {#relevante-r-regresjon}

Under følger en liste over hvilke oppgaver du skal klare i R fra denne modulen. Vår policy fra og med **vårsemesteret 2022** er at R-kommandoene under er *tilstrekkelige* for å løse oppgavene i datalabber og hjemmeeksamen i MET4. Det er med andre ord ikke nødvendig å lære seg teknikker utover det som er listet opp eksplisitt i listen under. Eventuelle nye teknikker som trengs for å løse en bestemt oppgave vil bli oppgitt og forklart dersom det er nødvendig. Det antas i tillegg at du kan den grunnleggende R-syntaksen som er dekket under [Introduksjon til R].

### Antakelser om datasett {-}

Du kan gjøre de samme antakelsene om datasettet som i [den tilsvarende oversikten i forrige modul](#relevant-r-testing): Datasettene er inneholdt i Excel-filer (`.xls` eller `xslx`) eller `.csv`-filer, som kolonner av variabler, med variabelnavn i første rad.

### Tilpasse en lineær regresjonsmodell {-}

```{r, echo = FALSE, eval = FALSE}
library(openxlsx)

set.seed(1)

n <- 100

data <- 
  tibble(x1 = runif(n, 10, 30),
         x2 = runif(n, 95, 101),
         x3 = sample(c(0,1), n, replace = TRUE)) %>% 
  mutate(y = x1 + x2 - 20*x3 + 5*rnorm(n))

write.xlsx(x = data, file = "datasett/data-reg.xlsx")

```

Her er et lekedatasett med tre forklaringsvariabler `x1`, `x2` og `x3`, samt en responsvariabel `y`: [data-reg.xsls](datasett/data-reg.xlsx).

```{r, echo = FALSE}
df <- readxl::read_excel("datasett/data-reg.xlsx")
```


```{r, eval = FALSE}
# Last inn data
library(readxl)
df <- read_excel("data-reg.xlsx")

# Syntaksen for å tilpasse en lineær regresjonsmodell med responsvariabel y og
# forklaringsvariabler x1, x2, x3 i datasettet df er som følger:
reg1 <- lm(y ~ x1 + x2 + x3, data = df)

# Dersom vi vil ha med interaksjonsledded mellom x1 og x3 kan vi skrive
# kommandoen under. Da vil også x1 og x3 automatisk bli med som variabler i
# tillegg til interaksjonsledde x1*x3.
reg2 <- lm(y ~ x1 + x2*x3, data = df)

# Log transformasjoner kan vi skrive rett inn i formelen
reg3 <- lm(log(y) ~ x1 + log(x2) + x3, data = df)

# Andre transformasjoner, som for eksempel dersom vi ønsker å ta med
# andregradsleddet x1^2, er greiest å få til dersom vi først lager en ny kolonne
# med kvadratene til x1, og så lager til regresjonen på vanlig måte:
df_ny <- df %>% mutate(x1_kvadrat = x1^2)
reg4 <- lm(y ~ x1 + x1_kvadrat+ x2 + x3, data = df_ny)

# Vi kan skrive ut et sammendrag for regresjonsmodellen ved hjelp av summary():
summary(reg1)

# Pakken stargazer inneholder en funksjon for å lage pene regresjonstabeller:
library(stargazer)
stargazer(reg1, reg2, type = "text")

# Man kan endre type-argumentet til "html", og sette out-argumentet til et
# filnavn hvis du vil skrive ut tabellen til en fil.
```

### Prediksjoner, med konfidens- og prediksjonsintervall {-}

La oss si at vi ønsker å predikere verdien av responsvariabelen for følgende verdier av forklaringsvariablene:

```{r, echo = FALSE}
tibble(x1 = c(10, 20, 30), x2 = c(95, 96, 100), x3 = c(1,1,0)) %>% 
  knitr::kable()
```
Da må vi først samle verdiene av forklaringsvariablene i en data frame; det kan vi enten gjøre direkte i R:

```{r, eval = FALSE}
ny_data <- data.frame(x1 = c(10, 20, 30), x2 = c(95, 96, 100), x3 = c(1,1,0))
```

Et alternativ kan være å lage til et regneark i Excel, og så lese det inn ved hjelp av `read_excel()`. **Det er viktig at variabelnavnene i det nye datasettet er eksakt de samme som forklaringsvariablene i datasettet du brukte til å estimere regresjonsmodellen!** Vi kan lage prediksjon med 95% konfidensintervall ved hjelp av `predict()`:

```{r, eval = FALSE}

prediksjoner <- predict(reg1, 
                        newdata = ny_data, 
                        interval = "confidence", 
                        level = 0.95)
```


Resultatet blir en ny data frame med kolonnene `fit` (prediksjoner), samt `lwr` og `upr`, som inneholder nedre og øvre grense i konfidensintervallet henholdsvis. Du kan endre til prediksjonsintervaller ved å bytte ut `"confidence"` med `"prediction"` (eventuelt `"none"` hvis du ikke ønsker noe intervall).

### Lage residualplott {-}

Vi må kunne lage følgende residualplott i R, her demonstrert med regresjonsmodellen `reg1` som vi laget over:

```{r, eval = FALSE}
# Henter ut residualer og predikerte verdier fra modellen, og setter de inn som 
# kolonner i en dataframe:
residualer <- data.frame(residualer = reg1$residuals,
                         predikert = reg1$fitted.values)

# Spredningsplott, residualer mot predikerte verdier
ggplot(residualer, aes(x = predikert, y = residualer)) +
  geom_point()

# Autokorrelasjonsplott
acf(reg1$residuals)

# Histogram for å sjekke normalantakelsen
ggplot(residualer, aes(x = residualer)) +
  geom_histogram()

# QQ-plott, observasjonene ligger langs en rett linje hvis de er normalfordelte
ggplot(residualer, aes(sample = residualer)) +
  stat_qq() +
  stat_qq_line()

# Regner ut og plotter Cooks avstand:
infl <- influence.measures(reg1)

# Lager enkelt plott av Cooks avstand
cooks_avstand <- data.frame(obs = 1:nrow(df),
                            cook = infl$infmat[, "cook.d"],
                            cook_infl = infl$is.inf[, "cook.d"])

ggplot(cooks_avstand, aes(x = obs, y = cook)) +
  geom_col()
```

Alle plottene over kan justeres og pyntes, for eksempel ved å gjøre endringer som i [R-videoen om plotting](#r-plotting). Se også forelesningsscriptet for flere tips om presentasjon av residualplott.



