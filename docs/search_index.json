[["index.html", "MET4 - Empiriske Metoder - høst 2025 Innledning", " MET4 - Empiriske Metoder - høst 2025 Håkon Otneim og Geir Drage Berentsen Innledning Sondre Velkommen til hjemmesiden for kurset MET4 - Empiriske metoder, som er et obligatorisk kurs på Bachelorprogrammet i Økonomi og Administrasjon ved Norges Handelshøyskole. Dette er et kurs i anvendt statistikk, spesielt tilpasset økonomistudiet, og vi skal fokusere på korrekt bruk av statistisk metodikk for å løse relevante problemstillinger i en verden der innsamlet data utgjør en stadig større del av beslutningsgrunlaget i bedrifts- og samfunnsstyring. Kursansvarlige er Sondre Hølleland og Geir Drage Berentsen, som begge jobber ved Institutt for Foretaksøkonomi ved NHH. Geir På denne siden vil du finne alle videosnuttene med kommentarer til lærebokens fremstilling, regneoppgaver, samt supplementer og referanser til andre bøker der læreverket vårt ikke strekker til (særlig mot slutten av kurset). Fremdriftsplan Vi har laget et forslag til progresjon i tabellen under. Kolonnen “jobbe med” refererer til tema på denne siden som dere finner igjen i menyen til venstre. I hovedsak bruker vi to uker på hvert tema i kurset og aktivitetene i disse ukene er i all hovedsag strukturert som følger: Uke 1: Selvstudie av det respektive temaet ved hjelp av videoer, regneoppgaver og teori på denne nettsiden. Vi tar derfor en dag uten noen organisert aktivitet, og så tar vi en dag der dere kan komme i Aud. A og spørre om ting dere lurer på samt jobbe med regneoppgaver. Vi passer på at alle har noen å jobbe med, så du må gjerne komme selv om du ikke har noen spesifikke som du har planlagt å sitte med. Regneoppgavene finner du under hvert tema til venstre. Foreleser vil være tilgjengelig. Uke 2: Vi gir en oversiktsforelesning i temaet og tar et seminar med mer utfordrende oppgaver i fellesskap, begge deler i Aud A (se Avsnitt 8 for seminaroppgavene). Ingen av de fysiske aktivitetene vil bli tatt opp eller streamet. Videoene og teorien på denne nettsiden er et mer en godt nok digitalt tilbud. Notatene fra oversiktsforelesningene og oppgaveseminar (Se 8) vil bli lagt ut på Canvas. Selv om det nå legges opp til et fleksibelt undervisningsopplegg der tempo og progresjon til dels kan tilpasses den enkeltes timeplan og studieteknikk, er det viktig å jobbe jevnt med kurset. MET4 er et krevende kurs som krever full innsats fra første til siste dag. For eksempel får en mye bedre utbytte av av de fysiske aktivitetene dersom man på forhånd har sett videoene og gjort noen av regneoppgavene relatert til temaet. Praktisk bruk av statistiske metoder står sentralt i MET4, og denne delen dekker vi gjenom dataøvinger med studentassistenter. I kalenderen under har vi markert ukene der vi har tid på datasal med studentassistenter med grønn farge. Du finner oppgaver til dataøvingene i menyen til venstre. Legg merke til følgende viktige datoer: 26.september Siste frist for oppmelding av grupper til obligatorisk øvelse. 10.oktober Frist for innlevering av den obligatoriske innleveringen. 25.november Individuell digital skoleeksamen. Eksamen vil være en 6 timers digital skoleeksamen. Eksamen vil ha fokus på både teori og dataanalyse i R. Begge delene er like omfattende og vil bli vurdert samlet hvor det settes en karakter. Hele eksamen vil gjennomføres digitalt på skolen og kandidaten disponerer selv hvor mye tid som brukes på teori og dataanalyse. TILLATTE HJELPEMIDLER: Alle trykte/egenskrevne hjelpemidler. Altså kan dere ta med så mange utskrifter, bøker og notater som dere ønsker. Uke Jobbe med Mandag 10:15 - 12:00 Torsdag 12:15 - 14:00 34 Introduksjon til R 18.08: Introduksjon til MET4 . Aud A 21.08:Introduksjon til R i Aud A 35 Grunnleggende statistikk 25.08: Oversiktsforelesning Grunnleggende statistikk i Aud A 28.08: (Ingen aktivitet) 36 Hypotesetesting og Dataøving 1 01.09: Oppgaveseminar 1 i Aud A. Se 8 for oppgaver. 04.09: Kontakttime i Aud A 37 Hypotesetesting 08.09: Oversiktsforelesning: Hypotesetesting i Aud A 11.09: Oppgaveseminar 2 i Aud A. Se 8 for oppgaver. 38 Regresjon og Dataøving 2 15.09: (Ingen aktivitet) 18.09: Kontakttime i Aud A 39 Regresjon 22.09: Oversiktsforelesning Regresjon, Aud A 25.09: Oppgaveseminar 3 Aud A. Se 8 for oppgaver. 40 Dataøving 3 og Obligatorisk innlevering 29.09: (Ingen aktivitet) 02.10: Oppgaveseminar 4 Aud A. Se 8 for oppgaver. 41 Tidsrekker 06.10: (Ingen aktivitet) 09.10: Kontakttime i Aud A 42 Tidsrekker 13.10: Oversiktsforelesning: Tidsrekker i Aud A 16.10: Oppgaveseminar 5 Aud A. Se 8 for oppgaver. 43 Avansert regresjon og maskinlæring/Dataøving 4 20:10 (Ingen aktivitet) 23.10: Kontakttime i Aud A 44 Avansert regresjon og maskinlæring / Dataøving 5 27.10: Oversiktsforelesning: Avansert regresjon og maskinlæring i Aud A 30.10: Oppgaveseminar 6, Aud A. Se 8 for oppgaver. 45 Forberedelse til eksamen 03.11: Praktisk informasjon om eksamen 06.11: Kontakttime. Aud A 46 Forberedelse til eksamen 10.11: (Ingen aktivitet) 13.11: Kontakttime, Aud A Lærebok og pensum Second EMEA edition. Vi bruker læreboken Statistics for Management and Economics av Gerald Keller, som nå foreligger i en relansert utgave (Second EMEA edition). I utgangspunktet er følgende kapitler pensum: 1–5, 9–13, 15–18 og 20. Mot slutten av kurset går vi gjennom noen tema som ikke er dekket i læreboken (logistisk regresjon, maskinlæring, paneldatamodeller, tidrekkemodeller). Der finner du referanser til andre kilder, samt en del materiale som vi har skrevet selv. I pensumgjennomgangen på denne siden finner du også en del kommentarer til læreboken, som for eksempel hva som er viktig, og hva som er mindre viktig for oss. Eldre utgaver av læreboken går bra. Vi har sluttet å bruke regneoppgaver fra boken, så vi trenger ikke lenger oppdatere oppgavenummer etc. for hver nye utgave. Regneoppgaver finner du her på denne siden under hvert tema. "],["introduksjon-til-r.html", " 1 Introduksjon til R", " 1 Introduksjon til R Vi skal i dette kurset bruke programmeringsspråket R til å gjøre beregninger og gjennomføre de ulike statistiske analysene som vi skal lære etter hvert. Dette vil være nytt for mange. Vi skal først og fremst skal skrive kode og kommandolinjer for å få ut resultater i R, noe som kan oppleves uvant siden vi ellers er vant med å klikke oss frem i et menysystem når vi jobber med ulike programmer. Trøsten kan være at ferdigheter i programmering blir stadig viktigere i mange yrker, spesielt innen økonomifaget. Vi må installere to ting på maskinen vår før vi går videre; selve programeringsspråket R, samt programmet RStudio som vi skal bruke til å skrive og kjøre koden. Begge deler er gratis, og begge deler fungerer fint på både Windows og Mac (og Linux!). Det er greiest å gjøre dette i riktig rekkefølge: Gå til r-project.org for å laste ned R til ditt operativsystem, og installer på vanlig måte uten å forandre på foreslåtte innstillinger. Gå til https://posit.co/download/rstudio-desktop/. Du skal der laste ned RStudio programmet for ditt operativsystem og installere på vanlig måte. Det er heller ikke her nødvendig å forandre på de foreslåtte innstillingene. Du kan så åpne RStudio, og følge sekvensen av videoforelesninger som følger under. "],["en-gjennomgang-av-rstudio.html", "1.1 En gjennomgang av RStudio", " 1.1 En gjennomgang av RStudio I denne videoen snakke vi litt om forskjellen mellom programmeringsspråket R og programmet RStudio. Vi åpner opp Rstudio og rusler gjennom det grafiske grensesnittet. "],["enkle-beregninger-og-variabler.html", "1.2 Enkle beregninger og variabler", " 1.2 Enkle beregninger og variabler Vi går videre og skriver våre første kommandoer i R. Det er kritisk at vi allerede nå setter i gang med å få programmeringen inn i fingrene, og det gjøres best ved å skrive inn kodelinjene slik det gjøres i videoen, og passe på at du får ut de samme resultatene. # Vi kan bruke R som en kalkulator: 2+2 # Det var enkelt! Vi må bruke paramteser dersom vi har mer kompliserte uttrykk: (2+8)/2 2+8/2 # Variabler er viktige i R. Vi kan lagre mer eller mindre hva som helst i # dataminnet ved å gi det navn: a &lt;- 5 a a*5 b &lt;- 3 # R gjennomfører alle operasjoner på høyresiden før verdien blir tilordnet # variabelen c: c &lt;- a+b c # Ingen feilmeldinger, advarsler eller spørsmål ved overskriving: c &lt;- 4 c &lt;- c + 2 c # La oss lage en feilmelding! d # Vi kan navngi omtrens slik vi vil, og dette er ikke et trivielt problem i # større prosjekter! hva_som_helst &lt;- &quot;hello world&quot; hva_som_helst Når du er ferdig med det kan du prøve deg på følgende lille oppgave: Oppgave: Velg dine tre favorittall og lagre dem i tre forskjellige variabler. Beregn så ditt magiske tall, som er summen av favorittallene dine. Lagre ditt magiske tall i en ny variabel, og gi denne variabelen et informativt navn som identifiserer hva det er. Fikk du det til? Kikk på løsningen under for å sjekke. Løsning tall1 &lt;- 1 tall2 &lt;- 87 tall3 &lt;- 101 magisk_tall &lt;- tall1 + tall2 + tall3 "],["vektorer.html", "1.3 Vektorer", " 1.3 Vektorer Vi introduserer begrepet vektorer som er svært viktig i statistikk generel og R spesielt. En vektor er ganske enkelt en samling med tall, og når vi senere begynner å jobbe med data kommer vi til å lagre observasjoner av ulikt slag i vektorer. Vi ser også at vi kan gjøre operasjoner på vektorer ved å bruke funksjoner. For eksempel bruker vi sum()-funksjonen til å regne ut summen av alle tallene som er lagret i en vektor. # Vi kan lage en vektor på følgende måte: vector1 &lt;- c(3, 5, 7.8, 10, 2, 0.16, -3) # Skriv ut: vector1 # Plukke ut verdier vector1[1] # Plukker ut verdier med hakeparanteser vector1[10] # Out-of-range error vector1[2:5] # Plukke ut en sekvens vector1[c(1,3)] # Plukke ut verdier basert på en ny vektor! # Bokstaven &quot;c&quot; står for combine. R gjør det veldig enkelt å jobbe med # vektorer: vector1 - 1 vector1*3 # Vi kan bruke *funksjoner* til å regne ut forskjellige ting: length(vector1) mean(vector1) sum(vector1) sd(vector1) # Vi kan lage vektorer av andre ting enn tall: vector2 &lt;- c(&quot;hello&quot;, &quot;world&quot;) # ... men en vektor kan bare inneholde en datatype. # Kanksje trenger vi standardavviket senere? sd_vector1 &lt;- sd(vector1) sd_vector1 Oppgave: Beregn maksimum- og minimumsverdien av vector1, samt medianen, ved å bruke funksjoner i R. (Hint: en dårlig skjult hemmelighet i anvendt programmering er at dersom vi ikke vet navnet på funksjonen vi skal bruke, så er Google vår’ beste venn!) Løsning # Relevante Google-søk: &quot;minimum value r&quot;, &quot;maximum r&quot;, &quot;median r&quot; min(vector1) max(vector1) median(vector1) "],["pakker.html", "1.4 Pakker", " 1.4 Pakker Vi lærer at når vi laster ned R så følger det med et grunnleggende sett av funksjoner (“base R”), men at det finnes et stort antall tilleggspakker. Vi kan enkelt laste ned og installere disse pakkene ved å skrive kommandoen install.packages(\"pakkenavn\"). Det trenger vi bare gjøre en gang på datamaskinen vår. For å bruke pakken må vi skrive kommandoen library(pakkenavn), og det må vi gjøre hver gang i restarter R. # For å installere pakken så kjører vi følgende kommando. Denne kjøres # bare en gang per masking, den blir installert en gang for alle. install.packages(&quot;readxl&quot;) # Når vi skal bruke pakken så laster vi den inn ved å bruke &quot;library()&quot;-, # funksjonen, og den må kjøres hver gang vi starter R: library(readxl) Oppgave: Installer følgende pakker, som vi kommer til å bruke senere i kurset: ggplot2 dplyr stargazer Løsning install.packages(&quot;ggplot2&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;stargazer&quot;) "],["mappesti.html", "1.5 Mappesti", " 1.5 Mappesti Vi kommer til å forholde oss til filer på flere måter. Vi skal lese inn datafiler, og vi kommer til å produsere ulike former for output, slik som figurer og tabeller. Vi må da ha kontroll på hva R bruker som gjeldende mappesti (“working directory”) der filer som skal leses inn ligger, og der ulike output-filer havner. Vi kan bruke funksjonen getwd() til å sjekke hva som er gjeldende mappesti. For å forandre mappestien kan vi bruke menysystemet (Session -&gt; Set Working Directory -&gt; Choose Directory), eventuelt funksjonen setwd() med ønsket mappesti som argument. Oppgave: Pass på at du har gjort følgende før du går videre til neste leksjon: Du har laget en dedikert mappe på datamaskinen din der du skal samle alt materiale som vi bruker i dette kapitlet. Du har lastet ned filen testdata.xls og lagt den i den nye mappen din. Du har endret gjeldende mappesti til denne mappen. Du har bekreftet at gjeldende mappesti nå er korrekt. "],["innlesing-av-data.html", "1.6 Innlesing av data", " 1.6 Innlesing av data Vi leser inn tabellen i excelfilen som en tabell (“data frame”) i R ved hjelp av funksjonen read_xls() i readxl-pakken og ser på noen enkle kommandoer for å jobbe med en slik tabell. # Datasettet er i .xls-formatet, så vi trenger readxl-pakker for å laste det inn # i R. library(readxl) # Inne i denne pakken så er det en funksjon som heter read_excel: read_excel(&quot;testdata.xls&quot;) # Det gikk bra, men for å bruke dette datasettet, så må vi lagre det til en # variabel. testdata &lt;- read_excel(&quot;testdata.xls&quot;) # Skriver ut (toppen av) datasettet. testdata # Nå ser vi datasettet i vinduet oppe til høyre. Vi kan se på det ved å # skrive navnet til datasettet i konsollen, og vi kan hente ut individuelle # kolonner som vektorer ved å bruke dollartegnet $: testdata$X1 # Regner ut gjennomsnittet for X1 og X2: mean(testdata$X1) mean(testdata$X2) # Hvor mange rekker/observasjoner har vi? nrow(testdata) Oppgave: Hvor mange kolonner har datasettet vårt? Kan du finne en måte å skrive ut en vektor som inneholder summen av X1- og X2-kolonnene i datasettet? (Altså, vi vil vite summen av de to første elementene i X1 og X2, summen av de to andre elementene, osv.) Hva er summen av alle tallene i X1- og X2-kolonnene itestdata? Løsning # 1 ncol(testdata) # 2 testdata$X1 + testdata$X2 # 3 sum(testdata$X1 + testdata$X2) "],["statistiske-analyser.html", "1.7 Statistiske analyser", " 1.7 Statistiske analyser Vi ser på et eksempel der vi kjører en enkel statistisk analyse (en \\(t\\)-test) på datasettet vårt, og hvordan vi kan gjøre ulike valg ved å endre argumenter i funksjonskallet. Vi bruker også hjelpefilene til å lese mer om funksjonen vi bruker. # En grunnleggende t-test for likhet mellom to forventningsverdier: t.test(testdata$X1, testdata$X2) # Antar lik varians og kjører tosidig test (dette kommer vi tilbake til): t.test(testdata$X1, testdata$X2, var.equal = TRUE, alternative = &quot;two.sided&quot;) # Du kan lese mer om enhver funksjon i R, inkludert om alle mulige argumenter og # eksempler på bruk, ved å skrive et spørsmålstegn foran funksjonsnavnet i # konsollen: ?t.test # Kanksje ønsker vi å hente ut p-verdien for testen og bruke den til noe senere. # Vi kan lagre testresultatet i en variabel først, og så bruke $-tegnet til å hente # ut p-verdien på følgende måte: testresult &lt;- t.test(testdata$X1, testdata$X2, var.equal = TRUE, alternative = &quot;two.sided&quot;) testresult$p.value # Hvilken annen informasjon finner vi i dette testobjektet? str(testresult) Oppgave: Hva er verdien av testobservatoren (test statistic) i testen som vi gjorde i denne videoen? Hint: Bruk hjelpefilene til t.test()-funksjonen. Løsning test_result$statistic "],["r-plotting.html", "1.8 Plotting", " 1.8 Plotting Vi lager vår første figur i R ved å bruke den innebygde plot()-funksjonen. Vi går så over til å se hvordan vi kan lage det samme plottet ved å bruke ggplot-pakken, som er det vi kommer til å bruke til å lage figurer i dette kurset. Vi ser også hvordan vi kan gå frem for å lagre plottet som en pdf-fil i arbeidsmappen vår. # Et viktig element i dataanalyse er å lage grafiske presentasjoner av datasett. # La oss lage et enkelt spredningsplott av våre to variabler ved å bruke den # innebygde plottefunksjonen i R. plot(testdata$X1, testdata$X2) # Gjør noen justeringer: plot(testdata$X1, testdata$X2, pch = 20, bty = &quot;l&quot;, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;) # I dette kurset skal vi heller bruke ggplot2-pakken til plotting: library(ggplot2) # Her er koden for å lage et enkelt spredningsplott for X1 og X2-kolonnenene i # datasettet vårt: ggplot(testdata, aes(x = X1, y = X2)) + geom_point() # Vi lagrer plottet ved å kjøre ggsave-kommandoen rett etter plottekommandoen: ggsave(&quot;testplot.pdf&quot;) # En mer fleksibel måte å gjøre dette på er å lagre plottet i en variabel, og så # gi navnet til plottet inn som et argument i ggsave-funksjonen. p &lt;- ggplot(testdata, aes(x = X1, y = X2)) + geom_point() ggsave(&quot;testplot.pdf&quot;, p) # På den måten så kan vi lagre plottet p når som helst, vi trenger ikke # nødvendigvis gjøre det rett etter plottekommandoene. Oppgave: Klarer du, for eksempel ved å søke etter relevante ggplot-kommandoer på nettet, å få prikkene i plottet til å bli større, og samtidig gjøre dem blå? Løsning ggplot(testdata, aes(x = X1, y = X2)) + geom_point(colour = &quot;blue&quot;, size = 5) "],["script.html", "1.9 Script", " 1.9 Script I stedet for å skrive kommandoene rett inn i konsollen, hopper vi nå over til teksteditoren i RStudio og lager et script i stedet. Her kan vi samle alle kommandoene våre i en fil, som vi kan lagre og kjøre igjen senere. Vi ser også hvordan vi enkelt kan kjøre enkeltlinjer i scriptet vårt i R-konsollen ved hjelp av Ctrl-Enter (Command-Enter på Mac). Vi ser at vi kan skrive kommentarer i scriptene våre ved å bruke #-tegnet, som kan være nyttig for å holde oversikten. Til slutt lagrer vi scripet i arbeidsmappen. Oppgave: Pass på at du nå har lagret scriptet som en .R-fil i mappen som vi laget for denne R-leksjonen. Lukk RStudio. Naviger så til denne mappen i filutforskeren og dobbelklikk på skriptet. Forhåpentligvis åpnes RStudio nå (Hvis ikke, eller hvis filen åpnes i det som heter R GUI, høyreklikker du på filen og velger “Åpne i”, og deretter RStudio. Du kan også gjerne sette RStudio som standarsprogram for .R-filer). Finn ut hva gjeldende arbeidsmappe nå er i RStudio. Hva skjedde nå? Hvorfor er dette nyttig? Løsning Når vi åpner RStudio ved å dobbeltklikke på skriptfilen, så blir arbeidsstien satt automastisk til mappen der skriptfilen ligger. Dette er veldig nyttig når vil kommer tilbake og skal jobbe videre med prosjektet vårt. "],["pipe.html", "1.10 Pipe-operatoren %&gt;% og enkel datavask", " 1.10 Pipe-operatoren %&gt;% og enkel datavask Vi går gjennom to nyttige programmeringsteknikker som begge er inneholdt i dplyr-pakken: Pipe-operatoren %&gt;%. som gjør at vi kan skrive opp en sekvens av funksjonskall i den rekkefølgen de skal kjøres, og uten å miste oversikten i et hav av paranteser. Funksjoner for enkel datavask: select() for å velge ut kolonner i et datasett, filter() for å filtrere rader basert på kriterier, og mutate() for å lage nye kolonner. Her er et par ekstra tips i denne sammenhengen: Når du bruker select(), så kan du bruke minustegnet for å spesifisere kolonner som du ikke vil ha med: testdata %&gt;% select(-X1). I filter()-funksjonen så kan vi bruke andre kriterier enn større enn (&gt;): &lt; = “mindre enn” &gt;= = “større enn eller lik” &lt;= = “mindre enn eller lik” == = “er lik” != = “er ikke lik” # Vi kan bruke klammeparantesen til å hente ut enkelttall, kolonner eller rader: testdata[1,2] testdata[1,] testdata[,2] testdata[, c(&quot;X1&quot;, &quot;X2&quot;)] # Vi kan også filtrere ut rader basert på kriterier: testdata[testdata$X1 &gt; 100,] # Eller vi kan legge til en ny kolonne, som er summen av to kolonner testdata$sum &lt;- testdata$X1 + testdata$X2 # Det finnes en enklere måte å gjøre dette på ved hjelp av den såkalte # &quot;pipe&quot;-operatoren og egne funksjoner for datamanipulasjon. Begge deler # er inkludert i dplyr-pakken, som du eventuelt på installere: # install.packages(&quot;dplyr&quot;) library(dplyr) # Det første konseptet er pipen, og det er en veldig fin kommando når vi # skal bruke flere funksjoner etter hverandre. La oss si at vi skal regne ut # logaritmen til kvadratroten til 2. I base R kan vi skrive: log(sqrt(2)) # Kvadratroten av 2 skrevet som en pipe blir 2 %&gt;% sqrt # ... og logaritmen til kvadratroten til 2 blir da 2 %&gt;% sqrt %&gt;% log # Dette kan vi lese fra venstre mot høyre i den rekkefølgen det skjer, og vi trenger # ikke holde styr på flere lag med paranteser. # Dette er en teknikk som gjør det veldig greit å jobbe med tabeller av data, og # vi skal se på tre veldig sentrale funksjoner i dplyr-pakken: # Bruk &quot;select()&quot; til å velge kolonner i et datasett: testdata %&gt;% select(X1, X2) # Bruk &quot;filter()&quot; for å filtrere ut rader basert på kriterier: testdata %&gt;% filter(X1 &gt; 100) # Bruk &quot;mutate()&quot; for å lage nye kolonner, gjerne basert på de vi allerede har: testdata %&gt;% mutate(sum = X1 + X2) # Legg merke til at vi her bare skriver navnet på datasettet en gang, nemlig helt i # starten. Vi kan også kjøre flere slike pipes etter hverandre, og lagre resultatet # i en ny variabel: ny_testdata &lt;- testdata %&gt;% select(X1, X2) %&gt;% filter(X1 &gt; 100) %&gt;% mutate(sum = X1 + X2) Oppgave: Basert på testdata, lag en tabell som bare inneholder radene der A1-kolonnen er lik 1 og A2-kolonnen er lik 0, men der vi bare skal ha med kolonnene X1 og X2. Lag til slutt en ny kolonne som inneholder differansen av tallene i X1 og X2-kolonnene. Løsning testdata %&gt;% filter(A1 == 1) %&gt;% filter(A2 == 0) %&gt;% select(X1, X2) %&gt;% mutate(differanse = X1 - X2) "],["r-ekstra.html", "1.11 Oppsummering og ekstra oppgaver", " 1.11 Oppsummering og ekstra oppgaver I denne modulen har vi gått gjennom noen helt grunnleggende funksjoner i R. Du har lært at R er navner på et programmeringsspråk, RStudio er navnet på et program der vi kan skrive og kjøre R-kode, og identifisert fire forskjellige vindu i RStudio: konsollen (der R-koden kjøres), teksteditoren (der vi skriver script), samt to vinduer der vi kan se en oversikt over hva som er i dataminnet og få opp plott og figurer som vi lager. Videre har du kjørt noen enkle kommandoer, lagret tall og vektorer ved hjelp av variabelnavn, prøvd ut noen innebygde R-funksjoner for å regne ut f.eks. gjennomsnitt og standardavvik av tallvektorer, laget et spredningsplott, lært hva et working directory (arbeidsmappe) er, og installert R-pakker, f.eks readxl som vi brukte den til å lese inn et lite datasett i R. Til slutt har du kjørt en \\(t\\)-test, skrevet et script (et lite program om du vil) der vi har lagret flere av kommandoene over i en tekstfil, og lært om pipe-operatoren og enkel datavask ved hjelp av funksjoner i dplyr-pakken. Dersom du har fulgt modulen selv har du nå kanskje skrevet et lite script i tekstvinduet som ser ut omtrent som koden under. Når du har gjort alt riktig, skal du nå kunne kjøre gjennom disse kodelinjene uten feilmeldinger ved hjelp av Ctrl-Enter. Dette er helt grunnleggende (Spør om hjelp! Gi hjelp!). Har du problemer her, sørg for å få dem ordnet. Spør først en medstudent om hjelp, og deretter eventuelt studentassistent eller foreleser. Studenter som har god erfaring med data og/eller programmering, kan lære mye av å hjelpe medstudenter løse feilmeldinger. # Introduksjon til R # ------------------- # Laster inn nødvendige pakker library(readxl) library(ggplot2) # Laster inn datasettet testdata &lt;- read_xls(&quot;testdata.xls&quot;) # Gjør t-testen til spørsmål F i den første dataøvingen testresultat &lt;- t.test(testdata$X1, testdata$X2, var.equal = TRUE, alternative = &quot;two.sided&quot;) # Skriver ut resultatet av denne t-testen testresultat # Lager et plott av variabelen X1 mot X2 p &lt;- ggplot(testdata, aes(x = X1, y = X2)) + geom_point() # Lagrer plottet ggsave(&quot;testplot.pdf&quot;, plot = p) Lagre scriptet ditt. I RStudio velger du File -&gt; Save og trykker Ok dersom det kommer opp et vindu om character encoding e.l. Finn en fornuftig plassering (gjerne i samme mappe som øvelsesdatasettet) og gi filen et fornuftig navn. Standard filending for R-script er .R, men det er skjult for de fleste Windowsbrukere. Lukk RStudio. Du kan nå åpne skriptfilen i RStudio igjen. Enten ved å dobbeltklikke på den, eller ved å åpne RStudio, velge File -&gt; Open file, og så videre (dersom skriptet ikke allerede ligger åpnet). Du kan også åpne skriptfilen i en hvilken som helst notatbok (Notebook e.l.) og se at det er en helt standard, ren tekstfil. Hva er fordelen med å lagre en analyse som et skript versus å gjøre ting i et menydrevet grafisk grensesnitt? Løsning Når vi lagrer koden vår i et skript sørger vi for at hele analysen vår er lagret, ikke bare resultatene. Med andre ord, dersom du på et senere tidspunkt ønsker å komme tilbake til et analyseprosjekt og gjøre noen enkle forandringer, så er det fort gjort å gjøre det i skriptet, og så kjøre hele analysen på nytt. Dersom du i stedet hadde brukt et menydrevet system for å gjennomføre analysen (pek og klikk) kunne du risikere å måtte gjøre alt sammen på nytt (hvis du da husker hvordan du gjorde det), fordi du ikke like enkelt kan lagre hvert eneste museklikk. Vi skal nå pynte på plottet og gjøre det riktig pent. Det gjør vi ved å legge til nye linjer i ggplot-kommandoen. Erstatt den nest siste linjen i skriptet med kommandoen under, og se at du får en figur omtrent som den som følger under det igjen (vi bruker aksetitler i henhold til oppgavene i den første datalabben, der vi får vite at datasettet representerer kvalitet på kaffeavlingen før og etter en omlegging i produksjonsmetode): ggplot(testdata, aes(x = X1, y = X2)) + geom_point(size = 2) + xlab(&quot;Produksjonsmetode 1&quot;) + ylab(&quot;Produksjonsmetode 2&quot;) + theme_classic() Merk at vi bruker “+”-tegnet til å legge til flere “lag” med grafiske egenskaper til plottet. Hvert “lag” består av en funksjon, som ofte kan ta argumenter; f.eks. brukes funksjonen geom_point() til å lage prikker, og så kan vi f.eks. bruke argumentet size til å styre størrelsen på prikkene. Kan du finne ut hva hvert enkelt av disse “lagene” gjør? Hint: ta bort en linje av gangen, og se hva som skjer. Pass på at det er et pluss mellom hvert lag. Prøv å endre på noen av lagene eller legg til nye. For eksempel kan du lage en tittel ved å legge til funksjonen ggtitle() som et lag, og du kan endre aksetitlene. Prøv også å bruke argumentet shape i geom_point() til å bytte ut prikkene med en annen form. Det finnes flere andre “tema” i tillegg til theme_classic(), f.eks. theme_bw(), theme_dark(), etc. Forslag Prøv for eksempel dette: ggplot(testdata, aes(x = X1, y = X2)) + geom_point(size = 2, shape = 4) + ggtitle(&quot;Produksjonskvalitet&quot;) + xlab(&quot;Ny aksetittel&quot;) + ylab(&quot;Enda en aksetittel&quot;) + theme_light() Det følger med omfattende dokumentasjon med R. Du kan lese om alle R-funksjoner ved å skrive ? før funksjonsnavnet i konsollen. Prøv for eksempel å skrive ?mean i konsollen og trykk enter. "],["grunnleggende-statistikk.html", " 2 Grunnleggende statistikk", " 2 Grunnleggende statistikk I denne modulen introduserer vi en del grunnleggende statistiske begreper. Mye vil oppleves som repetisjon, mens noe vil være nytt. Noe er veldig praktisk ved at vi kan bruke det direkte i eksempler, mens andre ting er mer teoretisk av natur. Felles for det vi skal se på her er at vi kommer til å bruke mange av begrepene vi lærer senere i kurset. I videoforelesningene går vi gjennom noen slides, og vi skriver et R-skript. Du kan laste disse ned ved å klikke på lenkene under: Slides til “Grunnleggende statistikk” R-script til “Grunnleggende statistikk” TIPS: Hvis du ønsker å laste ned lysbildene som PDF trykker du på linken over, velger “Skriv ut”, og så skriver du ut som PDF. Før du gjør det bør du scrolle gjennom alle sidene slik at ligningene vises korrekt. "],["deskriptiv-statistikk.html", "2.1 Deskriptiv statistikk", " 2.1 Deskriptiv statistikk 2.1.1 Videoforelesninger 2.1.2 Kommentarer Deskriptiv statistikk handler ikke om analyse eller regning, men om å presentere kompleks informasjon på en effektiv måte. Det er altså noe ganske annet enn det vi ellers snakker om i kurset, men det er likevel et av de nyttigste læringspunktet vi har. Hvem kan ikke regne med å måtte presentere tall og resultater i løpet av sin karriere? Eller selge inn forslag og planer for overordnede i håp om å bli lyttet til? Det kan være direkte avgjørende for din egen gjennomslagskraft at du er i stand til å produsere overbevisende tabeller og figurer i slike situasjoner, og det er det dette temaet handler om. I læreboken er det kapitlene 2–4 som behandler deskriptiv statistikk, men det er veldig Excel-fokusert, som ikke er så relevant for oss. Det er likevel ikke dumt å lese gjennom stoffet for å se hva det går i, og legg spesielt merke til følgende punkter: Ulike datatyper i avsnitt 2-1. 3-4: The art and science of graphical presentations. Hva er det som gjør en grafisk illustrasjon god? Prøv å ta inn over dere all informasjonen som vi lett kan lese ut av bildet på side 75 om Napoleons felttog mot Moskva. Her presenteres informasjon om tid, antall, geografi og temperatur på en helt eksepsjonelt effektiv måte! Videre er det noen grelle eksempler på hvordan vi kan bruke grafiske virkemidler til å gi skjeve fremstillinger. I videoforelesningen gir vi flere eksempler på dette. Kapittel 4 går litt mer i dybden om numeriske deskriptive teknikker, som gjennomsnitt, median, standardavvik, korrelasjon, osv. Dette skal være dekket greit i forelesningen, men boken går litt lenger. Det kan være en fin øvelse å kikke på eksemplene i læreboken og forsøke å gjenskape noen av Excel-figurene i R. Se på eksempel 3.2, der man skal lage to histogrammer over historiske avkastninger for to ulike investeringsstrategier. Vi leser inn datasettet (last ned fra Canvas) som under og kikker på det: ## Warning: package &#39;readxl&#39; was built under R version 4.3.3 library(readxl) returns &lt;- read_xlsx(&quot;Xm03-02.xlsx&quot;) returns ## # A tibble: 50 × 2 ## `Return A` `Return B` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30 30.3 ## 2 -2.13 -30.4 ## 3 4.3 -5.61 ## 4 25 29 ## 5 12.9 -26.0 ## 6 -20.2 0.46 ## 7 1.2 2.07 ## 8 -2.59 29.4 ## 9 33 11 ## 10 14.3 -25.9 ## # ℹ 40 more rows Hver stategi har sin kolonne. Merk at variabelnavnene har mellomrom i seg, noe som er upraktisk når vi jobber med et seriøst programmeringsspråk. En god vane er å rett og slett gi dem nye navn, ved f.eks. å kjøre colnames(returns) &lt;- c(\"returnA\", \"returnB\"), eller så må vi alltid referere til variabelnavnene ved å bruke slike “backticks” som vi ser under. Vi kan lage to enkle histogrammer slik vi gjorde det i forelesningen: library(ggplot2) ggplot(returns, aes(x = `Return A`)) + geom_histogram(bins = 10) ggplot(returns, aes(x = `Return B`)) + geom_histogram(bins = 10) Figur 2.1: To histogrammer Her er noen kontrollspørsmål som du kan prøve deg på: Hva er forskjellen på deskriptiv statistikk og statistisk inferens? Deskriptiv statistikk kan gjøres grafisk eller numerisk, eventuelt som tabeller av ulike numeriske mål. Nevn noen fordeler og ulemper man må veie mot hverandre når vi skal velge mellom grafisk og numerisk deskriptiv statistikk. "],["utvalg-og-estimering.html", "2.2 Utvalg og estimering", " 2.2 Utvalg og estimering You can, for example, never foretell what any one man will do, but you can say with presicion what an average number will be up to. Individuals vary, but percentages remain constant. So says the statistician. — Sherlock Holmes 2.2.1 Videoforelesninger 2.2.2 Kommentarer I videoforelesningene over går vi gjennom noen sentrale begreper i statistikk. Noen av dem skal vi bruke mye i fortsettelsen, mens andre er ment for å gi dere et solid teoretisk fundament når vi etter hvert skal begi oss ut på anvendt statistikk. Vi startet med å sette opp en liten agenda. Som et første steg kan du kikke på, og notere ned noen setninger til, disse punktene og se om du har fått med deg hva de betyr: Samplingfordelinger Forventning/varians Sentralgrenseteoremet Hva er samplingfordelingen til et gjennomsnitt? Hva er samplingfordelingen til en andel? Forventningsrett Konsistens I Boken er det kapittel 9 (Sampling distributions) og 10 (Introduction to estimation) som gjelder. Kapittel 6–8 omhandler stoff skal skal være greit dekket i MET2 (Sannsynlighet, fordelinger, stokastiske variable, osv.), men det kan være nyttig å skumme gjennom likevel hvis disse begrepene ligger langt bak i bevissheten din. Kapittel 9 starter med å diskutere samplingfordelingen til et gjennomsnitt. Dette er nyttig lesestoff, men de viktigste punktene er som følger: Dersom observasjonene \\(X_1, X_2, \\ldots, X_n\\) er normalfordelt, er også gjennomsnittet \\(\\overline X = \\frac{1}{n}\\sum_{i=1}^n X_i\\) normalfordelt. Dersom E\\((X_i) = \\mu\\) og Var\\((X_i) = \\sigma^2\\) for alle \\(i = 1,\\ldots,n\\), er E\\((\\overline X)=\\mu\\) og Var\\((\\overline X) = \\sigma^2/n\\). Dette regnet vi ut formelt. Dersom \\(n\\) er stor, er \\(\\overline X\\) tilnærmet normalfordelt, uavhengig av fordelingen til den enkelte \\(X_i\\). Dette følger av sentralgrensesetningen. Dette står i en boks på slutten av seksjon 9-1a. Hvor stor må \\(n\\) være for at denne tilnærmingen er god nok? Det finnes ikke et entydig svar på, men når vi passerer 50-100 observasjoner kan vi i våre MET4-problemer gjerne si at \\(n\\) er «stor nok». I 9-1b og 9-1c brukes sentralgrenseteoremet til å regne på normalsannsynligheter i MET2-stil. I 9-1d er det noen Excel-instruksjoner som du kan hoppe over hvis du vil. Tekstboksen i 9-2c oppsummerer det vi fant ut om samplingfordelingen til en observert andel. I seksjon 9-3 snakkes det om samplingfordelingen til differansen av to gjennomsnitt. Vi gikk ikke gjennom det eksplisitt i forelesningen, men det er ikke noe substansielt nytt her. Vi skal bruke dette reultatet i neste modul når vi skal sammenligne to gjennomsnitt. I seksjon 9-4 får vi forklart hva vi skal bruke samplingfordelinger til fremover. Bør leses. Kapittel 10 omhandler estimering, dvs hvordan vi bruker data til å «gjette» på verdien til en ukjent parameter. Vi forsøkte i forelesningen å gi litt intuisjon til begrepene forventningsrett estimator, variansen til en estimator, og konsistens. Vi kan lage et punktestimat av en forventningsverdi ved å ta gjennomsnittet av observasjoner, og vi kan lage et konfidensintervall ved å følge oppskriften i boksen på s. 316 (i 11. utgave). I eksempel 10.1 har vi 25 observasjoner fra en normalfordeling. Oppgaven er å estimere forventningsverdien med et tilhørende 95% konfidensuntervall. Pass på at du forstår den manuelle utregningen. I stedet for å bruke Excel (eller taste alle disse tallene inn på en kalkulator) kan du skrive et lite R-script som gjør det samme: # Vi skriver inn datasettet i en vektor demand &lt;- c(235, 374, 309, 499, 253, 421, 361, 514, 462, 369, 394, 439, 348, 344, 330, 261, 374, 302, 466, 535, 386, 316, 296, 332, 334) # Vi trenger 4 verdier for å regne ut konfidensintervallet: gj.snitt &lt;- mean(demand) # Regner ut gjennomsnittet z &lt;- 1.96 # Denne finner vi i tabellen sigma &lt;- 75 # Oppgitt i oppgaven n &lt;- length(demand) # Antall observasjoner # Vårt estimat av forventningsverdien er bare gjennomsnittet. # Regner ut nedre og øvre grense i konfidensintervallet (LCL, UCL): LCL &lt;- gj.snitt - z*sigma/sqrt(n) UCL &lt;- gj.snitt + z*sigma/sqrt(n) # Samler de tre tallene i en vektor og skriver ut: c(LCL, gj.snitt, UCL) ## [1] 340.76 370.16 399.56 I seksjon 10-2a forsøker boken å forklare fortolkningen av et konfidensintervall. Hovedpoengene her er at: Et 95%-konfidensintervall skal ikke tolkes som «sannsynligheten for at den sanne parameterverdien ligger i intervallet er 95%». Den korrekte tolkningen er: «Dersom vi hadde hatt tilgang til å trekke nye utvalg fra populasjonen med like mange observasjoner og bruker dem til å regne ut nye konfidensintervaller, vil 95 av 100 intervaller inneholde den sanne parameterverdien». Forskjellen på disse formuleringene er meget subtil, så subtil faktisk at det ikke er åpenbart at det er særlig god pedagogikk å peke på den. Problemet med den første formuleringen er at vi der kan få inntrykk av at det er den sanne parameterverdien som er stokastisk og avhengig av datasettet vi observerer, mens det strengt tatt er grensene til konfidensintervallet som er tilfeldige, og altså avhengige av datasettet. Det kommer klarere frem i den andre formuleringen. Bredden til et konfidensintervall er altså et uttrykk for usikkerhet, eller motsatt: presisjon. Seksjon 10-2b og 10-2c kan skummes raskt gjennom. Seksjon 10-3 handler om at vi først bestemmer oss for et presisjonsnivå (dvs bredde på konfiensintervallet) \\(B\\), og så regner ut hvor mange observasjoner vi trenger for å oppnå det. Vi kommer frem til en formelen \\[n = \\left(\\frac{z_{\\alpha/2}\\sigma}{B}\\right)^2,\\] men problemet i praksis er at vi gjerne ikke kjenner \\(\\sigma\\), og vi kan heller ikke estimere den fordi vi ikke har samlet inn data enda. Løsningen er at vi enten på bruke fornuften, eller eventuelt et tidligere estimat av \\(\\sigma\\) dersom det er tilgjengelig. Når du har vært gjennom dette stoffet skal du forhåpentligvis være i stand til å diskutere følgende spørsmål med f.eks. en medstudent: Hva er en samplingfordeling? Hva sier sentralgrenseteoremet? Hva mener en statistiker når hen sier at “gjennomsnittet konvergerer som \\(1/\\sqrt{n}\\)”? Hva er samplingfordelingen til et gjennomsnitt? Hva er samplingfordelingen til en andel? Hva vil det si at et estimator er forventningsrett? Hva vil det si at et estimator er konsistent? 2.2.3 Ekstra øving i R Som demonstrert i forelesningen kan vi i R simulere standard normalfordelte observasjoner (dvs normalfordelte observasjoner med \\(\\mu = 0\\) og \\(\\sigma^2 = 1\\)) med kommandoen rnorm(n), der n er antallet observasjoner vi ønsker. For eksempel kan vi kjøre følgende kode for å generere 10 observasjoner (du vil helt sikkert få andre verdier): n &lt;- 10 rnorm(n) ## [1] -0.3885254 -0.2163663 -0.3417094 1.1489744 -1.0275367 0.3581634 ## [7] -0.7275189 -0.4045447 0.8437766 0.5116078 Ved å skrive mean(dnorm(n)) i stedet regner vi ut gjennomsnittet av observasjonene direkte. La oss gjøre dette 100 ganger og notere ned gjennomsnittet hver gang. I stedet for å gjøre det manuelt, kan vi skrive et lite program som gjør dette for oss ved å bruke en for-løkke. Det er ikke nødvendig (eller pensum) å forstå akkurat hvordan dette fungerer, men dersom du kjører følgende linjer vil du få en ny vektor gj.snitt som inneholder 100 slike gjennomsnitt: gj.snitt &lt;- rep(NA, 100) for(i in 1:100) { gj.snitt[i] &lt;- mean(rnorm(n)) } Skriv ut denne vektoren og kontroller at det ser korrekt ut. Vi husker at funksjonen sd() regner ut standardavviket til en vektor. Hvilket tall forventer du å få ut dersom du nå kjører sd(gj.snitt) i konsollen? Stemmer det? Hint Standardavviket til de enkelte observasjonene er \\(\\sigma = 1\\), og standardavviket til et gjennomsnitt bestående av 10 observasjoner er \\(\\sigma/\\sqrt{n} = 1/\\sqrt{10} \\approx 0.32\\). Med andre ord skal det empiriske standardavviket sd(gj.snitt) være omtrent lik 0.32, pluss/minus en estimeringsfeil. Du kan gjerne regne ut 1000 gjennomsnitt i stedet for 100 ved å erstatte erstatte 100 med 1000 på to steder i koden over. Stemmer det bedre da? Hint gj.snitt &lt;- rep(NA, 1000) # Lager en tom vektor med 1000 plasser for(i in 1:1000) { # Fyller hver plass med et gjennomsnitt av gj.snitt[i] &lt;- mean(rnorm(n)) # 10 standard normalfordelte observasjoner. } Prøv å forklare. Svar Dette er ganske enkelt, men også litt vanskelig på en inception-aktig måte. På samme måte som at gjennomsnittet blir en mer og mer presis estimator for forventningsverdien når vi øker antall observasjoner (målt ved at standardavviket \\(\\sigma/\\sqrt{n}\\) blir mindre når antall obserasjoner \\(n\\) blir større), blir det empiriske standardavviket en mer og mer presis estimator av det sanne standardavviket når vi øker antall observasjoner. Altså; det empiriske standardavviket har også et standardavvik som går mot null som \\(1/\\sqrt{n}\\)  "],["oppgaverdeskriptiv.html", "2.3 Oppgaver", " 2.3 Oppgaver 2.3.1 Standard oppgaver Beskriv kort forskjellen mellom deskriptiv statistikk og statistisk inferens. Løsning Beskrivende statistikk bearbeider og presenterer data for å belyse faktiske forhold. Statistisk inferens, også kalt slutningsstatistikk, er å gjøre slutninger om populasjonen basert på det vi observerer i utvalget. Du skal spille kron og mynt. Motspilleren din er eieren av mynten, og hun påstår at den er rettferdig. Det vil si at om man flipper mynten svært mange ganger vil den gi et likt antall kron som mynt. Beskriv et eksperiment som tester denne påstanden. Hva er populasjonen i eksperimentet? Hva er utvalget? Hva er parameteren? Hva er estimatoren? Beskriv kort hvordan statistisk inferens kan bli brukt til å teste påstanden til motspilleren din. Løsning Kast mynten ett gitt antall ganger, for eksempel 100 ganger. Registrer hvor mange ganger den viser mynt og hvor mange ganger den viser kron. Populasjonen er det teoretiske resultatet av å kaste mynten uendelig mange ganger og registrere hvor mange ganger den viser mynt og hvor mange ganger den gir kron. Utvalget er antall mynt og kron i eksperimentet. Parameteren er andelen mynt (eller kron) i populasjonen. Estimatoren er den registrerte andelen mynt (eller kron) i eksperimentet. Estimatoren kan brukes til å vurdere om mynten faktisk er rettferdig. I neste modul av kurset skal dere lære om hypotesetesting og om hvilken hypotesetest man kan utføre for å avgjøre om man kan forkaste nullhypotesen om at mynten er rettferdig. En fabrikant av maskindeler påstår at mindre enn 15% av produktene er defekte. Når 1000 deler ble trukket fra en stor produksjon, var 12% defekte. Hva er populasjonen i dette tilfellet? Hva er utvalget? Hva er parameteren? Hva er estimatoren? Forklar kort hvordan estimatoren kan bli brukt til å gjøre inferens om parameteren for å undersøke påstanden om at 15% av produktene er defekte. Løsning Populasjonen er alle de aktuelle maskindelene som lages av fabrikanten. Utvalget er de 1000 delene som ble trukket fra en stor produksjon. Parameteren er andelen defekte maskindeler i alle de aktuelle maskindelene som produseres. 15% er den påståtte parameter. Estimatoren er andelen defekte maskindeler i utvalget. 12% er den observerte estimatoren. Vi kan estimere at andelen defekte maskindeler i populasjonen er 12%. Ved å bruke statistisk inferens kan vi undersøke om vi har nok statistisk bevis til å avvise påstanden om at den sanne andelen defekte maskindeler er 15%. Avgjør om de følgende datasettene inneholder forholdsdata, intervalldata, ordinale data eller dikotome og nominale data. Avgjør også hvilke regneoperasjoner som er tillatt. Antall mil en gruppe joggere løper hver uke Starlønnen til nyutdannede fra masterprogrammet til NHH Månedene et selskaps ansatte velger å ta ut ferie Temperatur i Celsius Bokstavkarakterene til studentene i MET4 De daglige aksjeprisene til Tesla Klesstørrelser (S, M, L) Antallet Toyota som er importert månedlig til USA de siste 5 årene Stillingene i Kjernestyret i NHHS (Leder, Prosjektansvarlig, Internansvarlig osv) Dato Løsning Forholdsdata: =, ≠, &lt;, &gt;, +, -, *, / Forholdsdata: =, ≠, &lt;, &gt;, +, -, *, / Nominale data: =, ≠ Intervalldata: =, ≠, &lt;, &gt;, +, - Ordinale data: =, ≠, &lt;, &gt; Forholdsdata: =, ≠, &lt;, &gt;, +, -, *, / Ordinale data: =, ≠, &lt;, &gt; Forholdsdata: =, ≠, &lt;, &gt;, +, -, *, / Nominale data: =, ≠ Intervalldata: =, ≠, &lt;, &gt;, +, - Skattebetalere som fyller ut egenmeldingen sin blir spurt de følgende spørsmålene. Hvilken type data utgjør svarene og hvilke regneoperasjoner er tillatt? Er det første gang du fyller ut egenmeldingen din? Hvor lang tid brukte du på å fylle ut egenmeldingen? Hvor enkelt/vanskelig var det å fylle ut egenmeldingen? (Veldig enkelt, nokså enkelt, hverken enkelt eller vanskelig, nokså vanskelig, veldig vanskelig) Løsning Nominale data: =, ≠ Intervalldata: =, ≠, &lt;, &gt;, +, - Ordinale data: =, ≠, &lt;, &gt; Du får oppgitt at gjennomsnittlig startlønn for NHH studenter etter endt utdanning er 560 000kr. Median startlønnen er 520 000kr. Hva forteller disse tallene deg? Løsning Selv om den gjennomsnittlige startlønnen er 560 000kr, så får halvparten av arbeidstakerne 520 000kr eller mindre. Det forteller også at det er noen arbeidstakere som drar gjennomsnittet opp. En videregående skole rapporterer at gjennomsnittsfraværet i antall dager i løpet av ett skoleår er 10 dager. Medianen er 3,5 dager. Hva forteller de to målene oss, og hvilke av de to beskriver best fraværet? Hvorfor? Løsning Det gjennomsnittlige fraværet er høyere enn medianen. Dette forteller oss at noen elever har høyt fravær som drar gjennomsnittet opp. Disse elevene er ikke representative for det store flertallet. Halvparten av elevene har 3.5 eller mindre dagers fravær. Derfor er medianen et mer robust sentermål, som ikke blir påvirket av store uteliggere. Du skal ta en språktest før utveksling og kan velge mellom to ulike språkkurs som skal forberede deg til testen. Begge kursene har rapportert kvartilene til testresultatene oppgitt i antall poeng studentene deres fikk på testene: Kvartil Kurs nummer 1 Kurs nummer 2 Første kvartil 230 225 Andre kvartil 240 235 Tredje kvartil 250 270 Hvordan tolker du kvartilene og hvilket kurs ville du ha valgt? Løsning Kvartilene kan fortelle oss noe om nivået og variasjonen i poengsummene til studentene som har tatt de to kursene. Vi ser at kurs nummer 1 har en symmetrisk fordeling med en median som er høyere enn kurs nummer 2. På en annen side har poengfordelingen til kurs nummer 2 en lengre høyre hale hvor tredje kvantilen tilsier at hele 25 % vil få 270 poeng eller mer. Det tilsvarende tallet for kurs nummer én er 250. Samlet sett er det kanskje “tryggere” med kurs nummer 1, mens det potensielle læringsutbytte har “større tak” i kurs nummer to. MET4 ble et år rettet av to sensorer og et boxplot av poengene de ga sine respektive kandidater var som følger: Hva kan du si karakterfordelingen til de to sensorene? Løsning De to sensorene har begge en median rundt 60 poeng, så halvparten av alle studenter får under 60 poeng og den andre halvdelen får over 60 poeng. Men Sensor 2 har større spredning i sine poeng og vil følgelig fordele studentene ut mer på karakterskalaen sammenlignet med Sensor 1. Du er kvalitetsansvarlig for et produkt og samler inn data på levetiden i måneder til produktet. Et histogram over disse dataene ser ut som følger: Hva sier histogrammet deg om produktet? Løsning Her ser vi en slags badekar-formet fordeling som ofte går igjen for levetiden til produkter. En rekke produkter går i stykker de første månedene. Dette kan være pga av produksjonsfeil som da slår ut tidlig i bruksperioden. Resten av produktene har en mer normalfordelt levetid med et gjennomsnitt rundt 120 måneder. Kovariansen til to variabler fra et utvalg har blitt beregnet til -150. Videre får du vite at empirisk standardavviket til den ene variabelen er 12 og 16 for den andre. Regn ut (utvalgs-) korrelasjonskoeffisienten \\(r\\) og bruk denne til å beskrive sammenhengen mellom variablene. Løsning Utvalgs korrelasjonskoeffisient er definert som utvalgskovariansen mellom to variabler delt på de empiriske standardavvikene til variablene. \\[r=s_{xy}/(s_x s_y )=-150/(16\\cdot12)=-0,7813\\] Korrelasjonskoeffisienten er alltid mellom -1 og 1 og måler graden av lineær sammenheng mellom variablene. Når korrelasjonskoeffisienten er -1 er det en perfekt negativ lineær sammenheng, og når den er 1 vil det si at det er en perfekt positiv lineær sammenheng mellom variablene. Når korrelasjonskoeffisienten er 0 vil det si at ikke er noen lineær sammenheng mellom variablene, men vi kan allikevel ikke si at de er uavhengige. Korrelasjonskoeffisienten -0,78 indikerer en moderat til sterk negativ lineær sammenheng mellom variablene. Betrakt spredningsplottene for datasett a-d under, og par hvert datasett med en av de følgende korrelasjonene: \\[r_1 = -0.37,\\quad r_2 = 0.81,\\quad r_3 = -0.02,\\quad r_4 = -0.96 \\] Løsning Datasett a: \\(r_2 = 0.81\\), Datasett b: \\(r_1 = -0.37\\), Dasett c: \\(r_4 = -0.96\\), Datasett d: \\(r_3 = -0.02\\) Datasett d er et eksempel på at korrelasjonen mellom to variabler kan være svært nær 0, selv om det er en tydelig (ikke-lineær) sammenheng mellom dem. Det som skjer når vi regner ut \\(r\\) i dette tilfellet er at den negative avhengigheten vi ser til venstre i figuren kanselleres av den positive avhengigheten til høyre i figuren. En normalfordelt populasjon har gjennomsnitt 40 og standardavvik 12. Hva sier sentralgrenseteoremet om gjennomsnittet av et utvalg på 100 observasjoner fra denne populasjonen? Dersom populasjonen ikke var normalfordelt, hvordan endrer dette svaret ditt i a? Løsning Dersom populasjonen er normalfordelt så vil også gjennomsnittet av et utvalg av populasjonen være normalfordelt. Gjennomsnittet vil ha forventning \\(40\\) og ha standardavvik \\(12/\\sqrt{100} = 1.2\\). Sentralgrenseteoremet sier at fordelingen til gjennomsnittet vil for alle praktiske formål nærme seg normalfordelingen når antall observasjoner øker, uavhengig av fordelingen til populasjonen. Dette endrer altså ikke konklusjonen om fordelingen til utvalget. La \\(X_1, X_2,\\dots X_n\\) være utfallet av en rekke kast med en tilfeldig terning. Hva er \\(P(X_1 = 1)\\) og \\(P(X_1 = 6)\\)? Hva er \\(P(\\overline{X} = 1)\\) og \\(P(\\overline{X} = 6)\\) dersom \\(n = 2\\)? Hva er \\(P(\\overline{X} = 1)\\) og \\(P(\\overline{X} = 6)\\) dersom \\(n = 5\\)? Løsning \\(P(X_1 = 1) = P(X_2 = 6) = 1/6\\) Det er bare en måte at gjennomsnittet av to kast kan bli \\(1\\) og det er at begge kastene blir \\(1\\): \\(P(\\overline{X} = 1) = P(X_1 = 1, X_2 = 1) = P(X_1 = 1)P(X_2 = 1) = 1/36\\), og tilsvarende for \\(P(\\overline{X} = 6)\\). Det er bare en måte at gjennomsnittet av 5 kast kan bli \\(1\\) og det er at alle fem kastene blir \\(1\\):\\(P(\\overline{X} = 1) = P(X_1 = 1)P(X_2 = 1)P(X_3 = 1)P(X_4 = 1)P(X_5 = 1) = 0.00013\\), og tilsvarende for \\(P(\\overline{X} = 6)\\). Denne oppgaven illustrerer hvordan gjennomsnittet, uavhengig av fordelingen til \\(X\\) som i dette tilfellet er uniformt fordelt på \\({1,2,3,4,5,6}\\), er normalfordelt \\(\\overline{X}\\sim N(\\mu, \\sigma^2/n)\\) og beveger seg mot forventningen til \\(X\\) (som i dette tilfellet er \\(\\mu = 3.5\\)). Det at variansen til gjennomsnittet \\(\\sigma^2/n\\) blir mindre når \\(n\\) øker ser vi er svært logisk her: Det er værre for gjennomsnittet til terningkastene å oppnå “ekstreme” verdier som \\(\\overline{X} = 1\\) eller \\(\\overline{X} = 6\\) siden dette innebærer at alle kastene må enten være 1 eller 6. Derimot er det en hel rekke kombinasjoner av kast som gir et gjennomsnitt nær \\(3.5\\), så slike verdier av gjennomsnittet er mye mer sannsynlig. Under ser du et histogram over gjennomsnittene dersom vi gjentar terningkasteksperimentet med henholdsvis \\(n=1,2, 5\\) kast 2000 ganger. Allerede for \\(n=5\\) ser vi at de fleste gjennomsnitt ligger nær det sanne gjennomsnittet på 3.5. La \\(\\hat{p}=X/n\\) være den estimerte suksessannsynligheten i et binomisk eksperiment \\(X\\) med \\(n=300\\) forsøk og (populasjons) suksessannsynlighet \\(p\\). Hva er sannsynligheten for at \\(\\hat{p}\\) er høyere enn 60% dersom \\(p = 0.5\\)? Gjenta oppgave a med \\(p = 0.55\\). Gjenta oppgave a med \\(p = 0.60\\). Løsning Her bruker vi sentralgrenseteoremet som sier at \\(\\hat{p}\\sim N(p, p(1-p))\\): \\[P(\\hat{p} &gt; 0.60) = P\\left(\\frac{\\hat{p} - p}{\\sqrt{p(1 - p)/n}} &gt; \\frac{0.60 - 0.5}{\\sqrt{0.5(1 - 0.5)/300}}\\right) = P(Z &gt; 3.46)\\approx 0\\]. \\[\\begin{equation*} \\begin{split} P(\\hat{p} &gt; 0.60) &amp;= P\\left(\\frac{\\hat{p} - p}{\\sqrt{p(1 - p)/n}} &gt; \\frac{0.60 - 0.55}{\\sqrt{0.55(1 - 0.55)/300}}\\right)\\\\ &amp;= P(Z &gt; 1.74)= 1 - P(Z &lt; 1.74) = 1 - 0.9591 = 0.0409. \\end{split} \\end{equation*}\\] \\[\\begin{equation*} \\begin{split} P(\\hat{p} &gt; 0.60) &amp;= P\\left(\\frac{\\hat{p} - p}{\\sqrt{p(1 - p)/n}} &gt; \\frac{0.60 - 0.60}{\\sqrt{0.60(1 - 0.60)/300}}\\right)\\\\ &amp;= P(Z &gt; 0)= 0.5\\quad\\text{(p.g.a. $Z$&#39;s symmetri rundt 0)}. \\end{split} \\end{equation*}\\] Vi ser at for \\(n=300\\) vil \\(\\hat{p}\\) sin normalfordeling være svært konsentrert rundt \\(p\\) (variansen er \\(p(1-p)/n\\)) og at en overstigelse på 0.05 eller mer fra den sanne suksessannsynligheten (som i oppgave a.) er svært usannsynlig. Anta at vi har to normalfordelte populasjoner hvor observasjoner fra populasjon 1 følger en \\(N(40, 6^2)\\) og observasjoner fra populasjon 2 følger en \\(N(38, 8^2)\\). Dersom man trekker 25 tilfeldige observasjoner fra hver populasjon, hva er sannsynligheten for at gjennomsnittet til trekningen fra populasjon 1 er større enn gjennomsnittet til trekningen fra populasjon 2? Løsning Vi vet da at \\(\\overline{X}_1 - \\overline{X}_2 &gt; 0\\) og utnytter at denne størrelsen er normalfordelt: \\[\\begin{equation*} \\begin{split} P(\\overline{X}_1 - \\overline{X}_2 &gt; 0) &amp;= P\\left(\\frac{\\overline{X}_1 - \\overline{X}_2 - (40 - 38)}{\\sqrt{\\frac{6^2}{25} + \\frac{8^2}{25}}} &gt; \\frac{0 - (40 - 38)}{\\sqrt{\\frac{6^2}{25} + \\frac{8^2}{25}}} \\right)\\\\ &amp;= P(Z &gt; -1) = 1 - P(Z &lt; -1) = 1 - 0.1587 = 0.8413 \\end{split} \\end{equation*}\\] Produsenten av en kaviar forteller deg at hver tube er reklamert til å veie 65 gram, men at maskinen som produserer kaviaren gir en vekt som er en normalfordelt med gjennomsnitt lik 65,5 gram og standardavvik 3.6 gram. La oss si at du trekker et tilfeldig utvalg av 40 tuber for å undersøke denne påstanden og at det tilfeldige utvalget har en gjennomsnittlig vekt lavere enn 64. Hva er sannsynligheten for dette utfallet? Kommenter påstanden til produsenten. Løsning \\[P(\\overline{X} &lt; 64) = P\\left(\\frac{\\overline{X} - 65.5}{3.6/\\sqrt{40}} &lt; \\frac{64 - 65.5}{3.6/\\sqrt{40}}\\right) = P(Z &lt; - 2.63) = 0.004\\] Det er svært usannsynlig at vi observerer et gjennomsnitt mindre enn 64 dersom det er sant at maskinen produserer tuber som er normalfordelt med gjennomsnitt 65.5 og standardavvik 3.6. Så påstanden er nok feil. Enten produserer maskinen vekter som er mindre, eller så er standardavviket større. En selger av robotgressklippere påstår at bare 4% av produktene må på service innen det første året etter installasjon. Du skal undersøke om påstanden stemmer og spør et tilfeldig utvalg av 100 husholdninger som har kjøpt robotgressklippere om de hadde den på service det første året. Hva vil du si om selgerens påstand dersom mer enn 6% svarer at de har hatt behov for service? Anta nå at du istedet spurte 400 hustander. Hva vil du si om selgerens påstand dersom mer enn 6% svarer at de har hatt behov for service? Løsning Da har vi altså en estimert service sannsynlighet på \\(\\hat{p} = 0.06\\) eller mer. For å finne ut om dette avviket fra 0.04 skyldes naturlig variasjon eller om den faktiske service raten er større enn 0.04 regner vi ut sannsynligheten for dette utfallet: \\[\\begin{equation*} \\begin{split} P(\\hat{p}&gt;0.06) &amp;= P\\left(\\frac{\\hat{p} - 0.04}{\\sqrt{0.04(1-0.04)/100}}&gt; \\frac{0.06 - 0.04}{\\sqrt{0.04(1-0.04)/100}}\\right)\\\\ &amp;= P(Z &gt; 1.02) = 1 - P(Z &lt; 1.02) = 1 - 0.8561 = 0.1539 \\end{split} \\end{equation*}\\] Det er altså ikke så usannsynlig at \\(\\hat{p} = 0.06\\) eller mer. Vi kan se for oss at vi gjentar den samme spørreundersøkelsen mange ganger. Da kan vi forvente at rundt 15 % av undersøkelsene gi en \\(\\hat{p} = 0.06\\) ved ren tilfeldighet selv om populasjonsverdien er \\(p=0.04\\). Selgerens påstand er ikke urimelig ved et slikt resultat. Sannsynligheten for et slikt utfall er da \\[\\begin{equation*} \\begin{split} P(\\hat{p}&gt;0.06) &amp;= P\\left(\\frac{\\hat{p} - 0.04}{\\sqrt{0.04(1-0.04)/300}}&gt; \\frac{0.06 - 0.04}{\\sqrt{0.04(1-0.04)/300}}\\right)\\\\ &amp;= P(Z &gt; 2.04) = 1 - P(Z &lt; 2.04) = 1 - 0.9793 = 0.02 \\end{split} \\end{equation*}\\] Nå har vi spurt flere husstander og vi ser at den samme estimerte service sannsynligheten (6 %) nå er mye mer usannsynlig å få dersom det faktisk er slik at bare 4 % trenger service. Selgerens påstand ville altså vært mer tvilsom ved et slik resultat. Forventningsrette estimatorer. Hva vil det si at en estimator er forventningsrett? Vis at hvis \\(X_1, X_2,\\dots X_n\\) er observasjoner fra en populasjon der \\(E(X_i) = \\mu\\) så er gjennomsnittet \\(\\overline{X}\\) en forventningsrett estimator av \\(\\mu\\). Vis at hvis \\(X\\) er et binomisk forsøk med \\(n\\) forsøk og suksessannsynlighet \\(p\\) så er \\(\\hat{p} = X/n\\) en forventningsett estimator av \\(p\\). Løsning Hvis \\(\\hat{\\theta}\\) er en estimator av \\(\\theta\\) så er \\(\\hat{\\theta}\\) en forventningsrett estimator dersom \\(E(\\hat{\\theta}) = \\theta\\). \\[\\begin{equation*} \\begin{split} E(\\overline{X}) &amp;= E\\left(\\frac{1}{n}(X_1 + X_2 + .. + X_n)\\right) = \\frac{1}{n}(E(X_1) + E(X_2) + ... + E(X_n))\\\\ &amp;= \\frac{1}{n}(\\mu + \\mu + ... + \\mu) = \\frac{1}{n}\\cdot n\\mu = \\mu \\end{split} \\end{equation*}\\] Vi husker at en binomisk variabel \\(X\\) har forventning \\(np\\). Altså er \\[E\\left(\\frac{X}{n}\\right) = \\frac{1}{n}E(X) = \\frac{1}{n}\\cdot np = p\\] Konsistente estimatorer. Hva vil det si at en estimator er konsistent? Vis at hvis \\(X_1, X_2,\\dots X_n\\) er observasjoner fra en populasjon der \\(\\text{Var}(X_i) = \\sigma^2\\) så er \\(\\text{Var}(\\overline{X}) = \\sigma^2/n\\). Bruk dette resultatet sammen med resultatet i 19 b til å forklare at \\(\\overline{X}\\) er en konsistent estimator av \\(\\mu\\). Vis at hvis \\(X\\) er et binomisk forsøk med \\(n\\) forsøk og suksessannsynlighet \\(p\\) så er \\(\\text{Var}(\\hat{p}) = p(1-p)/n\\). Bruk dette resultatet sammen med resultatet fra 19 c til å forklare at \\(\\hat{p}\\) er en konsistent estimator av \\(p\\). Løsning En estimator er konsistent hvis forskjellen mellom estimatoren og parameteren blir mindre når størrelsen på utvalget øker. \\[\\begin{equation*} \\begin{split} \\text{Var}(\\overline{X}) &amp;= \\text{Var}\\left(\\frac{1}{n}(X_1 + X_2 + .. + X_n)\\right) = \\left(\\frac{1}{n}\\right)^2(\\text{Var}(X_1) + \\text{Var}(X_2) + \\dots + \\text{Var}(X_n))\\\\ &amp;= \\left(\\frac{1}{n}\\right)^2(\\sigma^2 + \\sigma^2 + ... + \\sigma^2) = \\left(\\frac{1}{n}\\right)^2\\cdot n\\sigma^2 = \\frac{\\sigma^2}{n} \\end{split} \\end{equation*}\\] Vi vet fra før at \\(E(\\overline{X}) = \\mu\\). Vi ser av utrykket over at variansen til \\(\\overline{X}\\) avtar når \\(n\\) øker altså vil forskjellen mellom \\(\\overline{X}\\) og \\(\\mu\\) bli mindre jo større utvalg vi får. Vi husker at en binomisk variabel har varians \\(\\text{Var}(X) = np(1 - p)\\). Altså er \\[\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{X}{n}\\right) = \\left(\\frac{1}{n}\\right)^2 \\cdot\\text{Var}(X) = \\left(\\frac{1}{n}\\right)^2\\cdot np(1 - p) = \\frac{p(1 - p)}{n}\\] Igjen ser vi at variansen avtar når \\(n\\) øker altså vil forskjellen mellom \\(\\hat{p}\\) og \\(p\\) bli mindre jo større utvalg vi får. 2.3.2 Nøtter Nøtt 1. Korrelasjon er enten knyttet til et bestemt utvalg, eller en populasjon/modell, men vi sier sjelden “utvalgskorrelasjon” eller “populasjonskorrelasjon” og vi må ofte bare forstå dette ut fra situasjonen eller notasjonen (\\(r\\) versus \\(\\rho\\)). Men utvalgskorrelasjonen \\[r = s_{xy}/(s_xs_y)=\\frac{\\frac{1}{n-1}\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\frac{1}{n-1}\\sum_{i = 1}^n(x_i - \\overline{x})^2\\frac{1}{n-1}\\sum_{i = 1}^n(y_i - \\overline{y})^2}}\\] et altså en estimator av populasjonskorrelasjonen \\[\\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x\\sigma_y} = \\frac{E\\left[(X - E(X))(Y - E(Y))\\right]}{\\sigma_x\\sigma_y}\\]. Det betyr i praksis at \\(r\\) kommer med en usikkerhet siden den er basert på data. Det kan vises at dersom den simultane fordelingen til \\(X\\) og \\(Y\\) er en såkalt bivariat normalfordeling så vil vi for store utvalg ha at samplingfordelingen til \\(r\\) er tilnærmet normalfordelt med forventning \\(\\rho\\) og standardavvik \\(\\frac{(1 - \\rho^2)}{\\sqrt{n}}\\): \\[r\\sim N\\left(\\rho, \\frac{(1 - \\rho^2)^2}{n}\\right)\\] Anta at \\(X\\) og \\(Y\\) er bivariat normalfordelt. Forklar at \\(r\\) er en konsistent estimator av \\(\\rho\\). Hva kjennetegner populasjoner hvor \\(\\rho\\) kan estimeres med stor sikkerhet ved hjelp av \\(r\\) fra et utvalg? Beskriv en test av \\(H_0: \\rho = 0\\), mot \\(H_1: \\rho \\neq 0\\). Løsning Vi ser at når \\(n\\) blir stor går standardavviket til \\(r\\) mot null, og \\(r\\) vil derfor nærme seg den sanne korrelasjonen \\(\\rho\\) i populasjonen. Vi ser også at standardavviket til \\(r\\) er mindre jo nærmere \\(\\rho\\) er \\(\\pm 1\\). Utregninger av \\(r\\) basert på utvalg fra populasjoner med sterk positiv eller negativ lineær avhengighet (\\(\\rho\\) nær \\(\\pm 1\\)) kommer derfor med mindre usikkerhet. En test observator som vil være tilnærmet normalfordelt er da: \\[Z^* = \\frac{r - 0}{\\frac{(1 - r^2)}{\\sqrt{n}}}\\] og ved et \\(5\\%\\) signifikansnivå forkaster vi \\(H_0\\) dersom \\(|Z^*|&gt;1.96\\). Merk at sammenlignet med det sanne standardavviket har vi her erstattet \\(\\rho\\) med \\(r\\). Dette er litt som når vi erstatter \\(\\sigma\\) med \\(s\\) i den vanlige \\(Z\\) observatoren (\\(T\\) observatoren). Pga av sentralgrenseteoremet går dette fint for store utvalg. Obs: Det kommer mer om hypotesetesting i neste modul, så her foregriper vi begivenhetene litt. Nøtt 2. Anta at vi har følgende deterministiske sammenheng mellom variablene \\(X\\) og \\(Y\\): \\[Y = X^2\\] og at \\(X\\) er normalfordelt med forventning \\(0\\). Bruk definisjonen på kovarians til å vise at \\(\\text{cov}(X,Y)=0\\) og at korrelasjonen derfor også vil være \\(0\\) mellom disse variablene. Hint: Hvis \\(X\\) er symmetrisk fordelt og har forventning 0, så er \\(E(X^3) = 0\\). Hva sier dette resultatet oss om korrelasjon? Løsning Bruk definisjonen, erstatt \\(Y\\) med \\(X^2\\) og utnytt at \\(E(X) = E(X^3) = 0\\): \\[\\begin{equation*} \\begin{split} \\text{cov}(X,Y) &amp;= E(XY) - E(X)E(Y)\\\\ &amp;= E(X^3)- E(X)E(X^2)= 0 + 0*E(X^2)=0 \\end{split} \\end{equation*}\\] Dette er et ekstremt eksempel der avhengigheten mellom \\(X\\) og \\(Y\\) er deterministisk (Vet du \\(X\\) så vet du \\(Y\\) med 100 % sikkerhet), men korrelasjonen \\(\\rho = \\text{cov}(X,Y)/(\\sigma_X\\sigma_Y)\\) mellom variablene er allikevel 0. Altså er det fullt mulig at \\(X\\) og \\(Y\\) er avhengige selv om korrelasjonen mellom \\(X\\) og \\(Y\\) er null. Nøtt 3. Rulett er et sjansespill der et ruletthjul har 37 nummererte lommer der en kule kan lande. 18 av disse lommene er røde, 18 er svarte og 1 er grønn (iallefall i Europa). Hvis du f.eks satser 1 krone på rød og rød intreffer så vinner vi kronen vår tilbake og 1 krone til (1 krone i gevinst), mens hvis svart eller grønn intreffer så taper vi hele kronen (-1 krone i gevinst). Du satser 1 krone på rød. Hva er forventet gevinst? Hva er variansen til denne gevinsten? Hva er sannsynligheten for en positiv gevinst? Du fortsetter denne strategien \\(n = 50\\) ganger. Bruk sentralgrenseteoremet til å svare på spørsmålene i a. for den gjennomsnittlige gevinsten. Finnes det en (teoretisk) satsingsstrategi for å sikre positiv gevinst uansett? Løsning La \\(X\\) være gevinsten. Dette er en diskret variabel som bare kan ta to utfall, og den har følgende sannsynlighetsfordeling: \\(x_i\\) 1 -1 P(X = x_i) 18/37 19/37 For diskret fordelinger som dette har vi at \\[\\mu = E(X) = \\sum_{i = 1}^2 x_i P(X = x_i) = 1\\cdot(18/37) + (-1)\\cdot(19/37) = -0.027\\] For å finne variansen finner vi først \\[E(X^2) = \\sum_{i = 1}^2 x_{i}^2 P(X = x_i) = 1^2\\cdot(18/37) + (-1)^2\\cdot(19/37) = 1\\] og bruker regneregelen \\[\\sigma^2 = \\text{Var}(X) = E(X^2) - (E(X))^2 = 1 - (-0.027)^2 = 0.999\\] Sannsynligheten for positiv gevinst svarer til sannsynligheten for at ballen treffer rød, altså \\(18/37 = 0.4864\\). Sentralgrenseteoremet sier at når vi spiller et relativt stort antall runder så er \\(\\overline{X}\\) normalfordelt med forventning \\(E(\\overline{X}) = \\mu = -0.027\\) og varians \\(\\text{Var}(\\overline{X})=\\sigma^2/n = 0.999/50 = 0.02\\). Sannsynligheten for at den gjennomsnittlige gevinsten er positiv blir altså \\[\\begin{equation*} \\begin{split} P(\\overline{X}&gt;0) &amp;= P\\left(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} &gt; \\frac{0 - \\mu}{\\sigma/\\sqrt{n}}\\right) = P\\left(Z &gt; \\frac{0 - (-0.027)}{\\sqrt{0.02}}\\right)\\\\ &amp;= P(Z&gt;0.19) = 1 - P(Z &lt; 0.19) = 0.4246 \\end{split} \\end{equation*}\\] Kasinoer vet med andre ord å utnytte sentralgrenseteoremet. Har du noen penger (som du har råd til å tape) så sett hele beløpet på en farge og spill 1 gang, framfor å plassere småbeløp over lang tid. Det finnes noen strategier som kan sikre positiv gevinst, men som i praksis krever svært mye egenkapital og at det ikke er noe tak for hvor mye du kan satse ved bordet. Den såkalte Martingal strategien går ut på at du dobler innsatsen etter hvert tap og gir deg ved første gevinst. Gevinsten vil da svare til det første beløpet du satset. "],["relevant-r-grunnleggende.html", "2.4 Relevante R-kommandoer", " 2.4 Relevante R-kommandoer Under følger en liste over hvilke oppgaver du skal klare i R fra denne modulen. Vår policy fra og med vårsemesteret 2022 er at R-kommandoene under er tilstrekkelige for å løse oppgavene i datalabber og hjemmeeksamen i MET4. Det er med andre ord ikke nødvendig å lære seg teknikker utover det som er listet opp eksplisitt i listen under. Eventuelle nye teknikker som trengs for å løse en bestemt oppgave vil bli oppgitt og forklart dersom det er nødvendig. Det antas i tillegg at du kan den grunnleggende R-syntaksen som er dekket under Introduksjon til R. Antakelser om datasett Det antas at du har kontroll på hvilken mappesti R bruker, og at du kan forandre den dersom nødvendig. I denne modulen kan du anta at alle datasett består av \\(p\\) kolonner (variabler) med variabelnavn og \\(n\\) rader (observasjoner). Datasettene blir gitt enten som Excel-filer (.xls elle .xlsx) eller som .csv-filer. Dersom du ønsker å prøve på kommandoene selv, kan du laste ned følgende eksempelfiler: data.xls, data.xlsx, data.csv. Excel-filer kan leses inn i R ved hjelp av read_excel() som ligger i readxl-pakken: library(readxl) data &lt;- read_excel(&quot;data.xls&quot;) # ...eller dersom det er en .xlsx-fil: data &lt;- read_excel(&quot;data.xlsx&quot;) .csv-filer kan lastes inn i R ved hjelp av read_csv() som ligger i readr-pakken: library(readr) data &lt;- read_csv(&quot;data.csv&quot;) “csv” er en forkortelse for “comma separated values”, og grunnen til det kan du se ved å åpne opp data.csv i en ren tekst-editor. Da ser du at verdiene på rekken er separert med et komma. Av og til har disse filene litt andre formater, så vi må kunne håndtere følgende varianter: Dersom det ikke er komma som er brukt til å separere verdiene (det kan du alltid sjekke ved å se på filen i en tekst-editor), så kan du bruke funksjonen read_delim i stedet for read_csv, og bruke et argument for å spesifisere hvilket tegn som er brukt. Dersom det for eksempel er brukt semikolon “;”, kan du lese datasettet ved å bruke read_delim(\"data.xlsx\", delim = \";\"). Dersom komma er brukt for å skille desimaler fra heltall, som er vanlig i europeiske datasett, kan du bruke følgende kommando (der vi fortsatt antar at semikolon blir brukt til å skille mellom kolonner): read_delim(\"data.xlsx\", delim = \";\", locale = locale(decimal_mark = \",\"). Dersom en bestemt tekst-streng blir brukt til å indikere manglende verdier i datasettet kan du bruke argumentet na i read_csv() og read_delim() til å spesifisere denne tekst-strengen, slik at den blir tolket riktig ved innlesingen som manglende verdi. Dette argumentet fungerer også i read_excel(). Numerisk deskriptiv statistikk Vi bruker eksempeldatasettene data_survey.csv og data_fishermen_mercury.csv som eksempler. Du skal kunne bruke følgende funksjoner for å regne ut numerisk deskriptiv statistikk: survey &lt;- read_csv(&quot;data_survey.csv&quot;) fishermen &lt;- read_csv(&quot;data_fishermen_mercury.csv&quot;) mean(survey$age) # Gjennomsnitt median(survey$age) # Median table(survey$party) # Kan lese av typetallet manuelt. which.max(table(survey$party)) # Eller bruke funksjonen which.max() sd(fishermen$total_mercury) # Standardavvik quantile(fishermen$total_mercury, 0.20) # Kvantiler quantile(fishermen$total_mercury) # Kvartilene cor(fishermen$height, fishermen$weight) # Korrelasjonskoeffisient table(survey$religious) # Frekvenstabell survey %&gt;% # Krysstabell av to variabler (krever select(party, religious) %&gt;% # pipe-operatoren og select()-funks. table # fra dplyr-pakken) Grafisk deskriptiv statistikk med ggplot2 Du skal kunne lage følgende plott ved hjelp av ggplot2-pakken; library(ggplot2) # Boksplott fishermen %&gt;% ggplot + geom_boxplot(aes(y = total_mercury)) # Deler opp i grupper basert på en annen variabel fishermen %&gt;% ggplot + geom_boxplot(aes(y = total_mercury, x = as.factor(fisherman))) # Søylediagram survey %&gt;% ggplot + geom_bar(aes(x = as.factor(religious))) # Histogram survey %&gt;% ggplot + geom_histogram(aes(x = age)) # Spredningsplott fishermen %&gt;% ggplot + geom_point(aes(x = height, y = weight)) Du skal kunne grunnleggende bearbeiding av ggplot-figurer: # En bearbeidet variant fishermen %&gt;% ggplot + geom_point(aes(x = height, y = weight)) + # Dette er selve figuren xlab(&quot;Høyde&quot;) + # Tittel x-akse ylab(&quot;Vekt&quot;) + # Tittel y-akse ggtitle(&quot;Sammenheng mellom høyde og vekt&quot;) + # Tittel plott theme_classic() # Ryddig layout "],["hypotesetesting.html", " 3 Hypotesetesting", " 3 Hypotesetesting Hypotesetesting er et klassisk tema i statistikk. Vi skal først lære generelt om hva det egentlig vil si å teste en hypotese ved hjelp av statistikk, og kanskje like viktig: hva statistisk hypotesetesting ikke er. Vi går så videre til å lære noen vanlige anvendelser og ser hvordan alt dette kan implementeres i R. I videoforelesningene går vi gjennom noen slides, og vi skriver et R-skript. Du kan laste disse ned ved å klikke på lenkene under: Slides til “Hypotesetesting” R-script til “Hypotesetesting” TIPS: Hvis du ønsker å laste ned lysbildene som PDF trykker du på linken over, velger “Skriv ut”, og så skriver du ut som PDF. Før du gjør det bør du scrolle gjennom alle sidene slik at ligningene vises korrekt. "],["generelt-om-hypotesetesting.html", "3.1 Generelt om hypotesetesting", " 3.1 Generelt om hypotesetesting 3.1.1 Videoforelesninger 3.1.2 Kommentarer Her snakker vi om kapittel 11 i læreboken. Hvis du kan svare på følgende spørsmål har du i all hovedsak fått med deg de viktigste begrepene: Hva vil det si å gjennomføre en hypotesetest? Hva er Type I-feil og hva er Type II-feil? (Seksjon 11-1 forklarer dette greit) Hva er signifikansnivået (\\(\\alpha\\)) til en test? Styrken (the power) til en test er definert som \\(1-P(\\textrm{Type II-feil})=1-\\beta\\). Hvordan tolker du denne størrelsen? Se også 11-3d. Hva er \\(p\\)-verdien til en test (Seksjon 11-2c)? Les også 11-2d, e og f om hvordan vi fortolker og snakker om \\(p\\)-verdien på en korrekt måte. Vi kommer tilbake til dette i kapittel 3.2. "],["enpop.html", "3.2 Inferens om en populasjon", " 3.2 Inferens om en populasjon 3.2.1 Videoforelesninger 3.2.2 Kommentarer Dette er i hovedsak dekket av kapittel 12 i læreboken. Sjekk om du kan svare på følgende kontrollspørsmål: Hva er det vi tester når vi gjennomfører en \\(t\\)-test for én populasjon? Hva forutsetter vi? Hva er forskjellen på en ensidig og en tosidig test? (11-2j) Det kan også være greit å repetere konfidensintervaller i seksjon 11-2k for de som har glemt det fra MET2. I Seksjon 11-2g går boken gjennom en ett-utvalgs t-test i bokens Excel-plugin. La oss gjøre det samme i R. På kursets nettside finner du alle datasettene som følger med læreboken. I dette eksempelet er det snakk om Xm11-01.xlsx. Finn tak i denne filen (du kan også godt åpne den og se på den i Excel!), legg den i en mappe som du kan finne igjen, og åpne et nytt script i R-studio der du først sørger for å sette working directory til denne mappen slik vi gjorde i R-forelesningen. Etterpå leser du inn datasettet ved å bruke read_xlsx()-funksjonen som under: library(readxl) data &lt;- read_xlsx(&quot;Xm11-01.xlsx&quot;) # Vi bruker read_xslx() fordi det er en .xlsx-fil Konteksten til datasettet er gitt i eksempel 11.1. Det er altså balansen på 400 tilfeldig utvalgte kredittkontoer i en butikk, og en lurer på om forventet balanse er større enn 170. Vi setter opp følgende test: \\[\\begin{align*} &amp;H_0: \\mu = 170 \\\\ &amp;H_A: \\mu &gt; 170, \\end{align*}\\] der vi legger merke til at det blir brukt en ensidig test (hvorfor?). For å regne ut testobservatoren for å enutvalgs \\(z\\)-test trenger vi fire tall: \\(\\overline X\\), \\(\\mu_0\\), \\(n\\) og \\(\\sigma\\). Legger merke til at data har en kolonne som heter Accounts, og vi bruker dollartegnet til å hente den ut som en vektor. Regner ut observatoren: gj.snitt &lt;- mean(data$Accounts) # Gjennomsnittet av observasjonene mu0 &lt;- 170 # Henter fra teksten n &lt;- length(data$Accounts) # Antall observasjoner sigma &lt;- 65 # Henter fra teksten Z &lt;- (gj.snitt - mu0)/(sigma/sqrt(n)) # Verdien av testobservatoren Z # Skriver ut testobservatoren ## [1] 2.460462 Vi ser at testobservatoren har samme verdi som i Excel-gjennomgangen. Kritisk verdi finner vi rett fra R: qnorm(0.95) ## [1] 1.644854 Uansett; vi forkaster \\(H_0\\) siden testobservatoren er større enn kritisk verdi. Kapittel 11-3a-d gir enda mer forståelse for hypotesetesting. Hopp over e og f om du vil. Kapittel 11-4 snakker litt om hvordan vi skal bruke hypotesetesting videre. Kapittel 12 presenterer de tre testene (ett gjennomsnitt, en varians, en andel) i tur og orden. Det du først og fremst må kunne fra dette kapitlet er å gjennomføre disse testene, både for hånd med penn og papir, og i R. Under følger kode for å gjøre noen av bokens eksempler i R (les i boken for kontekst): Eksempel 12.1: \\[\\begin{align*} &amp;H_0: \\mu = 2.0 \\\\ &amp;H_A: \\mu &gt; 2.0, \\end{align*}\\] data &lt;- read_xlsx(&quot;Xm12-01.xlsx&quot;) # Manuell utregning gj.snitt &lt;- mean(data$Newspaper) mu0 &lt;- 2.0 n &lt;- length(data$Newspaper) s &lt;- sd(data$Newspaper) # Testobservator: (gj.snitt - mu0)/(s/sqrt(n)) ## [1] 2.236869 Signifikansnivået er satt til \\(\\alpha = 1\\%\\) i eksempelet. Kritisk verdi finner vi rett fra R: qt(0.99, df = n-1) ## [1] 2.351983 Altså forkaster vi ikke nullhypotesen. Sjekk gjerne verdiene vi regnet ut over og se at de stemmer overens med det som står i boken. Alternativt bruker vi t.test()-funksjonen direkte: t.test(data$Newspaper, alternative = &quot;greater&quot;, mu = 2.0, conf.level = 0.99) ## ## One Sample t-test ## ## data: data$Newspaper ## t = 2.2369, df = 147, p-value = 0.0134 ## alternative hypothesis: true mean is greater than 2 ## 99 percent confidence interval: ## 1.990716 Inf ## sample estimates: ## mean of x ## 2.180405 Resultatet blir selvsagt det samme. Når \\(p\\)-verdien er større enn signifikansnivået på 1%, kan vi ikke forkaste nullhypotesen. Eksempel 12.2 handler om å lage kondidensintervall, noe du også kan prøve å gjøre ved å regne ut de nødvendige tallene i R. De som synes dette er greit kan kikke på seksjonene 12-1b-e for å utvikle forståelsen enda litt mer. Eksempel 12.3: \\[\\begin{align*} &amp;H_0: \\sigma^2 = 1.0 \\\\ &amp;H_A: \\sigma^2 &lt; 1.0. \\end{align*}\\] Testobservator: \\[\\chi^2 = \\frac{(n-1)s^2}{\\sigma_0^2}.\\] data &lt;- read_xlsx(&quot;Xm12-03.xlsx&quot;) # Regner ut testobservatoren direkte denne gangen, uten å lagre tallene underveis: (length(data$Fills) - 1)*var(data$Fills)/1 # Kritisk verdi, 5% nivå, ensidig test, nedre hale: qchisq(0.05, df = length(data$Fills) - 1) ## [1] 15.2 ## [1] 13.84843 Vi kan altså ikke forkaste nullhypotesen. Igjen, les eksempelet i sin fulle lengde i boken for å forstå bedre hva som skjer. Figur 12.4 viser på en fin måte hva tallene betyr. Eksempel 12.5 kan være grei å kikke på også. Vi kan selvsagt bruke R som kalkulator og regne ut det vi trenger. Vi skal teste: \\[\\begin{align*} &amp;H_0: p = 0.5 \\\\ &amp;H_A: p &gt; 0.5. \\end{align*}\\] Vi har en observert andel på \\(\\widehat p = 407/765 = 0.532\\) etter å ha spurt \\(n = 765\\) personer. Testobservatoren er \\[Z = \\frac{\\widehat p - p}{\\sqrt{p(1-p)/n}}.\\] p.hatt &lt;- 407/765 p0 &lt;- 0.5 n &lt;- 765 (p.hatt - p0)/sqrt(p0*(1-p0)/n) ## [1] 1.771599 Kritisk verdi for en ensidig z-test på 5% nivå er 1.645 (qnorm(0.95)), og vi kan forkaste nullhypotesen. Seksjonene 12-3d-f bør leses på egen hånd, mens vi hopper over 12-3g. "],["inferens-om-to-populasjoner.html", "3.3 Inferens om to populasjoner", " 3.3 Inferens om to populasjoner 3.3.1 Videoforelesninger 3.3.2 Kommentarer Vi har gått gjennom kapittel 13, som i all hovedsak handler om å sammenligne to gjennomsnitt (som vi kan gjøre på tre forskjellige måter), to varianser og to andeler. Her følger noen kontrollspørsmål som du kan tenke over, og bruke som utgangspunkt for diskusjon i f.eks. kollokviegrupper: Hva er nullhypotesen når vi skal gjennomføre en t-test for to populasjoner? … og hvilke antagelser må vi gjøre? Hvordan ser testobservatoren ut for en to-utvalgs t-test, og kan du gi en intuitiv forklaring for hvorfor den ser ut som den gjør? Når kan vi bruke matchede par, og hva er hensikten? Hvilken testobservator brukes for sammenligning av to varianser, og hvilken fordeling har den under nullhypotesen? Kan du gi en intuitiv forklaring for hvorfor den ser ut som den gjør? Hvilken test brukes for å teste om to andeler er like, og hva må du anta? Videre bør du sjekke at du kan utføre 3 typer \\(t\\)-tester, test for like varianser og test for like andeler både for hånd (relevant for skoleeksamen) og i R (relevant til hjemmeeksamen og datalabber). Den enkleste måten å gjøre \\(t\\)-tester i R på er å bruke funksjonen t.test(). Kikk på eksempel 13.1 i lærebokens 11. utgave, der vi har observert årlige avkastninger til to aksjefond som er kjøpt henholdsvis med og uten megler. # Leser inn datasettet funds &lt;- read_xlsx(&quot;Xm13-01.xlsx&quot;) # Ser at det er to kolonner, «Direct» og «Broker». Alternativhypotesen på s.433 spesifiserer at # differansen i forventninger er *større* enn null, signifikansnivået skal være 5%. Antar først # ulik varians og at vi ikke skal gjøre en paret test: t.test(funds$Direct, funds$Broker, alternative = &quot;greater&quot;, paired = FALSE, var.equal = FALSE, conf.level = 0.95) ## ## Welch Two Sample t-test ## ## data: funds$Direct and funds$Broker ## t = 2.2872, df = 97.489, p-value = 0.01217 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.79661 Inf ## sample estimates: ## mean of x mean of y ## 6.6312 3.7232 Du kan så sjekke at du får ut de samme tallene på s. 434–435. Videre kan du skrive inn ?t.test i R-konsollen i RStudio for å lese mer om hvilke argumenter vi kan bruke i t.test()-funksjonen. Der ser vi at argumentene paired, var.equal og conf.level som utgangspunkt allerede er satt til FALSE, FALSE og 0.95 henholdsvis, så det hadde vi strengt tatt ikke trengt å spesifisere i funksjonskallet over. Vi kan enkelt kjøre den samme testen under antakelsen om like varianser ved å sette var.equal = TRUE: # Ser at det er to kolonner, «Direct» og «Broker». Alternativhypotesen på s.433 spesifiserer at # differansen i forventninger er *større* enn null, signifikansnivået skal være 5%. Antar først # ulik varians og at vi ikke skal gjøre en paret test: t.test(funds$Direct, funds$Broker, alternative = &quot;greater&quot;, paired = FALSE, var.equal = TRUE, conf.level = 0.95) ## ## Two Sample t-test ## ## data: funds$Direct and funds$Broker ## t = 2.2872, df = 98, p-value = 0.01217 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.7967156 Inf ## sample estimates: ## mean of x mean of y ## 6.6312 3.7232 Resultatet bli akkurat det samme. Siden testens \\(p\\)-verdi er mindre enn 5%, kan vi forkaste nullhypotesen og slå fast at forskjellen i gjennomsnitt er statistisk signifikant. I kapittel 13-3 leser vi om matchede par. Datasettet i eksempel 13.1 har like mange observasjoner i de to populasjonene, så vi kan tenke oss at målingene er gjort sekvensielt i tid, slik at vi kan matche dem, og heller se om gjennomsittet av differansene er signifikant forskjellig fra null. Enkelt: # Ser at det er to kolonner, «Direct» og «Broker». Alternativhypotesen på s.433 spesifiserer at # differansen i forventninger er *større* enn null, signifikansnivået skal være 5%. Antar først # ulik varians og at vi ikke skal gjøre en paret test: t.test(funds$Direct, funds$Broker, alternative = &quot;greater&quot;, paired = TRUE, conf.level = 0.95) ## ## Paired t-test ## ## data: funds$Direct and funds$Broker ## t = 2.5178, df = 49, p-value = 0.007563 ## alternative hypothesis: true mean difference is greater than 0 ## 95 percent confidence interval: ## 0.9716497 Inf ## sample estimates: ## mean difference ## 2.908 Da ser vi at \\(p\\)-verdien ble enda mindre. I eksemplene 13.4 og 13.5 kan du prøve selv. Pass på at du kan gjøre beregningene manuelt også, der du regner ut gjennomsnitt, testobservator, kritisk verdi osv, slik at du forstår hva som foregår. Kapittel 13-2 omhandler forskjellen mellom observasjonsdata og eksperimentelle data. Det er i grunn ganske viktig å sette seg inn i den forskjellen fordi det ofte har betydning for tolkningen vår av statistiske resultater. Det er et eksplisitt krav for å lykkes i MET4 at du er i stand til å sette resultatene inn i en fornuftig kontekst. I kapittel 13-4 kan vi lese om varianstesten. Eksempel 13.7 ser slik ut i R: bottle &lt;- read_xlsx(&quot;Xm13-07.xlsx&quot;) var.test(bottle$`Machine 1`, bottle$`Machine 2`, alternative = &quot;greater&quot;) ## ## F test to compare two variances ## ## data: bottle$`Machine 1` and bottle$`Machine 2` ## F = 1.3988, num df = 24, denom df = 24, p-value = 0.2085 ## alternative hypothesis: true ratio of variances is greater than 1 ## 95 percent confidence interval: ## 0.7051295 Inf ## sample estimates: ## ratio of variances ## 1.398807 Til slutt har vi test for to andeler i kapittel 13-5. De setter opp to varianter, en der vi sjekker om differansen mellom to andeler er like (\\(p_1 - p_2 = 0\\)), som er det vi har dett på i forelesning, men det går selvsagt like fint å sette opp en nullhypotese der differansen mellom andelene er lik et bestemt tall \\(D\\). Det finnes ingen ferdig prosedyre for denne testen i R, men vi kan sette den opp likevel ved å regne ut testobservatoren fra datasettet. Vi ser på eksempel 13.9, der vi får oppgitt salget av en del forskjellige varenummer, og vi ønsker å finne ut om andelen «9077» er større i Supermarked 1 enn i Supermarked 2: # Laster inn data. Her er det to utvalg med forskjellig antall observasjoner, så jeg # velger å lese inn de to kolonnene hver for seg: soap1 &lt;- read_xlsx(&quot;Xm13-09.xlsx&quot;, range = cell_cols(&quot;A&quot;)) soap2 &lt;- read_xlsx(&quot;Xm13-09.xlsx&quot;, range = cell_cols(&quot;B&quot;)) # Hvor stor andel utgjør «9077» i de to kolonnene? p1 &lt;- mean(soap1 == 9077) p2 &lt;- mean(soap2 == 9077) # De to utvalgsstørrelsene: n1 &lt;- nrow(soap1) n2 &lt;- nrow(soap2) # Felles estimat for p under nullhypotesen: p &lt;- (n1*p1 + n2*p2)/(n1 + n2) # Testobservatoren: z &lt;- (p1 - p2)/sqrt(p * (1-p)*(1/n1 + 1/n2)) # Kritisk verdi på 5% nivå for en ensidig test: qnorm(0.95) ## [1] 1.644854 Siden \\(z = 2.9\\) forkaster vi nullhypotesen om at det er lik andel «9077» i de to populasjonene. "],["kjikvadrattester.html", "3.4 Kjikvadrattester", " 3.4 Kjikvadrattester 3.4.1 Videoforelesninger 3.4.2 Kommentarer Vi må kunne to anvendelser av kjikvadrattester, der hver av de har sitt eget delkapittel i boken: Teste for om en gitt fordeling passer med obervasjoner (“Goodness-of-fit”). Teste for uavhengighet. I den første anvendelsen får vi oppgitt en diskret sannsynlighetsfordeling der vi har noen mulige utfall \\(u_1, \\ldots, u_k\\), med tilhørende sannsynligheter \\(p_1, \\ldots,p_k\\). Dersom vi skal observere \\(n\\) utfall fra denne fordelingen, vil vi forvente \\(e_i = p_i\\cdot n\\) observasjoner av utfall \\(u_i\\). Nå har det seg slik at vi har observert \\(n\\) utfall fra fordelingen, og utfall \\(u_i\\) har skjedd \\(f_i\\) ganger. Vi lurer da på om de observerte frekvensene (\\(f_i\\)) er så forskjellige fra de forventede frekvensene (\\(e_i\\)) at vi ikke lenger tror at \\(p_1, \\ldots,p_k\\) er den sanne sannsynlighetsfordelingen. Vi kom frem til en fornuftig testobservator: \\[\\chi^2 = \\sum_{i=1}^k \\frac{(f_i - e_i)^2}{e_i},\\] som er \\(\\chi^2\\)-fordelt med \\(k-1\\) frihetsgrader dersom nullhypotesen er sann. Det betyr at vi kan sjekke om verdien av testobservatoren så stor (dvs, \\(f\\)´ene er for forskjellige fra \\(e\\)`ene) at vi ikke lenger tror at \\((p_1, \\ldots, p_k)\\) er den sanne sannsynlighetsfordelingen. Vi gjorde eksempelet i dette delkapitlet i forelesningen, og brukte følgende kommandoer: p0 &lt;- c(0.45, 0.40, 0.15) # Fordeling under H0 f &lt;- c(102, 82, 16) # Observerte frekvenser chisq.test(x = f, p = p0) ## ## Chi-squared test for given probabilities ## ## data: f ## X-squared = 8.1833, df = 2, p-value = 0.01671 Den andre anvendelsen er å teste for om to kjennetegn opptrer uavhengig av hverandre. Ideen er den samme som over, fordi vi kan skrive sannsunligheten for «\\(A\\) og \\(B\\)» som et produkt dersom de ar uavhengige: \\[P(A \\cap B) = P(A)\\cdot P(B).\\] Vi kan regne ut hvor mange observasjoner vi forventer å se for hver kombinasjon av de to kjennetegnene (\\(e_{ij}\\)), og bruke kjikvadrattesten over til å sjekke om disse er langt fra det vi faktisk har observert (\\(f_{ij}\\)). Boken har et eksempel på dette som de regner ut både for hånd og i Excel. Slik kan vi gjøre det i R: # Leser inn data mba &lt;- read_xlsx(&quot;Xm15-02.xlsx&quot;) # Kikker på datasettet mba ## # A tibble: 152 × 2 ## Degree `MBA Major` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 1 ## 2 1 1 ## 3 1 1 ## 4 1 1 ## 5 2 2 ## 6 1 3 ## 7 3 1 ## 8 1 1 ## 9 2 1 ## 10 2 2 ## # ℹ 142 more rows Vi legger merke til at strukturen på datasettet er litt annerledes enn krysstabellen som er vist s. 601 i læreboken. I stedet for at vi har telt opp antall studenter i hver enkelt kominasjon av «bachelorgrad» og «masterprofil», har vi fått oppgitt en tabell der hver rad representerer en enkeltstudents fagkombinasjon. Vi kan dog enkelt lage en krysstabell i R: table(mba) ## MBA Major ## Degree 1 2 3 ## 1 31 13 16 ## 2 8 16 7 ## 3 12 10 17 ## 4 10 5 7 Det er denne som brukes som argument i chisq.test(): chisq.test(table(mba)) ## ## Pearson&#39;s Chi-squared test ## ## data: table(mba) ## X-squared = 14.702, df = 6, p-value = 0.02271 Her er det bare å sammenligne tallene med det som læreboken finner i Excel. Noen kontrollspørsmål: Vi har lært to veldig spesifikke anvendelser av kjikvadrattester. Hvilke? Kan du gi en intuitiv forklaring på hvorfor testobservatoren vår er fornuftig? Litt mer vanskelig: Kan du gi en intuitiv forklaring for hvorfor testobservatoren er tilnærmet kjikvadratfordelt? "],["oppgaver.html", "3.5 Oppgaver", " 3.5 Oppgaver 3.5.1 Standard oppgaver Introduksjon til hypotesetesting For hvert av scenarioene i a og b, gjør følgende: Sett opp relevant nullhypotese og alternativhypotese (hint: nullhypotese avhenger av hvor ‘bevisbyrden’ bør ligge) Definer type I-og type II-feil. Diskuter konsekvensene av type I-feil og type II-feil i det aktuelle scenarioet. En ny type medisin skal vurderes for kommersialisering. Du sitter i et vurderingspanel som skal vurdere om medisinen kan bli godkjent eller ikke. Du blir presentert to ulike investeringer å velge mellom. En av dem er veldig risikabel, men med stor potensiell profitt. Den andre er mindre risikabel, men med lavere potensiell profitt. Løsning \\(H_0\\): Den nye medisinen er ikke trygg og effektiv. \\(H_1\\): Den nye medisinen er trygg og effektiv. Type I-feil: Forkaste \\(H_0\\) når \\(H_0\\) er sann. Konsekvens: Risikerer at vi begynner å produsere en medisin som ikke er trygg og effektiv. Type II-feil: Forkaster ikke \\(H_0\\) når \\(H_1\\) er sann. Konsekvens: Vi lar være å produsere en medisin som faktisk er trygg og effektiv. Kommentar: Signifikansnivået ved produksjon av medisiner settes ofte lavt fordi konsekvensene av en type I-feil kan være svært alvorlige. \\(H_0\\): Den mest risikable investeringen er mest lønnsom. \\(H_1\\): Den mest risikable investeringen er ikke mest lønnsom. Type I-feil: Forkaste \\(H_0\\) når \\(H_0\\) er sann. Konsekvens:Vi investerer i den minst risikable investeringen, som ikke er mest lønnsom. Type II-feil: Forkaster ikke \\(H_0\\) når \\(H_1\\) er sann. Konsekvens: Vi investerer i den mest risikable investeringen, som ikke er mest lønnsom. Vi har følgende hypoteser og informasjon om dataene: \\(H_0: \\mu = 150\\) mot \\(H_1: \\mu \\neq 150\\). \\(\\sigma = 10\\), \\(n =100\\), \\(\\overline{x} = 150\\). Bestem verdi av testobservator, forkastelsesområde dersom signifikansnivået er \\(\\alpha = 0.05\\), og p-verdi. Konkluder. Løsning Her er \\(\\sigma\\) kjent så vi kan bruke en z-observator. Forkastelsesområde \\(Z &lt; -z_{0.025}=-1.96\\) eller \\(Z&gt; z_{0.025}=1.96\\). \\[Z = \\frac{\\overline{x} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{150 - 150}{10/\\sqrt{100}}=0\\] p-verdi\\(=2P(Z &gt; 0) = 2\\times0.5=1\\). Vi kan ikke forkaste nullhypotesen \\(H_0: \\mu = 150\\). Faktisk er det ekstremt sannsynlig (p-verdi = 1) å observere det vi har observert dersom nullhypotesen er sann. Vi har følgende hypoteser og informasjon om dataene: \\(H_0: \\mu = 55\\) mot \\(H_1: \\mu &gt; 55\\). \\(\\sigma = 20\\), \\(n = 25\\), \\(\\overline{x} = 67\\). Regn ut testobservatoren \\(Z\\). Regn ut p-verdi. Regn ut p-verdi, denne gangen med \\(\\overline{x}\\) = 63. Regn ut p-verdi, denne gangen med \\(\\overline{x}\\) = 59. Fastslå hva som skjer med verdien av testobservatoren (Z) og p-verdien når \\(\\overline{x}\\) nærmer seg \\(55\\) (verdien av \\(\\mu\\) under \\(H_0\\)). Løsning \\[Z = \\frac{\\overline{x} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{67 - 55}{20/\\sqrt{25}}=3\\] \\(\\text{p-verdi} = P(Z &gt; 3.00) = 1 – P(Z&lt;3) = 1 – 0.9987 = 0.0013\\). Kommentar: Her kan du bruke pnorm(3) i R til å regne ut \\(P(Z&lt;3)\\). Ny verdi av testobservator blir da \\(Z = 2\\). \\(\\text{p-verdi} = P(Z &gt; 2.00) = 1 – 0.9772 = 0.0228\\). Ny verdi av testobservator blir da \\(Z = 1\\). \\(\\text{p-verdi} = P(Z &gt; 1.00) = 1 – 0.8413 = 0.1587\\). Vi ser at testobservatoren minker og p-verdien øker når \\(\\overline{x}\\) nærmer seg \\(55\\). Forklaring: La \\(\\mu_0 = 55\\) være verdien av \\(\\mu\\) under \\(H_0\\). Z-observatoren måler avviket mellom antagelsen under \\(H_0\\) og dataene vi observerer og avtar derfor når vår observasjon av \\(\\overline{x}\\) nærmer seg \\(\\mu_0\\). P-verdien sier hvor sannsynlig det er å observere de dataene vi har dersom \\(H_0\\) er sann og øker følgelig når \\(\\overline{x}\\) nærmer seg \\(\\mu_0\\). Annta at vi har følgende hypoteser og informasjon om dataene: \\(H_0: \\mu = 50\\) mot \\(H_1: \\mu &gt; 50\\). \\(\\sigma = 10\\), \\(n = 40\\), \\(\\alpha = 0.05\\). Bestem \\(\\beta\\), altså sannsynligheten for en type-II feil, under antagelsen at \\(\\mu = 55\\). Løsning Forkastningsområdet blir da \\[Z = \\frac{\\overline{X} - 50}{10/\\sqrt{40}} &gt; Z_{0.05} = 1.645 \\] dvs at vi forkaster \\(H_0\\) når \\[\\overline{X} &gt; 50 + 1.645\\times\\frac{10}{\\sqrt{40}}=52.6\\] En type-II feil svarer til å ikke forkaste \\(H_0\\) når \\(H_1\\) er sann. For å regne på sannsynligheten for type-II feil må vi ikke bare anta at \\(H_1\\) er sann, men være spesifike på hva verdien til \\(\\mu\\) er (i dette tilfellet 55). Vi lurer altså på hva sannsynligheten for at vi ikke er i forkastnings området (\\(\\overline{X} &lt; 52.6\\)) gitt at \\(\\mu = 55\\): \\[\\begin{equation*} \\begin{split} \\beta &amp;= P(\\overline{X} &lt; 52.6\\quad\\text{gitt at $\\mu = 55$})\\\\ &amp;= P(\\frac{\\overline{X} - 55}{10/\\sqrt{40}} &lt; \\frac{52.6 - 55}{10/\\sqrt{40}})\\\\ &amp;=P(Z &lt; -1.52) = 0.064 \\end{split} \\end{equation*}\\] Merk: Et begrep som ofte blir brukt om tester er styrken til testen \\(1-\\beta\\) som da er sannsynligheten for å forkaste \\(H_0\\) når \\(H_1\\) er sann. En god test har god (stor) styrke. Hadde vi gjentatt denne testen mange ganger ville vi ha forkastet \\(H_0\\) i \\((100 - 6.4)\\% = 93.6\\%\\) av gangene dersom det faktisk er slik at sann \\(\\mu\\) er 55. En leder frykter at den gjennomsnittlige tiden ansatte daglig bruker på sosiale medier overstiger 45 minutter. For å teste denne mistanken, plukker han ut et tilfeldig utvalg på 15 personer, og spør om tid brukt på sosiale medier etter en tilfeldig arbeidsdag. Resultatene er oppsummert nedenfor. 70, 96, 58, 88, 34, 42, 34, 56, 68, 46, 26, 18, 22, 60, 84 Hvis samlingen av tider er normalfordelt med standardavvik på 20 minutter, kan lederen hevde at mistanken hans stemmer på et 1 % signifikans-nivå? Tror du observasjonene over er et representativt utvalg? Hva kunne lederen gjort annerledes? Løsning Lederen ønsker altså å teste \\(H_0: \\mu = 45\\) mot alternativ hypotesen \\(H_1: \\mu &gt; 45\\). Testobservatoren er da gitt ved \\[Z = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{53.46 - 45}{20/\\sqrt{15}} = 1.64\\] Sannsynligheten for å observerer noe minst like ekstremt og til fordel for \\(H_1\\) (p-verdien) er da \\[P-verdi = P(Z &gt; 1.645) \\approx 0.05 &gt; 0.01\\]. Lederen kan altså ikke trekke denne konklusjonen på 1 % signifikansnivå. For å forkaste null hypotesen på et 1 % signifikansnivå, måtte vi hatt p-verdi lavere enn 1 %. Dersom lederen spør ansatte fjes til fjes er det nærliggende å tro at de ansatte vil underdrive sin bruk av sosiale medier. En anonym spørreundersøkelse ville nok gitt et mer representativt utvalg. Gjennomsnitt og standardavvik i et utvalg på \\(n=100\\) er \\(\\overline{x} = 20\\) og \\(s = 2\\). Finn 95 % konfidensintervall av gjennomsnitt (\\(\\mu\\)) i populasjonen. Gjenta a. med \\(s = 5\\). Gjenta a. med \\(s = 10\\). Fastslå hvordan det estimerte konfidensintervallet endrer seg når vi øker \\(s\\). Anta at \\(s=5\\) og regn et 95 % konfidensintervall dersom størrelsen på utvalget er henholdsvis \\(n = 50\\) og \\(n=10\\). Fastlå hvordan det estimerte konfidensintervallet endrer seg når vi øker \\(n\\). Løsning \\[\\overline{x} \\pm t_{\\alpha/2, n - 1}s/\\sqrt{n} = 20 \\pm 1.984\\times 2/\\sqrt{100} = [19.60 ,20.40]\\] \\[ 20 \\pm 1.984\\times 5/\\sqrt{100} = [19.01 ,20.99]\\] \\[ 20 \\pm 1.984\\times 10/\\sqrt{100} = [18.02 ,21.98]\\] Konfidensintervallet blir større når \\(s\\) øker. \\[\\overline{x} \\pm t_{\\alpha/2, n - 1}s/\\sqrt{n} = 20 \\pm 2.09\\times 5/\\sqrt{50} = [18.58 ,21.42]\\] \\[\\overline{x} \\pm t_{\\alpha/2, n - 1}s/\\sqrt{n} = 20 \\pm 2.26\\times 5/\\sqrt{10} = [16.42 ,23.58]\\] Vi ser at jo større \\(n\\) er jo mindre blir konfidensintervallet. Flere observasjoner gjør at vi med større sikkerhet (smalere intervall) kan si hvor \\(\\mu\\) ligger. Med sterkt fall i flyreiser og passasjerer på grunn av koronakrisen var det i samme periode sannsynlig med færre forsinkelser i flytrafikken. Før krisen hevdet et flyselskap at de landet presist 92 % av flyreisene. I et tilfeldig utvalg flyreiser hos det samme selskapet under krisen ble 156 av 165 vurdert til å være presise. Kan vi konkludere på 5 % signifikansnivå at det er færre forsinkelser under koronakrisen? Løsning La \\(p\\) være den sanne andelen av flyreiser som kommer presist under krisen. Vi skal da teste \\(H_0: p = 0.92\\) mot \\(H_1: p &gt; 0.92\\). Vårt estimat på \\(p\\) er \\(\\hat{p}=156/165 \\approx 0.945\\) og testobservatoren er gitt ved \\[z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}} = \\frac{0.945 - 0.92}{\\sqrt{0.92(1 - 0.92)/165}} \\approx 1.18\\] Dersom vi bruker kritisk verdi ville forkastningsområdet vært \\(z &gt; 1.645\\) og siden \\(z &lt; 1.645\\) kan vi altså ikke forkaste \\(H_0\\). Alternativt kan vi regne ut p-verdien som er “sannsynligheten for det vi har observert eller noe enda mer til fordel for \\(H_1\\). En enda mer positiv verdi enn 1.18 ville vært til fordel for \\(H_1\\), derfor er \\[p-verdi = P(Z &gt; 1.18) = = 1 - P(Z &lt; 1.18) \\approx 0.12\\] og vi kan derfor ikke forkaste \\(H_0\\) siden p-verdien ikke er mindre enn 0.05. TIPS: \\(P(Z&lt;1.18) =\\) pnorm(1.18) i R. Vi har følgende informasjon fra to tilfeldige utvalg fra to ulike normalfordelte populasjoner: \\(\\overline{x}_1 = 400\\), \\(s_1 = 130\\), \\(n_1 = 130\\), \\(\\overline{x}_2 = 390\\), \\(s_2 = 50\\), \\(n_2 = 130\\). Kan vi hevde på et 5 % signifikansnivå at \\(\\mu_1\\) er større enn \\(\\mu_2\\)? Det oppgis at \\[\\nu = \\frac{\\left(\\frac{s^2_{1}}{n_1} + \\frac{s^2_{2}}{n_2}\\right)^2}{\\left(\\frac{s^2_{1}}{n_1}\\right)^2/(n_1 - 1) + \\left(\\frac{s^2_{2}}{n_2}\\right)^2/(n_2 - 1)}\\approx 166\\] Gjenta a., denne gangen med \\(s_1 = 30\\) og \\(s_2 = 15\\). Som over, oppgis det at \\(\\nu \\approx 190\\). Fastslå hva som skjer hvis utvalgenes standardavvik blir mindre. Gjenta a., denne gangen med utvalg på \\(n_1 = n_2 = 20\\) observasjoner. Som over, oppgis det at \\(\\nu \\approx 28\\). Fastslå effekten av å redusere utvalgsstørrelser. Løsning Vi skal altså teste \\(H_0: \\mu_1 - \\mu_2 = 0\\) mot \\(H_1: \\mu_1 - \\mu_2 &gt; 0\\). Siden variansene i utvalget er såpass ulike antar vi at vi må bruke varianten av testen med ulik varians. Testobservatoren er da gitt ved \\[T = \\frac{\\overline{x}_1 - \\overline{x}_2 - 0}{\\sqrt{\\frac{s^2_{1}}{n_1} + \\frac{s^2_{2}}{n_2}}}=\\frac{400 - 390}{\\sqrt{\\frac{130^2}{130} + \\frac{50^2}{130}}} = 0.82\\] Antall frihetsgrader oppgis til å være \\(v \\approx 166\\). Vi forkaster \\(H_0\\) dersom \\(T &gt; t_{0.05, 166} = 1.654\\), og siden dette ikke er tilfelle her kan vi ikke forkaste \\(H_0\\) ved 5 % signifikansnivå. Testobservatoren blir i dette tilfellet \\[T = \\frac{\\overline{x}_1 - \\overline{x}_2 - 0}{\\sqrt{\\frac{s^2_{1}}{n_1} + \\frac{s^2_{2}}{n_2}}}=\\frac{400 - 390}{\\sqrt{\\frac{30^2}{130} + \\frac{15^2}{130}}} = 3.40\\] Antall frihetsgrader oppgis til å være \\(v \\approx 190\\). Siden \\(T = 3.40 &gt; t_{0.05, 190} = 1.653\\) forkaster vi \\(H_0\\) på 5 % signifikansnivå. Når standardavvikene i utvalgene blir mindre øker verdien av testobservatoren. Mindre standardavvik betyr at vi er mer sikre på at \\(\\overline{x}_1 - \\overline{x}_2\\) ligger nær \\(\\mu_1 - \\mu_2\\), og at en evt. differanse kan tyde på avvik fra \\(H_0\\). Dette blir reflektert av en større testobservator. Testobservatoren blir da \\[T = \\frac{\\overline{x}_1 - \\overline{x}_2 - 0}{\\sqrt{\\frac{s^2_{1}}{n_1} + \\frac{s^2_{2}}{n_2}}}=\\frac{400 - 390}{\\sqrt{\\frac{130^2}{20} + \\frac{50^2}{20}}} = 0.32\\] Antall frihetsgrader oppgis til å være \\(v \\approx 28\\). Vi forkaster \\(H_0\\) dersom \\(T &gt; t_{0.05, 28} = 1.701\\), og siden dette ikke er tilfelle her kan vi ikke forkaste \\(H_0\\) ved 5 % signifikansnivå. Få observasjoner representerer større usikkerhet om differansen \\(\\overline{x}_1 - \\overline{x}_2\\) bare skyldes tilfeldighet, og dette reflekteres av testobservatoren som vil synke når utvalgsstørrelsen reduseres. Et nystartet firma har utviklet to løsninger for automatisk registrering av av antall lus på oppdrettslaks. Metode A er litt dyrere enn metode B, men firmaet mener Metode A en den raskeste metoden. For å teste ut denne hypotesen blir begge metodene brukt til å registrere lus på 11 basseng av ulik størrelse og med forskjellig antall fisk. Antall minutter hver metode tar blir registrert for hvert basseng. Under er det gitt en R-utskrift fra en to-utvalgs t-test. Still opp nullhypotesen og den alternative hypotesen i samsvar med R-utskriften. Bruk også utskriften til å formulere en konklusjon for en test med 5% signifikansnivå. ## ## Welch Two Sample t-test ## ## data: metodeB and metodeA ## t = 1.6129, df = 18.969, p-value = 0.06164 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -0.7959228 Inf ## sample estimates: ## mean of x mean of y ## 83.44886 72.41662 Under er det gitt en R-utskrift fra en paret t-test for de samme dataene. Still opp nullhypotesen og den alternative hypotesen i samsvar med R-utskriften. Formuler testobservatoren og bruk også utskriften til å formulere en konklusjon for en test med 5% signifikansnivå. ## ## Paired t-test ## ## data: metodeB and metodeA ## t = 5.4477, df = 10, p-value = 0.0001409 ## alternative hypothesis: true mean difference is greater than 0 ## 95 percent confidence interval: ## 7.361811 Inf ## sample estimates: ## mean difference ## 11.03225 Hvilke av de to foregående testene bør en bruke i dette tilfellet? Løsning La \\(\\mu_1\\) og \\(\\mu_2\\) være forventet tid til registrering av lus i et basseng for hhv metode B og metode A. Da er R-utskriften en test av \\(H_0: \\mu_1 - \\mu_2 = 0\\) mot det enside alternativet \\(H_1: \\mu_1-\\mu_2 &gt; 0\\). Fra utskriften ser vi at \\(p-value= 0.06164 &gt; 0.05\\), altså kan vi ikke forkaste \\(H_0\\) ved \\(5\\%\\) signifikansnivå. Praktisk tolkning: Vi kan ikke konkludere med at det er noe forskjell i tiden de to metodene bruker. b.I en paret t-test baserer vil testen på de parvise differansene \\(d_i = x_i - y_i\\), der \\(x_i\\) og \\(y_i\\) er tiden hhv metode B og A bruker på å registrere lus i basseng nr. i. Utskriften viser en test av \\(H_0: \\mu_d = 0\\) mot det ensidige alternativet \\(H_1: \\mu_d &gt; 0\\). Testobservatoren er da gitt ved \\[T=\\frac{\\overline{d}-0}{s_d/\\sqrt{n}}\\] Fra utskriften ser vi at \\(p-value= 0.0001409 &lt; 0.05\\), altså kan vi forkaste \\(H_0\\) ved \\(5\\%\\) signifikansnivå. Praktisk tolkning: Det ser ut til at metode A er raskere enn metode B. En paret t-test er generelt det riktige valget dersom observasjonene som blir “paret” er avhengige. I dette tilfellet er det naturlig å tro at tiden metode A og B bruker på et basseng er avhengige størrelser (f.eks vil et basseng med mye fisk ta lang tid å registrere for begge metodene). I en to-utvalgs t-test antar vi derimot at tiden det tar for metode A og B å registrere lus for et basseng er uavhengige. Vi har følgende informasjon fra to tilfeldige utvalg fra to ulike normalfordelte populasjoner: \\(s_{1}^2 = 1400\\), \\(n_1 = 60\\), \\(s_{2}^2 = 700\\), \\(n_2 = 60\\). Kan vi hevde at de to utvalgene har ulik varians? Bruk 5 % signifikansnivå. Gjenta a., denne gangen med \\(n_1 = 30\\) og \\(n_2 = 30\\). Fastslå hva som er effekten på verdi av testobservator og konklusjon av testen når vi reduserer utvalgsstørrelse. Løsning Her ønsker vi å test \\(H_0: \\sigma_{1}^2/\\sigma_{2}^2 = 1\\) mot \\(H_1: \\sigma_{1}^2/\\sigma_{2}^2 \\neq 1\\). Husk at for en tosidig test er det smart å formulere null- og alternativ hypotesen slik at den største utvalgsvariansen kommer i telleren til testobservatoren: \\[F = s_{1}^2/s_{2}^2 = 1400/700 = 2\\] Da trenger vi nemlig kun å sammenligne testobservatoren med den øvre kvantilen i F-fordelingen. Forkastningsområdet blir i dette tilfellet \\(F &gt; F_{0.025, 59, 59} = 1.67\\). Siden \\(F = 2\\) er større enn \\(1.67\\) kan vi altså forkaste \\(H_0\\) til fordel for \\(H_1\\) på et 5 % signifikansnivå. Forkastningsområdet er da \\(F &gt; F_{0.025, 29, 29} = 2.1\\) og siden \\(F &lt; 2.1\\) kan vi ikke forkaste \\(H_0\\) på et 5 % signifikansnivå. Testobservatoren forblir uendret, men vi ser at den kritiske verdien som må overstiges for å få forkastning øker når antall observasjoner avtar. Konklusjonen blir derfor motsatt i oppgave b. Skulle vi fått forkastning også i b. måtte det reduserte utvalget blitt kompensert av et større avvik mellom \\(s_1\\) og \\(s_2\\). En bedrift som har en dyr leieavtale av en parkeringsplass vurderer innkjøp av elektroniske sparkesykler. Tanken er at de som bor nær bedriften da kan benytte seg av disse istedenfor å kjøre bil. I et prøveprosjekt får bedriften leid en rekke sparkesykler i 20 arbeidsdager og antall biler på parkeringsplassen blir registrert daglig. Den samme registreringen blir gjort de 20 påfølgende arbeidsdagene når sparkesyklene ikke er tilgjengelig. En ansatt som er ansvarlig for prøveprosjektet bruker R til å utføre to tester basert på data fra disse to registreringene. Formuler null- og alternativhypotesen til den første testen. Hvorfor utfører den ansatte denne testen? Formuler null- og alternativhypotesen samt testobservatoren til den andre testen. Trekk en praktisk konklusjon om innføringen av sparkesykler ut fra utskriften. ## ## F test to compare two variances ## ## data: uten_sparkesykkel and med_sparkesykkel ## F = 1.5385, num df = 19, denom df = 19, p-value = 0.3559 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.6089665 3.8870052 ## sample estimates: ## ratio of variances ## 1.538524 ## ## Two Sample t-test ## ## data: uten_sparkesykkel and med_sparkesykkel ## t = 2.4621, df = 38, p-value = 0.009232 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 2.525173 Inf ## sample estimates: ## mean of x mean of y ## 39.91587 31.90524 Løsning La \\(\\sigma^2_{1}\\) og \\(\\sigma^2_{2}\\) være populasjonsvariansene til antall biler som kommer for å parkere på henholdsvis dager der sparkesykler ikke er tilgjengelig og dager der sparkesykler er tilgjengelig. Utskriften viser da en test av \\(H_0: \\sigma^2_{1}/\\sigma^2_{2} = 1\\) mot \\(H_1: \\sigma^2_{1}/\\sigma^2_{2} \\neq 1\\). Den ansatte utfører denne testen for å se om han kan bruke varianten av to-utvalgs t-test som antar lik varians når vedkommende tester om innføringen av sparkesykler har noen effekt (se b.). Ut fra p-verdien på 0.3559 (altså ingen forkastning av \\(H_0\\)) vil det være greit å bruke en slik test i dette tilfellet. Tips: variabel navnet som står først i testen (“uten_sparkesykkel”) indikerer hvilke av de to variansen som står i telleren av \\(\\sigma^2_{1}/\\sigma^2_{2}\\) (i \\(H_0\\)) og \\(s^2_{1}/s^2_{2}\\) (i testobservatoren) i testen R utfører. La \\(\\mu_1\\) og \\(\\mu_2\\) være forventet antall biler på henholdsvis dager der sparkesykler ikke er tilgjengelig og dager der sparkesykler er tilgjengelig. Utskriften viser en en-sidig, to-utvalgs t-test av hypotesen \\(H_0: \\mu_1 - \\mu_2 = 0\\) mot \\(H_1: \\mu_1 - \\mu_2 &gt; 0\\). \\(H_1\\) er altså om det forventes ferre biler på dager med sparkesykler enn på dager uten sparkesykler. “Two sample t-test” betyr at vi har antatt lik varians (ellers ville det stått “Welch two sample t-test”), og testobservatoren er derfor gitt ved: \\[T = \\frac{\\overline{x}_1 - \\overline{x}_2 - 0}{\\sqrt{s^2_{P}(1/n_1 + 1/n_2)}}\\] der \\[s^2_{P} = \\frac{(n_1 - 1)s^2_{1} + (n_2 - 1)s^2_{2}}{n_1 + n_2 -2}\\] Ut fra p-verdien på 0.009232 forkaster vi \\(H_0\\) på 1 % (!) signifikansnivå. Vi har med andre ord god grunn til å tro at en innføring av sparkesykler vil redusere antall biler på parkeringsplassen. Tips: Variabelnavnet som står først (“uten_sparkesykkel”) indikerer hva som er første verdi i differansene \\(\\mu_1 - \\mu_2\\) (i hypotesene) og \\(\\overline{x}_1 - \\overline{x}_2\\) (i testobservatoren) i testen R utfører. Vi har følgende informasjon om fra to tilfeldige utvalg fra to populasjoner: I utvalg 1 har \\(x_1 = 100\\) av \\(n_1 = 200\\) individer et spesielt kjennetegn, mens i utvalg 2 er det det tilsvarende tallet \\(x_2 = 90\\) av \\(n_2 = 200\\). Utfør en test for å undersøke om de to populasjonene har ulik andel av kjennetegnet. Bruk 5 % signifikansnivå. Regn ut p-verdien for testen over. Gjenta a., denne gangen med \\(x_1 = 190\\) og \\(x_2 = 180\\). Fastslå effekten på p-verdien av å øke andelene (dersom differansen er uendret). Løsning Vi skal her teste hypotsen \\(H_0: p_1 - p_2 = 0\\) mot \\(H_1: p_1 - p_2 \\neq 0\\). Med \\(\\hat{p_1} = 100/200 = 0.5\\), \\(\\hat{p}_2 = 90/200 = 0.45\\) og \\(\\hat{p} = (100 + 90)/(200 + 200) = 0.475\\) er testobservatoren er gitt ved \\[Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}} = \\frac{0.5 - 0.45}{\\sqrt{0.475(1-0.475)(1/200 + 1/200)}}=1.00\\] Siden dette er en tosidig test er forkastningsområdet til testen \\(|Z| &gt; z_{\\alpha/2} = z_{0.025}=1.96\\). Siden 1 &lt; 1.96 kan vi altså ikke forkaste \\(H_0\\) ved 5 % signifikansnivå. P-verdier for tosidige tester kan være litt tricky å forstå. Vi er altså ute etter sannsynligheten for at noe “minst like ekstremt og til fordel for \\(H_1\\) intreffer”. For en tosidig test ville vi også reagert på store negative verdier av \\(Z\\), og “minst like ekstremt” i negativ retning ville vært \\(Z &lt; - 1.00\\). Altså blir p-verdien \\[\\text{P-value} = P(Z &lt; -1.00\\quad \\text{og/eller}\\quad Z &gt; 1.00)\\\\ = P(Z &lt; -1.00) + P(Z &gt; 1.00) = 2P(Z &gt; 1.00) = 2*(1 - 0.841) = 0.317\\] Regner på nytt ut \\(\\hat{p}_1 = 0.95, \\hat{p}_2 = 0.90, \\hat{p} = 0.925\\). Testobservatoren er da gitt ved \\[Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}} = \\frac{0.95 - 0.90}{\\sqrt{0.925(1-0.925)(1/200 + 1/200)}}=1.89\\] P-verdien er som før gitt ved \\[ \\text{P-value} = 2P(Z &gt; 1.89) = 2*(1 - 0.97) = 0.06 \\] Vi ser at p-verdien avtar når andelene blir større. Vi er her ganske nære på å forkaste \\(H_0\\) selv om differansen mellom \\(\\hat{p}_1\\) og \\(\\hat{p}_2\\) er den samme som i oppgave a. I nevneren til testobservatoren ser vi at variansen til differansen \\(\\hat{p}_1 - \\hat{p}_2\\) er proposjonal med \\(\\hat{p}(1-\\hat{p})\\). Denne vil bli mindre når \\(\\hat{p}\\) nærmer seg 1 (eller 0), og er størst når \\(\\hat{p} = 0.5\\). Vi er derfor sikrere på at differansen \\(\\hat{p}_1 - \\hat{p}_2\\) i oppgave b. ligger nær den sanne differansen i populasjon sammenlignet med oppgave a. Bevismateriale mot \\(H_1\\) øker (p-verdien går ned), men i dette tilfellet holder det ikke til å forkaste \\(H_0\\). En kredittutsteder gir kundene en rating basert på blant annet gjeld, formue og tidligere betalingsanmerkninger. Tanken er at en lav rating medfører høyere sannsynlighet mislighold av lånet sitt. For å sjekke om ratingen gir en reell indikasjon på mislighold, blir det registrert antall mislighold i et utvalg med rating under 800, og i et utvalg med rating 800 eller høyere. Rating &lt; 800 Rating &gt; 800 Utvalgsstørrelse 612 854 Mislighold 14 9 Kan vi konkludere med at personer med rating lavere enn 800 har høyere sannsynlighet for mislighold av lånet sitt sammenlignet med de med rating over 800? Bruk 5 % signifikansnivå. Løsning La \\(p_1\\) være sannsynlighet for mislighold for de med rating under 800 og la \\(p_2\\) være den samme sannsynligheten for de med rating over 800. Vi skal da teste \\(H_0: p_1 - p_2 = 0\\) mot det ensidige alternativet \\(H_1: p_1 - p_2 &gt; 0\\). Med \\(\\hat{p_1} = 14/612 = 0.0229\\), \\(\\hat{p}_2 = 0.0105\\), og \\(\\hat{p}=(14 + 9)/(612 + 854)=0.0157\\) er testobservatoren gitt ved \\[Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}} = \\frac{0.0229 - 0.0105}{\\sqrt{0.0157(1-0.0157)(1/612 + 1/854)}}=1.874\\] Forkastningsområdet er gitt ved \\(Z &gt; z_\\alpha = 1.645\\), og siden 1.874 &gt; 1.645 forkaster vi \\(H_0\\). Det er grunn til å tro at personer med rating lavere enn 800 har større sannsynlighet for mislighold. To konkurrerende selskaper, A og B, dominerer et marked for et bestemt produkt og har historisk sett hatt markedsandeler på hhv. 40 % og 50 % (altså 10 % til andre markeder). Selskap A gjennomfører så en markedsføringskampanje. For å avgjøre kampanjens effekt trekker et markedsanalyseselskap et tilfeldig utvalg av 300 kunder som spørres om deres produkt preferanse: Foretrukket selskap A B andre Frekvens 140 142 18 Har markedsandelene endret seg? Løsning Vi skal her test \\(H_0: p_1 = 0.4, p_2 = 0.5, p_3 = 0.1\\) mot \\(H_1:\\) minst to sannsynligheter er forskjellige. Utregning av testobservator blir da som følger: Foretrukket selskap \\(f_i\\) \\(e_i\\) \\((f_i - e_i)^2/e_i\\) A 140 \\(300\\times 0.4 = 120\\) 3.3333 B 142 \\(300\\times 0.5 = 150\\) 0.4266 andre 18 \\(300\\times 0.1 = 30\\) 4.8000 Forkastelsesområdet er \\(\\chi^2 &gt; \\chi^2_{\\alpha, k - 1} = \\chi^2_{0.05, 2} = 5.99\\). Siden \\(\\chi^2 = 8.564 &gt; 5.99\\) forkaster vi \\(H_0\\) på 5 % signifikansnivå. Et produksjonsselskap har mulighet til å bruke tre forskjellige kjemikalier for å fremstille det samme produktet. Lederen ved produksjonsselskapet ønsker å undersøke hvorvidt det er forskjell på hvor mange mangelfulle produkter som blir produsert når en varierer bruken av type kjemikalier. Hun gjør et tilfeldig utvalg på 700 produkter og klassifiserer dem som enten tilfredsstillende eller mangelfull. Funnene er oppsummert i tabellen nedenfor: Klassifisering/kjemikalie Kjemikalie 1 Kjemikalie 2 Kjemikalie 3 sum Tilfredstillende 268 216 164 648 Mangelfull 15 17 20 52 sum 283 233 184 700 Utfør en test for å avgjøre om det er en sammenheng mellom mangelfulle produkter og kjemikalie brukt under produksjon. Løsning Vi skal altså teste \\(H_0:\\) De to variablene klassfisering og kjemikalie er uavhengige mot \\(H_1:\\) Variablene klassifisering og kjemikalie er avhengige. Vi begynner med å regne ut de forventede frekvensene ved bruk av formelen \\(e_{ij} = f_{i.}f_{.j}/n\\): klas./kjem. Kjem 1 Kjem 2 Kjem 3 Tilfreds. \\(e_{11} = 648\\times 283/700 = 262.0\\) \\(e_{12} = 648\\times 233/700 = 215.7\\) \\(e_{13} = 648\\times 184/700 = 170.3\\) Mangel. \\(e_{21} = 52\\times 283/700 = 21.0\\) \\(e_{22} = 52\\times 233/700 = 17.3\\) \\(e_{23} = 52\\times 184/700 = 13.7\\) Så regner vi ut hvert element \\((f_{ij} - e_{ij})^2/e_{ij}\\) som skal inn i summen til test observatoren: klas./kjem. Kjem 1 Kjem 2 Kjem 3 Tilfreds. \\((268 - 262.0)^2/262 = 0.1384\\) 0.0004 0.2353 Mangel. 1.7255 0.0055 2.9327 Testobservatoren er da gitt ved \\[\\chi^2 = \\sum_{i = 1}^2\\sum_{j = 1}^3(f_{ij} - e_{ij})^2/e_{ij} = 0.1385 + 0.0004 + 0.2352 + 1.7255 + 0.0055 + 2.9327 = 5.0378\\] Forkastelsesområdet er \\(\\chi^2 &gt; \\chi^2_{\\alpha, (r - 1)(s - 1)} = \\chi^2_{0.05, 2} = 5.99\\). Siden \\(\\chi^2 = 5.0378 &lt; 5.99\\) forkaster vi ikke \\(H_0\\) på 5 % signifikansnivå. I R ville vi gjort følgende: f = matrix(c(268, 216, 164, 15, 17, 20), nrow = 2, ncol = 3, byrow = T) chisq.test(f) ## ## Pearson&#39;s Chi-squared test ## ## data: f ## X-squared = 5.038, df = 2, p-value = 0.08054 Individuell eksamen V21, oppgave 1 a - c: Løsning Løsningsforslag "],["relevante-r-testing.html", "3.6 Relevante R-kommandoer", " 3.6 Relevante R-kommandoer Under følger en liste over hvilke oppgaver du skal klare i R fra denne modulen. Vår policy fra og med vårsemesteret 2022 er at R-kommandoene under er tilstrekkelige for å løse oppgavene i datalabber og hjemmeeksamen i MET4. Det er med andre ord ikke nødvendig å lære seg teknikker utover det som er listet opp eksplisitt i listen under. Eventuelle nye teknikker som trengs for å løse en bestemt oppgave vil bli oppgitt og forklart dersom det er nødvendig. Det antas i tillegg at du kan den grunnleggende R-syntaksen som er dekket under Introduksjon til R. Antakelser om datasett Di kan gjøre de samme antakelsene om datasettet som i den tilsvarende oversikten i forrige modul: Datasettene er inneholdt i Excel-filer (.xls eller xslx) eller .csv-filer, som kolonner av variabler, med variabelnavn i første rad. \\(t\\)-tester Du må kunne gjøre de ulike \\(t\\)-testene ved hjelp av t.test()-funksjonen. vi bruker eksempeldatasettet testdata.xls som illustrasjon, som inneholder de to kolonnene X1 og X2. I bunn og grunn handler det om å hente ut de relevante kolonnene i datasettet som vektorer, og så å sette argumentene i t.test()-funksjonen riktig. # Tosidig, ett-utvalgs $t$-test for om forventningen til `X1` er lik 100: t.test(testdata$X1, mu = 100) # Samme testen, men ensidig med alternativhypotese om at $\\mu &gt; 100$ t.test(testdata$X1, mu = 100, alternative = &quot;greater&quot;) # For å gjøre ensidig test den andre veien bytter du ut `&quot;greater&quot;` med `&quot;less&quot;`. # Vanlig to-utvalgs $t$-test for om forventningen til `X1` og `X2` er like: t.test(testdata$X1, testdata$X2) # Vanlig to-utvalgs t-test der vi antar at variansen i de to populasjonene er like: t.test(testdata$X1, testdata$X2, var.equal = TRUE) # Parret to-utvalgs $t$-test, som bare gir mening dersom de to vektorene med # observasjoner er like lange, og dersom observasjonene er matchet opp slik at # første element i den første vektoren skal matches mot første elemenet i den # andre vektoret etc.: t.test(testdata$X1, testdata$X2, paired = TRUE) Vi kan bytte ut alternativhypotesen til ensidige tester over alt ved å sette alternative = til ønskelig verdi. Videre kan vi tenke oss tilfeller der observasjonsvektorene ikke er like lange, og for eksempel kommer i en excel-fil slik som denne der den ene kolonnen inneholder flere tall enn den andre. Det gjør ingenting. Vi kan lese inn datasettet på vanlig måte, og legger merke til at de “tomme” plassene i tabellen blir fylt med NA. Vi kan kjøre \\(t\\)-testene på vanlig måte. testdata2 &lt;- readxl::read_excel(&quot;ulik-lengde.xlsx&quot;) testdata2 t.test(testdata2$X1, testdata2$X2) ## # A tibble: 15 × 2 ## X1 X2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 5 ## 2 2 6 ## 3 3 7 ## 4 4 8 ## 5 5 9 ## 6 6 10 ## 7 7 11 ## 8 8 12 ## 9 9 13 ## 10 10 14 ## 11 11 NA ## 12 12 NA ## 13 13 NA ## 14 14 NA ## 15 15 NA ## ## Welch Two Sample t-test ## ## data: testdata2$X1 and testdata2$X2 ## t = -1, df = 22.975, p-value = 0.3277 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.603173 1.603173 ## sample estimates: ## mean of x mean of y ## 8.0 9.5 Varianstester Varianstest for ett uvalg er ikke bygget inn i R som en egen funksjon. Da må vi gjøre den “semi-automatisk” og regne ut testobservatoren ved å bruke formelen direkte. Her tester vi om variansen til X2-kolonnen i testdatasettet er lik 10: # Varianstest s &lt;- sd(testdata$X2) sigma_0 &lt;- 10 n &lt;- length(testdata$X2) # Testobservatoren T &lt;- (n-1)*s^2/sigma_0^2 # Kritiske verdier L &lt;- qchisq(0.025, df = n - 1) U &lt;- qchisq(0.975, df = n - 1) Vi må så sammenligne verdien av testobservatoren T med de kritiske verdiene L og U. Legg merke til at vi har brukt en funksjon i R (qchisq()) for å finne de aktuelle kvantilene i kjikvadratfordelingen. For to-utvalgs varianstest kan vi bruke funksjonen var.test(): var.test(testdata$X1, testdata$X2) Vi kan også her gjøre ensidige tester ved å sette alternative =-argumentet til enten \"less\" eller \"greater\". Tester for andeler Både for ett-utvalgs og to-utvalgs test for andeler kan vi fylle inn tall direkte i testobservatoren på denne måten (sett inn for de nødvendige verdiene): # Ett-utvalgs test for andeler p_hatt &lt;- mean(testdata$A1) p_null &lt;- 0.5 n &lt;- length(testdata$A1) Z &lt;- (p_hatt - p_null)/sqrt(p_null*(1-p_null)/n) # To-utvalgs test for andeler p1_hatt &lt;- mean(testdata$A1) p2_hatt &lt;- mean(testdata$A2) n1 &lt;- length(testdata$X1) n2 &lt;- length(testdata$X2) p_hatt &lt;- mean(c(testdata$A1, testdata$A2)) # &lt;- Regner ut felles p_hatt Z &lt;- (p1_hatt - p2_hatt)/sqrt((1/n1 + 1/n2)*p_hatt*(1-p_hatt)) Testobservatorene må da sammenlingnes med relevant kvantil i normalfordelingen, som du enten finner i tabell i læreboken eller ved å bruke funksjonen qnorm() med ønsket kvantil som argument. Kjikvadrattester for goodness-of-fit Hvis vi skal teste for goodness-of-fit for en fordeling kan vi raskt regne ut testobservatoren direkte. Vi bruker eksempelet fra forelesningsvideoen: p0 &lt;- c(0.45, 0.40, 0.15) # Fordeling under H0 f &lt;- c(102, 82, 16) # Observerte frekvenser e &lt;- p0*sum(f) # Forv. frekv. under H0 T &lt;- sum((f - e)^2/e) # Testobservator c &lt;- qchisq(.95, df = 2) # Kritisk verdi # Forkast H0? T &gt; c Alternativt så kan vi bruke chisq.test()-funksjonen direkte. Pass på å bruke riktige argumentnavn (x = og p =) når du skal gjøre goodness-of-fit: # Eventuelt direkte chisq.test(x = f, p = p0) Dersom du skal teste for uavhengighet så kan du anta at datasettet kommer som en ferdig kontingenstabell, med den ene gruppeinndelingen langs kolonnene, og den andre gruppeinndelingen langs radene. En slik tabell kan du sende rett inn i chisq.test()-funksjonen for å gjøre en test for uavhengighet: # Test for uavhengighet immigration &lt;- read_xls(&quot;immigration-wide.xls&quot;, range = &quot;B2:E12&quot;, col_names = c(&quot;many&quot;, &quot;some&quot;, &quot;few&quot;, &quot;none&quot;)) chisq.test(immigration) Kjikvadrattester for uavhengighet Som eksempel kan vi ta oppgaven fra Dataøving 2. Vi starter med å lese inn dataene: # les inn data library(readxl) violence &lt;- read_excel(&quot;violence.xlsx&quot;) Vi bruker select() til å velge de to variablene vi er interessert i og funksjonen table() til å lage en krysstabell: violence_redusert &lt;- violence %&gt;% select(violent_treatment, experienced_violence) krysstabell &lt;- table(violence_redusert) krysstabell ## experienced_violence ## violent_treatment Less Violent Violent ## Less Violent 114 9 ## Violent 33 93 Det ser ut til at det er en klar sammenheng mellom faktisk og opplevd voldelighet ved at de fleste forsøkspersonene havner på diagonalen i krysstabellen over. Nå skal vi utføre en test av nullhypotesen \\(H_0\\): faktisk og opplevd voldelighet er uavhengige kjennetegn, mot den alternative hypotesen \\(H_1:\\) at de er avhengige kjennetegn. Dette er en chikvadrattest som kan utføres i R på følgende måte: chisq.test(krysstabell) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: krysstabell ## X-squared = 111.06, df = 1, p-value &lt; 2.2e-16 og vi ser at vi får en soleklar forkastning. Dette er også logisk gitt hva vi faktisk tester her. Kritiske verdier i R Som vi har sett over baserer tester i R seg stort sett på en eller annen datainput. Som output får du både testobservator og p-verdi. Hvis du allikevel av en eller annen grunn (f.eks. på eksamen) trenger å finne en kritisk verdi i en fordeling, så kan det være nyttig å vite hvordan man gjør dette i R. De kritiske verdiene finner vi ved å bruke kvantilfunksjonene til den respektive fordelingen. Disse funksjonene har alltid et argument p og har som output det tallet som har sannsynlighetsmasse p til venstre for seg. Du må derfor velge p ut fra signifikansnivå og om det er en ensidig/tosidig test. Kritiske verdier i T-fordelingen Dersom du f.eks skal utføre en t-test for \\(H_0: \\mu = \\mu_0\\) mot \\(H_1: \\mu &lt; \\mu_0\\), med signifikansnivå \\(\\alpha = 0.05\\) og har \\(40\\) observasjoner er kritisk verdi: qt(0.05, df = 40 - 1) ## [1] -1.684875 og du forkaster da dersom din testobservator \\(T\\) er mindre enn dette tallet. Hadde alternativ hypotesen vært \\(H_1: \\mu &gt; \\mu_0\\) ville kritisk verdi vært qt(1 - 0.05, df = 40 - 1) ## [1] 1.684875 og du forkaster dersom din testobservator \\(T\\) er større enn dette tallet. For en to-sidig test (\\(H_1: \\mu\\neq \\mu_0\\)) forkaster du dersom \\(T\\) faller uten for intervallet: L &lt;- qt(0.05/2, df = 40 - 1) U &lt;- qt(1 - 0.05/2, df = 40 - 1) c(U,L) ## [1] 2.022691 -2.022691 Kritiske verdier i standardnormalfordelingen Helt likt som for t-fordelingen bare at det ikke er noen frihetsgrader involvert: # Ensidig mu &lt; mu0, alpha = 0.05 qnorm(0.05) ## [1] -1.644854 # Ensidig mu &gt; mu0, alpha = 0.05 qnorm(1 - 0.05) ## [1] 1.644854 # Tosidig L &lt;- qnorm(0.05/2) U &lt;- qnorm(1 - 0.05/2) c(U,L) ## [1] 1.959964 -1.959964 Alternativt er det bare å pugge tallene \\(1.64\\) og \\(1.96\\). Kritiske verdier i \\(\\chi^2\\)-fordelingen # Kritisk verdi, 5% nivå, ensidig test, nedre hale, 30 frihetsgrader qchisq(0.05, df = 30) ## [1] 18.49266 og du forkaster dersom testobservatoren er mindre enn dette tallet. # Kritisk verdi, 5% nivå, ensidig test, øvre hale, 30 frihetsgrader qchisq(1 - 0.05, df = 30) ## [1] 43.77297 og du forkaster dersom testobservatoren er større enn dette tallet. # Kritisk verdi, 5% nivå, tosidig test, 30 frihetsgrader L &lt;- qchisq(0.05/2, df = 30) U &lt;- qchisq(1 - 0.05/2, df = 30) c(L, U) ## [1] 16.79077 46.97924 og du forkaster nullhypotesen dersom testobservatoren havner utenfor dette intervallet. Kritiske verdier i F-fordelingen F-fordelingen trenger vi når vi skal teste om to varianser er like. Skal du gjøre denne testen manuelt er det lurt å sette den største variansen i telleren av testobservatoren \\[F = \\frac{S_{1}^2}{S_{2}^2}.\\] Uavhengig om det er en ensidig eller tosidig test, trenger vi da bare å sjekke om testobservatoren overstiger den kritiske verdien i den øvre halen i F-fordelingen. Har vi brukt \\(30\\) observasjoner til å regne ut \\(S_{1}^2\\) og \\(25\\) observasjoner til å regne ut \\(S_{1}^2\\) blir kritisk verdi i en to-sidig test der \\(H_1: \\sigma_{1}^2 \\neq \\sigma_{2}^2\\) : # Kritisk verdi, 5% nivå, tosidig test qf(1 - 0.05/2, df1 = 30 - 1, df2 = 25 - 1) ## [1] 2.217443 Er det en ensidig test hvor \\(H_1: \\sigma_{1}^2 &gt; \\sigma_{2}^2\\) blir kritisk verdi: # Kritisk verdi, 5% nivå, ensidig test qf(1 - 0.05, df1 = 30 - 1, df2 = 25 - 1) ## [1] 1.945259 Husk: Du kan definere hva som er \\(S_{1}^2\\) og \\(S_{2}^2\\), og med strategien over definerer du alltid \\(S_{1}^2\\) til være den av variansene som er størst. "],["regresjon.html", " 4 Regresjon", " 4 Regresjon I forrige modul fokuserte vi på binære spørsmål av typen “Er det en forskjell mellom disse to populasjonene, eller ikke?”, “Er disse kjennetegnene uavhengige, eller ikke?”, og så videre. I denne modulen skal vi prøve å gå et steg lenger og tillate mer interessante spørsmål. I stedet for bare å spørre om en eller annen effekt er til stede (eller ikke), så vil vi heller finne ut hvor stor denne effekten er, hvilken retning den går, og kanskje om vi kan bruke kunnskapen vi får om statistiske sammenhenger til å si noe fornuftig om hva som vil skje for noe som vi enda ikke har observert. Da er det regresjon som gjelder, og mer spesifikt for vår del: lineær regresjon. Regresjon er et hovedtema i MET4. Vi innfører en statistisk modell som i sin enkleste form sier at en forklaringsvariabel \\(X\\) henger sammen med en responsvariabel \\(Y\\) på en helt bestemt måte, nemlig gjennom ligningen \\[Y = \\beta_0 + \\beta_1 X + \\epsilon.\\] Ligningen over sier at det er en lineær sammenheng mellom \\(X\\) og \\(Y\\), men at det i tillegg kommer en uforutsigbar støyvariabel \\(\\epsilon\\) som gjør at vi ikke vil kunne observere den lineære sammenhengen direkte. Det vi derimot kan gjøre, er å bruke de observerte \\(X\\)er og \\(Y\\)er til å finne ut hvilke verdier av \\(\\beta_0\\) og \\(\\beta_1\\) som passer best. Til det bruker vi minste kvadraters metode, som beskrevet i videoforelesningene i denne modulen. Vi deler arbeidet med regresjon inn i tre deler. I den første (og største) delen går vi grundig gjennom ulike sider vi den enkle lineære regresjonsmodellen over. I den andre delen ser vi på multippel regresjon som er en utvidelse av enkel regresjon der vi tillater flere forklaringsvariabler på høyre side av likhetstegnet, og i den tredje delen ser vi på ulike praktiske aspekter ved regresjonsmodellering og modellbygging. I videoforelesningene går vi gjennom noen slides, og vi skriver et R-skript. Du kan laste disse ned ved å klikke på lenkene under: Slides til “Regresjon” R-script til “Regresjon” TIPS: Hvis du ønsker å laste ned lysbildene som PDF trykker du på linken over, velger “Skriv ut”, og så skriver du ut som PDF. Før du gjør det bør du scrolle gjennom alle sidene slik at ligningene vises korrekt. "],["enkel-regresjon.html", "4.1 Enkel regresjon", " 4.1 Enkel regresjon 4.1.1 Videoforelesninger 4.1.2 Kommentarer Vi har sett på en del figurer som illustrerer noen pedagogiske poenger, og lærebokens kapittel 16 går detaljert til verks når de beskriver de ulike læringsmomentene: I kapittel 16.1 kan vi lese mer om den statistiske modellen som vi kaller enkel regresjon. I kapittel 16.2 introduseres minste kvadraters metode for å estimere regresjonskoeffisientene ved hjelp av data. De viser til og med hvordan det kan gjøres manuelt ved hjelp av bildatasettet, men det er selvsagt kun for å illustrere hvodan formlene ser ut. Vi estimerer ved hjelp av R, og vi har sett i videoforelesningen hvordan vi gjør det ved hjelp av lm()-funksjonen. Det som gjør regresjon til et statistisk problem er feilleddet \\(\\epsilon\\). Vi tenker oss at for en gitt verdi av \\(X\\), så vil «naturen» regne ut verdien av \\(Y\\) ved å regne ut den lineære sammenhengen \\(Y = \\beta_0 + \\beta_1 X\\), og så legge til støyvariabelen \\(\\epsilon\\) som trekkes fra en sannsynlighetsfordeling. Vi kan ikke observere direkte hvilke \\(\\epsilon\\) som «naturen» har «trukket» (for da ville vi med en gang kunne regnet oss frem til verdiene av \\(\\beta_0\\) og \\(\\beta_1\\)). For gitte estimater av regresjonskoeffisientene \\(\\widehat \\beta_0\\) og \\(\\widehat \\beta_1\\) (som vi kan finne f.eks. ved hjelp av minste kvadraters metode), så kan vi regne ut de observerte residualene \\[\\widehat\\epsilon_i = Y_i - \\widehat Y_i = Y_i - (\\widehat \\beta_0 + \\widehat \\beta_1 X_i).\\] Ved å analysere residualene kan vi si mer om f.eks Er det egentlig en lineær sammenheng mellom \\(X\\) og \\(Y\\)? Hvis det er mønstre og sammenhenger i de observerte residualene, tyder det på at den enkle lineære modellen ikke fanger opp hele sammenhengen mellom \\(X\\) og \\(Y\\). Vi kan gå mer spesifikt til verks: nøyaktig hvilke antakelser om residualene er ser ut til å være brutt? I senere økonometrikurs vil dere kunne lære mer om hvordan vi håndterer de ulike problemene. Hvor stor er variansen til \\(\\epsilon\\)? Det brukes videre til å sette opp den viktige signifikanstesten for om stigningstallet i regresjonen er forskjellig fra null. Alt dette behandles grudig i bokens kapittel 16.3–16.6. Her bør teksten leses godt. Kode til bileksempelet finnes i scriptet som følger med videoforelesningene. Når det gjelder enkel regresjon kan du sjekke om du har fått med deg det vesentligste ved å diskutere følgende spørsmål: Hva er responsvariabelen og hva er forklaringsvariabelen i enkel regresjon? Hva er fortolkningen av de to regresjonskoeffisientene? Hvilket prinsipp er det vi legger til grunn når vi skal bestemme (estimere) verdien av koeffisientene ved hjelp av data? Skriv opp formlene for koeffisientestimatene. Kan du gi en intuitiv fortolkning av disse? Er de rimelige? Kan du ved hjelp av formelen for \\(\\widehat\\beta_1\\) utlede sammenhengen mellom stigningstallet \\(\\beta_1\\) og korrelasjonskoeffisienten* mellom \\(X\\) og \\(Y\\)? Hvilken rolle spiller feilleddet (\\(\\epsilon\\))? Skriv opp de 4 + 1 forutsetningene. Når må den siste være oppfylt? Når kan vi klare oss uten? Hva er testobservatoren når vi tester H\\(_0: \\beta_1 = 0\\)? Kan du holde styr på de fire standardavvikene vi har jobbet med i denne forelesningen? Hva mener vi med å diagnostisere en regresjonsmodell? Hva er \\(R^2\\), og hva måler den? Hva sier \\(R^2\\) ikke noe om? Her er noen grunnleggende ferdigheter fra kapittel 16. Klarer du dette? Bruke til å tilpasse en enkel regresjonsmodell for et datasett? Bruke til å skrive ut oversiktlige regresjonstabeller? Tolke en regresjonsutskrift? Hente ut relevant informasjon etter en slik tilpasning? Bruke informasjon fra regresjonsutskriften til å regne ut antall stjerner for hånd? Lage diagnoseplott i ? Diagnistisere en modell? Identifisere innflytelsesrike observasjoner? "],["multippel-regresjon.html", "4.2 Multippel regresjon", " 4.2 Multippel regresjon 4.2.1 Videoforelesninger 4.2.2 Kommentarer I kapittel 17 utvides regresjonsbegrepet til multippel regresjon, som i prasis betyr at vi kan ha flere enn en forklaringsvariable: \\[Y = \\beta_0 + \\beta_1X_1 + \\cdots \\beta_kX_k + \\epsilon,\\] men utover dette er alle detaljene vi har snakket om de samme. For eksempel: Tolkningen av regresjonskoeffisienten: En endring på en enhet i forklaringsvariabelen \\(X_j\\) henger sammen med \\(\\beta_j\\) enhets endring i responsvariabelen \\(Y\\) (merk at jeg ikke brukker begrepet “fører til”, vi kan ikke uten videre fortolke sammenhengen som kausal!). Analysen av residualene \\(\\widehat \\epsilon_i = Y_i - \\widehat Y_i\\) er den samme og har samme formål 1–3 som over. \\(R^2\\) har samme fortolkning. R-kommandoen er den samme, vi bare sette pluss mellom forklaringsvariablene, f.eks reg &lt;- lm(Y ~ X1 + ... + Xk, data = x) I tillegg innfører vi noen nye begreper: Justert \\(R^2\\): Vi viste i forelesningen at vi vil alltid klare å øke \\(R^2\\) ved å legge til forklaringsvariable, selv om de ikke har noe med problemet å gjøre. Derfor innførte vi en justert \\(R^2\\) som tar høyde for nettopp dette, ved å bli større bare dersom den aktuelle forklaringsvariebelen faktisk forklarer en reell mengde av variasjonen i responsvariabelen. Se avsnitt 17-2f i læreboken. Multikolinearitet: Dersom en forklaringsvariabel er sterkt korrelert med en eller flere andre forklaringsvariabler har vi multikolinearitet. Det blir naturlig nok et problem å skille effekter fra hverandre når de i realiteten er helt eller nesten like. Ekstremtilfellet er perfekt multikolinearitet der en variabel er en eksakt lineær funksjon av en eller flere andre variable. Det typiske tilfellet er at vi har to kolonner der vi måler det samme fenomenet, men med to ulike enheter, f.eks. cm og m. Selvsagt kan vi ikke klare å identifisere en separat og uavhengig effekt av \\(X\\) på \\(Y\\) om vi skifter måleenhet, og vi vil få en feilmelding dersom vi prøver på det. Det er ekvivalent med å dele på null (every time you divide by zero, God kills a kitten!). Løsning: fjern en av kolonnene fra regresjonsanalysen. Verre er det om to variable måler nesten det samme, men ikke helt, som i skoledataeksempelet der vi kunne bruke både innbyggertall og antall femteklassinger i kommunen som forklaringsvariabler. De henger tett sammen, men selvsagt ikke eksakt, og det virker rart å kunne knytte separate efekter til disse to variablene. I dette tilfellet får vi likevel ikke feilmeldinger, men konsekvensen kan fort bli at standardavvikene (usikkerheten!) til koeffisientestimatene eksploderer, og at ingen av variablene blir signifikant forskjellige fra null, selv det det faktisk er en sterk sammenheng mellom kommunestørrelse og prøveresultat (husk at testobservatoren: \\(t = \\widehat \\beta_k/\\sigma_{\\beta_k}\\) blir liten når nevneren blir stor). F-test for multiple sammenligninger: Dette henger nøye sammen med variansanalyse (analysis of variance, ANOVA), som nå er tatt ut av pensum i kurset. For å forstå dette kan vi sette opp et eksempel, med to forklaringsvariabler: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2.\\] Etter å ha brukt miste kvadraters metode for å estimere de tre koeffisientene er vi kanskje interessert i å vurdere den statsistiske signifikansene til de to stigningstallene separat. Da tester vi de to nullhypotesene \\(\\beta_1 = 0\\) og \\(\\beta_2 = 0\\), som vi i praksis gjør ved å se på hvor mange stjerner de får i regresjonsutskriften. Men sett at ingen av koeffisientene er signifikant forskjellige fra null, kan vi da slutte at vi ikke kan forkaste hypotesen \\(\\beta_1 = \\beta_2 = 0\\), dvs at begge koeffisientene er lik null, og at ingen av forklaringsvariablene forklarer variasjon i \\(Y\\)? NEI, det kan vi ikke. Vi kan for eksempel lett tenke oss at vi på grunn av multikolinearitet ikke får separate forkastninger av de to nullhypotesene, men at ved å fjerne en variabel, så blir den andre signifikant. For å virkelig forstå dette problemet kan du godt lese starten på kapittel 14.1 samt kapittel 14.2 om multiple sammenligninger (som strengt tatt ikke er pensum), men essensen er altså: \\[\\textrm{Å forkaste H}_0: \\beta_1 = 0 \\textrm{ og H}_0: \\beta_2 = 0 \\textrm{ er ikke det samme som å forkaste H}_0: \\beta_1 = \\beta_2 = 0!\\] For å gjennomføre den siste testen må vi sette opp en egen testobservator, som viser seg å være \\(F\\)-fordelt. Læreboken lister opp noen detaljer i avsnitt 17-2f, og essensen er at vi setter opp en brøk på formen \\[F = \\frac{\\textrm{Variasjon i } Y \\textrm{ som fanges opp av regresjonsmodellen med } X_1 \\textrm{ og }X_2}{\\textrm{Variasjon i } Y \\textrm{ som fanges opp av regresjonsmodellen uten } X_1 \\textrm{ og }X_2}.\\] Dersom denne brøken viser seg å være stor (som definert av signifikansnivå og frihetsgrader, se lærebok), forkaster vi nullhypotesen om at begge koeffisientene begge kan være lik null. I en generell multippel regresjon med \\(k\\) forklaringsvariable rapporterer R F-statistic: etc, med verdien av \\(F\\)-observatoren i testen for \\[H_0: \\beta_1 = \\cdots = \\beta_k = 0,\\] og dersom den oppgitte \\(p\\)-verdien er mindre enn f. eks. 5%, kan vi slutte at ikke alle koeffisientene kan være null samtidig (selv om ingen av koeffisientene i seg selv nødvendigvis er signifikant forskjellig fra null). Som en såkalt fun fact kan vi nevne at det er enkelt å teste for signifikansen til grupper av variable på denne måten, f.eks hvis det er noen variable som måler lignende ting (si \\(X_2, X_4\\) og \\(X_5\\)). I R kan du estimere to modeller, en modell som inkluderer variablene (f.eks. reg_stor) og en modell der du tar bort de aktuelle variablene (f.eks. reg_liten). Du kan da kjøre kommandoen anova(reg_stor, reg_liten) for å teste \\[H_0: \\beta_2 = \\beta_4 = \\beta_5 = 0.\\] Kritikk av læreboken: Læreboken har en tabell på s. 701 som viser sammenhengen mellom ulike statistiske størrelser som vi kan regne ut for en regresjonsmodell. \\(R^2\\) kjenner vi som forklaringsgraden, \\(s_{\\epsilon}\\) er standardavviket til residualene, \\(F\\) er testobservatoren for modellgyldighet som vi definerte uformelt over, og som er definert formelt nederst på s. 700, mens SSE (Sum of Squares Error) henger nøye sammen med standardavviket, som vi også kan se på s. 700. På disse sidene ser vi mange ligninger som viser hvordan disse størrelsene formelt henger sammen, og i tabellen på s. 701 ser vi blant annet at dersom SSE er liten, er også \\(s_{\\epsilon}\\) liten, \\(R^2\\) er nær null, og \\(F\\)-observatoren er stor. Det er greit nok, men de har en ekstra kolonne som slår fast at regresjonsmodellen er good. Her menes det ikke at regresjonsmodellen er god i den forstand at vi skal reagere med glede eller lettelse (slik noen gjerne gjør), men at variasjonen i datamaterialet i stor grad lar seg forklare av modellen vår. I et tenkt eksempel der den sanne sammenhengen mellom \\(Y\\) og \\(X\\) er gitt ved \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), men der \\(\\beta_1\\) er forholdsvis liten og \\(s_{\\epsilon}\\) er relativt stor, vil f.eks. \\(R^2\\) bli liten, selv om den enkle lineære regresjonsmodellen repsesenterer sannheten og av alle tenkende mennesker må sies å være god. Det er desverre mange lærebøker som blander disse to fortolkningene, ikke gjør det! Her er enda noen grunnleggende begreper. Har du fått med deg dette? Hva mener vi med at en observasjon er innflytelsesrik? Hva er grunnen til at vi trenger justert \\(R^2\\) med flere forklaringsvariable? Hva er forskjellen på perfekt og tilnærmet multikolinearitet i lineær regresjon? Hva blir konsekvensen i hvert av tilfellene? Kan du gi en praktisk og intuitiv forklaring på hvorfor multikolinearitet nødvendigvis må være et problem? Hva er forskjellen på statistisk og økonomisk signifikans? Kan du sette opp konkrete eksempler der vi kan estimere statistisk signifikante, men ikke økonomisk signifikante effekter i multippel regresjon? Hva med den motsatte situasjonen, økonomisk signifikant, men ikke statistisk signifikant? Grunnleggende ferdigheter: Klarer du dette? Bruke R til å tilpasse en multippel regresjonsmodell for et datasett? Bruke R til å finne særlig innflytelsesrike observasjoner? Tolke en multippel regresjonsutskrift? "],["modellbygging.html", "4.3 Modellbygging", " 4.3 Modellbygging 4.3.1 Videoforelesninger 4.3.2 Kommentarer Kapittel 18 dekker de grunnleggende begrepene innen modellbygging. I kap. 18.1 snakkes det om polynomiske modeller, i kap. 18.2 behandles dummyvariabler. Kapittel 18.3 og 18.4 handler om hvordan vi i praksis kan jobbe for å velge ut variable i en gitt situasjon. Forelesningene dekker i grunn greit det vi skal få med oss her. Som en sjekk om du har fått med deg det vesentlige, kan du svare på følgende spørsmål: Vi har lært tre typer log-transformasjoner. Hva blir fortolkningen av koeffisientene for hver av disse? Kan du nevne tre gode grunner til at log-transformasjoner er nyttige? Hvorfor sier vi at følgende modell er lineær? \\(Y = \\beta_0 + \\beta_1X + \\beta_1X^2 + \\varepsilon\\) Vil vi ikke få problemer med multikolinearitet i modellen over? Nevn en veldig god grunn til at vi må være ytterst forsiktig med polynomtransformasjoner. Hva er en dummyvariabel? Hva er fortolkningen av regresjonskoeffisienten til en dummyvariabel? Hva er fortolkningen av regresjonskoeffisienten til et interaksjonsledd mellom målevariabelen \\(X\\) og dummyvariabelen \\(D\\)? Et utrolig viktig poeng, men bruk tid til å tenke over og formulere et svar: Hvorfor er det viktig å tenke på multippel testing i sammenheng med variabelutvelgelse? Grunnleggende ferdigheter: Klarer du dette? Bruke logtransformasjoner i R? Bruke poynomtransformasjner i R? Sette opp en fornuftig regresjonsmodell ved å ta utgangspunkt i et datasett og et analyseformål, og argumentere godt for dine valg? Denne ferdigheten har blitt testet på hver eneste hjemmeeksamen i manns minne! "],["oppgaver-1.html", "4.4 Oppgaver", " 4.4 Oppgaver 4.4.1 Regresjon med en forklaringsvariabel Oppgave 1 Forskere har brukt statistikk til å undersøke om TV-titting er forbundet med overvekt. De har samlet inn data fra 15 10-åringer om antall timer TV-titting per uke og antall kilo overvekt hos barnet (rapportert som differanse fra normalvekt). De innsamlede dataene er oppsummert i tabellen: TV_titting Overvekt 42 17 35 5 28 -1 34 0 37 13 38 15 32 5 33 7 18 -7 28 7 36 6 29 7 29 4 34 15 18 -5 Bruk R til å lage et spredningsplott av resultatene. Hva tror du om forholdet mellom de to variablene utfra figuren? Sett opp regresjonsuttrykket for å undersøke om overvekt er forbundet med TV-titting. La overvekt være responsvariabelen. Hva er betydningen av hver parameter i uttrykket? Bruk R til å estimere parameterne. Hva kan koeffisientene fortelle deg om forholdet mellom overvekt og TV-titting? Beregn et 95 % konfidensintervall for \\(\\beta_1\\). Hint: Bruk ‘summary()’ på regresjonsmodellen for å finne \\(S(\\hat{\\beta}_1)\\) som da gitt i kolonnen “Std. Error”. Bruk R til å beregne et 95 % prediksjonsintervall for overvekt i kilo for et barn som ser på TV 30 timer i uken. Hva forteller intervallet deg? Bruk R til å beregne et 95 % konfidensintervall for gjennomsnittlig overvekt for barn som ser 30 timer på TV i uken. Hvordan er tolkningen av dette intervallet forskjellig fra det i oppgave d? Hva er forventet overvekt for barn som ser 0 timer på TV i uken utfra modellen? Hva kan være problematisk ved å gjøre denne type analyser av en regresjonsmodell? Løsning df_tv &lt;- data.frame( TV_titting = c(42, 35, 28, 34, 37, 38, 32, 33, 18, 28, 36, 29, 29, 34, 18), Overvekt = c(17, 5, -1, 0, 13, 15, 5, 7, -7, 7, 6, 7, 4, 15, -5)) plot(df_tv$TV_titting, df_tv$Overvekt, type = &quot;p&quot;, xlab=&quot;TV-titting&quot;, ylab=&quot;Overvekt&quot;) Plottet viser en ganske tydelig trend om at TV-titting og overvekt er relaterte. Dette kan vi undersøke nærmere. Vi setter overvekt som responsvariabel og TV-titting som forklaringsvariabel. Uttrykket blir da: \\[\\begin{equation} \\text{Overvekt} = \\beta_0 + \\beta_1 \\text{TV-titting} + \\epsilon \\end{equation}\\] Ser vi bort fra dataene ville \\(\\beta_0\\) være forventet overvekt for personer som ikke ser på tv. Av spredningsplottet ser det derimot ut som at vi ikke har data ved 0 TV-titting og det er derfor ikke fornuftig med en direkte tolkning av denne parameteren. Under antagelsen om at regresjonsmodellen er gyldig forteller \\(\\beta_1\\) hvor mye vi forventer at overvekten øker dersom man øker TV-tittingen med 1 time. Vi estimerer en regresjonsmodell fra dataene og skriver ut tabellen med resultatene: reg &lt;- lm(Overvekt ~ TV_titting, df_tv) summary(reg) ## ## Call: ## lm(formula = Overvekt ~ TV_titting, data = df_tv) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.1917 -2.6147 0.2795 2.6785 6.8083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -22.2124 5.1359 -4.325 0.000824 *** ## TV_titting 0.8942 0.1602 5.583 8.88e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.026 on 13 degrees of freedom ## Multiple R-squared: 0.7057, Adjusted R-squared: 0.683 ## F-statistic: 31.17 on 1 and 13 DF, p-value: 8.88e-05 # For en finere utskrift kan du bruke følgende: # library(stargazer) # stargazer(reg, type=&quot;text&quot;) Det er tydelig sigifikans for at hver av parameterne er ulik null. For \\(\\beta_1\\) betyr dette at trenden vi observerte i spredningplottet var signifikant. Videre kan vi tolke det positive fortegnet til \\(\\beta_1\\) som at flere timer TV-titting er relatert med økt overvekt. Merk at vi ikke kan si utfra dataene at mer TV-titting fører til overvekt. En mer riktig tolkning er å si at de forekommer samtidig i populasjonen. (Dersom vi ønsket å finne ut av kausaliteten måtte man på et tilfeldig utvalg av barn satt noen til å se mye på TV og noen til å se mindre på TV, og deretter undersøkt om dette resulterte i statistisk signifikant høyere overvekt hos en av gruppene. Dette krysser imidlertid noen etiske grenser om å påføre barn overvekt, dersom hypotesen er sann.) Fra R utskriften ser vi at \\(S(\\hat{\\beta}_1)=0.1602\\). Videre er \\(t_{0.025, 13} = 2.16\\). Et 95 % konfidensintervall for \\(\\beta_1\\) er da gitt ved: \\[\\left[\\hat{\\beta}_1 - t_{\\alpha/2, n - 2}S(\\hat{\\beta}_1), \\hat{\\beta}_1 + t_{\\alpha/2, n - 2}S(\\hat{\\beta}_1)\\right]\\\\ =\\left[0.8942 - 2.16\\times0.1602, 0.8942 + 2.16\\times0.1602\\right]\\\\ =\\left[0.5481, 1.2402\\right]\\] e) reg.pred &lt;- predict(reg, newdata = data.frame(TV_titting = c(30)), interval = &quot;predict&quot;) reg.pred ## fit lwr upr ## 1 4.614735 -4.380262 13.60973 # stargazer(reg.pred, type=&quot;text&quot;) Dersom vi trekker et nytt barn som ser 30 timer på TV per uke, vil barnet i 95 % av tilfellene være innenfor prediksjonsintervallet dersom vi hadde gjentatt dette mange ganger. reg.conf &lt;- predict(reg, newdata = data.frame(TV_titting = c(30)), interval = &quot;confidence&quot;) reg.conf ## fit lwr upr ## 1 4.614735 2.317582 6.911888 #evt # library(stargazer) # stargazer(reg.conf, type=&quot;text&quot;) Konfidensintervallet viser hvor gjennomsnittet av målinger på et nytt sett med barn som ser 30 timer på TV per uke vil ligge i 95 % av nye eksperimenter. reg.zero &lt;- predict(reg, newdata = data.frame(TV_titting = c(0)), interval = &quot;prediction&quot;) reg.zero ## fit lwr upr ## 1 -22.21237 -36.30997 -8.11477 # evt # library(stargazer) # stargazer(reg.zero, type=&quot;text&quot;) Beste gjetning er at barn som ser 0 timer på TV i uken er 22.2 kg under normalvekt (!). Et kjapt søk viser at barn på 10 år veier mellom 25.5 og 39 kg. Det høres urimelig ut at veldig lite TV-titting har sammenheng med ekstrem underernæring blant den samme populasjonen av barn som ser rundt 30 timer på TV i uken. Det er flere ting som kan gå galt når man gjør en slik analyse av en modell: Modellen kan være gal. Vi antar et lineær forhold mellom TV-titting og overvekt, noe som ikke trenger å være riktig ved 0 timer TV-titting. Datasettet dekker ikke den gruppen barn som ser 0 timer på TV, så med mindre modellen er helt riktig (noe den sjelden er) vil den ikke kunne generalisere så langt utenfor området vi estimerte parameterne på. Med mindre forholdet mellom TV-titting og overvekt er kausalt, kan det være at sammenhengen som er observert mellom dem i dataene ikke vil være det samme langt utenfor det aktuelle data-området. Oppgave 2 Vi skal undersøke om alder har sammenheng med hvor lang tid man bruker på et puslespill. Vi har data fra et tilfeldig utvalg av 210 voksne personer om alder og tid brukt på oppgaven. Uttrykk alder som \\(X\\) og tid i minutter som \\(Y\\). Følgende deskriptive statistikker er beregnet for datasettet: \\(s_{xy}= 8, s_x^2, = 110, s_y^2 = 42, \\bar{x} = 40, \\bar{y} = 20\\) hvor \\(\\bar{x},\\bar{y}\\) er gjennomsnittene. Sett opp regresjonsuttrykket for sammenhengen mellom \\(X\\), \\(Y\\) og støy \\(\\epsilon\\). Hva er antagelsen for sammenhengen mellom X og \\(\\epsilon\\)? Hva er uttrykket for beste gjetning/prediksjon \\(\\hat{Y}_i\\) gitt en verdi av forklaringsvariabelen \\(X_i\\) for regresjonsmodellen du har satt opp? Hva er uttrykket for residualene? Vis at hva uttrykket for regresjonsparameterne blir når de estimeres med minste kvadraters metode. Regn ut minste kvadraters estimat av parameterne. Løsning Regresjonsuttrykket er \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X + \\epsilon \\end{equation}\\] Vi antar at forklaringsvariabelen \\(X\\) er uavhenging støyen \\(\\epsilon\\). Dermed vil også kovariansen være null, \\[\\begin{equation} \\text{Cov}(X, \\epsilon) = 0. \\end{equation}\\] Beste gjetning er forvetningen betinget på utfallet av forklaringsvariabelen \\(X\\): \\[\\begin{align} \\hat{Y}_i &amp;= E[Y|X = X_i] = E[\\beta_0 + \\beta_1 X + \\epsilon|X = X_i] \\\\ &amp;= \\beta_0 + \\beta_1 X_i + E[\\epsilon|X = X_i] \\\\ &amp;= \\beta_0 + \\beta_1 X_i + E[\\epsilon] \\\\ &amp;= \\beta_0 + \\beta_1 X_i \\end{align}\\] Residualene er forskjellen mellom data og prediksjon \\[\\begin{equation} r_i = \\hat{Y}_i - Y_i \\end{equation}\\] Minste kvadraters metode minimerer summen av residualene kvadrert, så \\[\\begin{align} SSE &amp;= \\sum_{i} r_i^2 \\\\ &amp;= \\sum_{i} (\\beta_0 + \\beta_1 X_i - Y_i)^2. \\end{align}\\] Minimér ved å sette den deriverte for hver parameter til null: \\[\\begin{align} \\frac{dSSE}{d\\beta_0} &amp;= \\sum_{i} 2 (\\beta_0 + \\beta_1 X_i - Y_i) \\overset{!}{=} 0 \\\\ \\implies \\quad \\beta_0 &amp;+ \\beta_1 \\frac{\\sum_{i} X_i}{N} = \\frac{\\sum_{i} Y_i}{N} \\\\ \\implies \\quad \\beta_0 &amp;= \\frac{\\sum_{i} Y_i}{N} - \\beta_1 \\frac{\\sum_{i} X_i}{N}, \\end{align}\\] og \\[\\begin{align} \\frac{dSSE}{d\\beta_1} &amp;= \\sum_{i} 2 (\\beta_0 + \\beta_1 X_i - Y_i)X_i \\\\ &amp;= 2 \\beta_0 \\sum_{i} X_i + 2 \\beta_1 \\sum_{i} X_i^2 - 2\\sum_{i} X_i Y_i \\overset{!}{=} 0 \\\\ &amp;\\implies \\quad \\beta_0 \\frac{\\sum_{i} X_i}{N} + \\frac{\\sum_{i} X_i^2}{N} = \\frac{\\sum_{i} X_i Y_i}{N} . \\end{align}\\] Satt inn for \\(\\beta_0\\) blir det \\[\\begin{align} \\left ( \\frac{\\sum_{i} Y_i}{N} - \\beta_1 \\frac{\\sum_{i} X_i}{N} \\right ) \\frac{\\sum_{i} X_i}{N} + \\beta_1\\frac{\\sum_{i} X_i^2}{N} &amp;= \\frac{\\sum_{i} X_i Y_i}{N} \\\\ \\beta_1 \\left ( \\frac{\\sum_{i} X_i^2}{N} - \\left ( \\frac{\\sum_{i} X_i}{N} \\right )^2 \\right ) &amp;= \\frac{\\sum_{i} X_i Y_i}{N} - \\frac{\\sum_{i} X_i}{N}\\frac{\\sum_{i} Y_i}{N} \\\\ \\beta_1 S_X^2 &amp;= S_{XY}. \\end{align}\\] Da blir beta_1 &lt;- 8 / 110 beta_1 ## [1] 0.07272727 og beta_0 &lt;- 20 - beta_1 * 40 beta_0 ## [1] 17.09091 Oppgave 3 Figurene nedenfor viser residualene (feilleddene) vi får ut fra et par ulike modeller. Ser plottene ut til å tilfredsstille kravene for enkel regresjon? Hvis ikke, hva ser ut til å være galt for hver av figurene? Hvordan kan man håndtere brudd på betingelsene i de ulike situasjonene? b. c.  d. Løsning Her ser støyleddet ut som det har konstant varians og forventing lik null. Det betyr at antagelsene er oppfylt. Støyen er symmetrisk om null, men kan se ut som den øker. Altså er det brudd på antagelsen om konstant varians. Økningen ser ut til å være jevn, og en log-transformasjon kunne i dette tilfellet vært til hjelp. Her ser det ut som at vi har perioder med høy varians etterfulgt av perioder med lavere varians. Vi skal ikke se så mye nærmere på hvordan dette kan håndteres i MET4, men man kan komme over det i senere kurs. Her ser det ut som at støyen har en relasjon i tid, og at høye/lave verdier henger sammen med høye/laver verdier. Det er ikke åpenbart at det er dette som foregår, og vi burde undersøkt det nærmere. Senere i kurset skal vi håndtere dette ved bruk av tidsrekkemodeller. Merk at det ikke alltid er lett å se direkte fra figurene hva som er galt. Riktig bruk av statistiske tester og diagnoseplott kan hjelpe med å oppdage brudd på betingelsene. Figurene i denne oppgaven er generert ved simulering, vi kan derfor vite akkurat hva som er galt. For virkelige data vil kunne være vanskeligere å tolke fordi det ikke nødvendigvis er én ting som er galt om gangen. Man må også være på vakt for overtolkning av det som egentlig er tilfeldigheter. 4.4.2 Multippel regresjon og modellbygging Oppgave 1 Denne oppgaven skal gi et forhold til hvordan teorien funker i praksis. Vi skal simulere data fra konstruerte modeller gjøre analyser på dem. Fordelen med å begynne her er at man får er forhold til hvordan statistikk kan og bør tolkes utfra teorien. Med virkelige data vil antagelsene som ligger til grunn for modellene stort sett være brutt, i større eller mindre grad, og kjernen i god statistisk analyse er å vite hva man kan og ikke kan gjøre likevel. God kjennskap til hvordan det burde funke i teorien er en viktig byggestein for statistisk forståelse. Vi generer opp data som skal brukes til å estimere en model. rnorm trekker tall fra en standard normalfordeling. mutate() legger til nye kolonner i dataframe-en basert på allerede eksisterende kolonner. set.seed() setter startpunktet for pseudotilfeldige tall, og om du setter samme seed får du de samme tilfeldige tallene som vi har brukt. library(tidyverse) set.seed(4) df_mdl &lt;- data.frame( x1 = rnorm(100, sd = 2), x2 = rnorm(100, sd = 2), eps = rnorm(100) ) %&gt;% mutate( y = 2 * x1 + (-1) * x2 + eps ) Hva er de eksakte parameterne i modellen som passer til dataene vi har generert, og hva er fordelingen til hver av forklaringsvariablene og støy-leddet? Er betingelsen om uavhengig feilledd oppfylt? Generér opp dataene selv og estimer parameterne basert på dataene. Tolk estimatene. Lag 95 % konfidens- og prediksjonsintervaller for en prediksjon hvor \\(X_1=1, X_2=1\\) basert på estimatene dine. Hva er eksakt prediksjonsintervall basert på modellen vi har generert data fra? Ser dine prediksjonsintervaller rimelige ut utfra dette? Hvorfor er de ikke eksakt like? Stemmer konfidensintervallet overens med modellen? Tolk konfidensintervallet basert på at vi kan generere opp nye data (med et annet seed). Hva er P-verdien i F-test for at vi har en signifikant sammenheng mellom responsvariabelen og forklaringsvariablene? Løsning Fordelingen til forklaringsvariablene er \\[\\begin{align} X_1, X_2 \\sim N(0, 2) \\end{align}\\] fordi vi har trukket dem fra en normalfordeling med forventning 0 og standardavvik 2. Videre er fordelingen til støyleddet en standard normalfordeling \\(\\epsilon\\sim N(0,1)\\). Betingelsen om uavhengig feilledd er oppfylt siden vi ikke har brukt feilleddet til å generere forklaringsvariablene. Det er responsvariabelen, men den skal altså være avhengig av støyleddet. Modellen tar formen \\[\\begin{align} Y = 2 X_1 - X_2 + \\epsilon,\\quad \\epsilon \\sim N(0, 1), \\end{align}\\] hvor parameterne er eksakt fordi vi har laget dataene selv. reg &lt;- lm(y ~ x1 + x2, data = df_mdl) stargazer(reg, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 1.986*** ## (0.053) ## ## x2 -1.091*** ## (0.048) ## ## Constant -0.055 ## (0.096) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.958 ## Adjusted R2 0.957 ## Residual Std. Error 0.956 (df = 97) ## F Statistic 1,094.997*** (df = 2; 97) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Vi ser at estimatene av koeffisientene er signifikante, og at sanne verdier er innenfor konfidensintervallet for hver av dem. Det er ikke et konstantledd i modellen vår, noe som stemmer overens med at den ikke er signifikant forskjellig fra null i regresjonsmodellen. new_df &lt;- data.frame(x1 = 1, x2 = 1) reg_pred &lt;- predict(reg, newdata = new_df, interval = &quot;predict&quot;) reg_conf &lt;- predict(reg, newdata = new_df, interval = &quot;confidence&quot;) ## ## Prediksjonsintervall ## ==================== ## fit lwr upr ## -------------------- ## 1 0.840 -1.073 2.752 ## -------------------- ## ## Konfidensintervall ## =================== ## fit lwr upr ## ------------------- ## 1 0.840 0.600 1.080 ## ------------------- Eksakt prediksjonsintervall for modellen som genererte dataene er \\[\\begin{align} \\bar{Y}|X_1,X_2 \\pm \\alpha_{0.025} \\sigma_{\\epsilon} &amp;= 2 \\cdot 1 - 1 \\cdot 1 \\pm 1.96 \\cdot 1 \\\\ &amp;= 1 \\pm 1.96 \\\\ &amp;= [-0.96, 2.96]. \\end{align}\\] Estimert prediksjonsintervall er bredere enn det estimerte intervallet. Dette kommer av usikkerhet knyttet til estimatene av forventning og standardfeil. Det er benyttet T-test i estimatet av prediksjonsintervallet for å ta hensyn til denne usikkerheten. Man burde ikke få smalere prediksjonsintervaller enn det som faktisk er “sant” med mindre noen av modellantagelsene er usann. Merk at når vi generer opp data selv vet vi sannheten, det gjør vi ikke for virkelige data. Vi burde få at \\(\\bar{Y}|X_1,X_2 = 1\\) er innenfor konfidensintervallet, noe ser ut til å være oppfylt. Tolkningen er at dersom vi kjører analysen på mange ulike datasett vil \\(\\bar{Y}|X_1,X_2 = 1\\) havne innenfor konfidensintervallet i 95 % av tilfellene. Dette kan du prøve selv, bare pass på at du ikke setter seed-et før du generer opp et nytt datasett. Svaret gis i regresjonstabellen stargazer(reg, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 1.986*** ## (0.053) ## ## x2 -1.091*** ## (0.048) ## ## Constant -0.055 ## (0.096) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.958 ## Adjusted R2 0.957 ## Residual Std. Error 0.956 (df = 97) ## F Statistic 1,094.997*** (df = 2; 97) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 F-statistikken er angitt som \\(1094.997\\) med frihetsgrader \\(2, 97\\). P-verdien regnes som pf(1094.997, 2, 97, lower.tail = FALSE) ## [1] 2.716911e-67 som er veldig liten. Altså bekrefter analysen at det er sammenheng mellom forklaringsvariablene og responsvariabelen. Hva ville du tenkt var galt om vi ikke fikk denne konklusjonen fra F-testen, gitt den modellen vi har brukt til å generere dataene? Hint: Det er faktisk en reell sammenheng her. Oppgave 2 Denne oppgaven følger samme premiss som oppgave 1, men handler om kolinearitet. Last ned datasettene 1 og 2, hvor y er responsvariabel og forklaringsvariable er navngitt x. Disse dataene er lagret som .RData filer. For å laste inn dataene i R laster du filene over ned i en mappe, setter mappestien til denne mappen og kjører følgende R-kommandoer: load(&quot;colinearity_1.RData&quot;) load(&quot;colinearity_2.RData&quot;) Finn kolinearitet Hva må man passe dersom man skal lage en regresjonsmodell for responsen Y? Løsning Vi kan begynne med å plotte forholdet mellom de ulike variablene: pairs(y ~ ., data = df_colin1) corrplot(cor(df_colin1), method = &quot;number&quot;) Her ser det ut som vi har et forhold mellom \\((X_2, X_3)\\). Videre ser det ut som at alle tre forklaringsvariable har en sammenheng med responsvariablen \\(Y\\). reg_colin1 &lt;- lm(y ~ ., data = df_colin1) stargazer(reg_colin1, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 1.348*** ## (0.053) ## ## x2 -0.695*** ## (0.064) ## ## x3 -0.208** ## (0.102) ## ## Constant -0.095 ## (0.097) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.916 ## Adjusted R2 0.913 ## Residual Std. Error 0.960 (df = 96) ## F Statistic 346.946*** (df = 3; 96) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 \\(X_1\\) og \\(X_2\\) kommer ut som signifikante, og \\(X_3\\) ved lavere signifikansnivå. Vi kan sjekke variance inflation factor (VIF) vif(reg_colin1) ## x1 x2 x3 ## 1.018945 1.785963 1.774712 som oppgir at estimeringen i seg selv ikke burde være noe problem. Kommentarer: Det reelle forholdet mellom variablene i dette eksempelet er \\[\\begin{align} X_1 &amp;\\sim N(0, 2) \\\\ X_2 &amp;\\sim N(0, 2) \\\\ X_3 &amp;= 0.5 X_2 + \\epsilon_3 \\quad \\epsilon_3 \\sim N(0,1) \\\\ Y &amp;= 1.3 X_1 - 0.8 X_2 + \\epsilon, \\quad \\epsilon \\sim N(0,1), \\end{align}\\] og vi kan tenke oss at dette representerer det kausale forholdet mellom variablene. Siden vi har generert dataene kan vi si det. Det er videre interessant å merke seg i dette eksempelet at \\(X_3\\) kommer ut med en signifikant koeffisient selv om den rent kausalt ikke har noen sammenheng med \\(Y\\). \\(X_3\\) er kun relatert til \\(X_2\\). Dersom man ønsker å gjøre prediksjon kan det være forklaringskraft i \\(X_3\\) for å gjøre prediksjon av \\(Y\\), men i så tilfelle antas det at forholdet mellom \\((X_2, X_3)\\) også vedvarer i fremtiden. Dersom man skal gjøre inferens om en tenkt kausal sammenheng er det mulig å gå på en blemme i et tilfelle som dette. Hvis man ønsker å forklare noe og det er sammenheng mellom to forklaringsvariable kan det fra økonomisk prespektiv være interessant å spørre seg om man bryr seg om \\(X_3\\) i det hele tatt og skal ta den ut. Kanksje \\(X_3\\) er vanskelig å samle inn data om eller den åpenbart ikke har sammenheng med Y. Om den åpenbart ikke skal ha sammenheng med \\(Y\\) kan støyleddet \\(\\epsilon_3\\) også skape unødig støy i prediksjonene. Plott forholdet mellom de ulike variablene: pairs(y ~ ., data = df_colin2) corrplot(cor(df_colin2), method = &quot;number&quot;) Her ser vi et tydelig forhold mellom parene \\((X_3, X_4)\\), \\((X_4, X_5)\\), \\((X_3, X_5)\\) og \\((X_2, X_5)\\). Ellers ser det kun ut som det er et tydelig forhold mellom \\(Y\\) og \\(X_1, X_2\\). Med så mange forhold kan det være snakk om multikolinearitet, som ikke er så lett å se utfra 2D-plott. reg_colin2 &lt;- lm(y ~ ., data = df_colin2) stargazer(reg_colin2, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 2.049*** ## (0.053) ## ## x2 -1.179*** ## (0.111) ## ## x3 -0.198* ## (0.114) ## ## x4 0.101** ## (0.050) ## ## x5 0.092 ## (0.097) ## ## Constant -0.018 ## (0.095) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.963 ## Adjusted R2 0.961 ## Residual Std. Error 0.932 (df = 94) ## F Statistic 486.175*** (df = 5; 94) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Vi ser på VIF for om det kan være problemer med estimering grunnet kolinearitet stargazer(vif(reg_colin2), type = &quot;text&quot;, title = &quot;VIF&quot;) ## ## VIF ## ============================= ## x1 x2 x3 x4 x5 ## ----------------------------- ## 1.062 5.731 5.517 1.662 7.769 ## ----------------------------- hvor vi ser at \\(X_5\\) nærmer seg problematisk område for å få en overdreven varians i estimatene. Dersom målet er å estimere sammenheng mellom alle forklaringsvariable og \\(Y\\) vil dette kunne gå gjennom siden VIF ikke er altfor høy. Hvis det derimot er snakk om modellbygging bør man tenke gjennom om noen av variablene ikke er så viktige og bør tas ut. Kommentarer: Det reelle forholdet mellom variablene i dette eksempelet er \\[\\begin{align} X_1 &amp;\\sim N(0, 2) \\\\ X_2 &amp;\\sim N(0, 2) \\\\ X_3 &amp;\\sim N(0, 2) \\\\ X_4 &amp;= X_3 + \\epsilon_4, \\quad \\epsilon_4 \\sim N(0,2) \\\\ X_5 &amp;= X_2 + X_3 + \\epsilon_5, \\quad \\epsilon_5 \\sim N(0,1) \\\\ Y &amp;= 2 X_1 - X_2 + \\epsilon, \\quad \\epsilon \\sim N(0,1) \\end{align}\\] Her kan vi igjen merke oss at variablene \\(X_3, X_4, X_5\\) kommer inn i regresjonsmodellen med noen koeffisienter som er svakt signifikante. I dette tilfellet trenger ikke dette ha så mye å si. Likevel, ettersom det ikke er noen direkte sammenheng mellom \\(X_3, X_4, X_5\\) ville det vært ideelt å ikke ha dem med i regresjonsmodellen for å unngå feilestimering av koeffisientene til \\(X_1\\) og \\(X_2\\). Fra anvendt statistikk perspektiv bør man tenke gjennom om det gir mening at \\(X_3, X_4, X_5\\) forklarer \\(Y\\). "],["relevante-r-regresjon.html", "4.5 Relevante R-kommandoer", " 4.5 Relevante R-kommandoer Under følger en liste over hvilke oppgaver du skal klare i R fra denne modulen. Vår policy fra og med vårsemesteret 2022 er at R-kommandoene under er tilstrekkelige for å løse oppgavene i datalabber og hjemmeeksamen i MET4. Det er med andre ord ikke nødvendig å lære seg teknikker utover det som er listet opp eksplisitt i listen under. Eventuelle nye teknikker som trengs for å løse en bestemt oppgave vil bli oppgitt og forklart dersom det er nødvendig. Det antas i tillegg at du kan den grunnleggende R-syntaksen som er dekket under Introduksjon til R. Antakelser om datasett Du kan gjøre de samme antakelsene om datasettet som i den tilsvarende oversikten i forrige modul: Datasettene er inneholdt i Excel-filer (.xls eller xslx) eller .csv-filer, som kolonner av variabler, med variabelnavn i første rad. Tilpasse en lineær regresjonsmodell Her er et lekedatasett med tre forklaringsvariabler x1, x2 og x3, samt en responsvariabel y: data-reg.xsls. # Last inn data library(readxl) df &lt;- read_excel(&quot;data-reg.xlsx&quot;) # Syntaksen for å tilpasse en lineær regresjonsmodell med responsvariabel y og # forklaringsvariabler x1, x2, x3 i datasettet df er som følger: reg1 &lt;- lm(y ~ x1 + x2 + x3, data = df) # Dersom vi vil ha med interaksjonsledded mellom x1 og x3 kan vi skrive # kommandoen under. Da vil også x1 og x3 automatisk bli med som variabler i # tillegg til interaksjonsledde x1*x3. reg2 &lt;- lm(y ~ x1 + x2*x3, data = df) # Log transformasjoner kan vi skrive rett inn i formelen reg3 &lt;- lm(log(y) ~ x1 + log(x2) + x3, data = df) # Andre transformasjoner, som for eksempel dersom vi ønsker å ta med # andregradsleddet x1^2, er greiest å få til dersom vi først lager en ny kolonne # med kvadratene til x1, og så lager til regresjonen på vanlig måte: df_ny &lt;- df %&gt;% mutate(x1_kvadrat = x1^2) reg4 &lt;- lm(y ~ x1 + x1_kvadrat+ x2 + x3, data = df_ny) # Vi kan skrive ut et sammendrag for regresjonsmodellen ved hjelp av summary(): summary(reg1) # Pakken stargazer inneholder en funksjon for å lage pene regresjonstabeller: library(stargazer) stargazer(reg1, reg2, type = &quot;text&quot;) # Man kan endre type-argumentet til &quot;html&quot;, og sette out-argumentet til et # filnavn hvis du vil skrive ut tabellen til en fil. Prediksjoner, med konfidens- og prediksjonsintervall La oss si at vi ønsker å predikere verdien av responsvariabelen for følgende verdier av forklaringsvariablene: x1 x2 x3 10 95 1 20 96 1 30 100 0 Da må vi først samle verdiene av forklaringsvariablene i en data frame; det kan vi enten gjøre direkte i R: ny_data &lt;- data.frame(x1 = c(10, 20, 30), x2 = c(95, 96, 100), x3 = c(1,1,0)) Et alternativ kan være å lage til et regneark i Excel, og så lese det inn ved hjelp av read_excel(). Det er viktig at variabelnavnene i det nye datasettet er eksakt de samme som forklaringsvariablene i datasettet du brukte til å estimere regresjonsmodellen! Vi kan lage prediksjon med 95% konfidensintervall ved hjelp av predict(): prediksjoner &lt;- predict(reg1, newdata = ny_data, interval = &quot;confidence&quot;, level = 0.95) Resultatet blir en ny data frame med kolonnene fit (prediksjoner), samt lwr og upr, som inneholder nedre og øvre grense i konfidensintervallet henholdsvis. Du kan endre til prediksjonsintervaller ved å bytte ut \"confidence\" med \"prediction\" (eventuelt \"none\" hvis du ikke ønsker noe intervall). Lage residualplott Vi må kunne lage følgende residualplott i R, her demonstrert med regresjonsmodellen reg1 som vi laget over: # Henter ut residualer og predikerte verdier fra modellen, og setter de inn som # kolonner i en dataframe: residualer &lt;- data.frame(residualer = reg1$residuals, predikert = reg1$fitted.values) # Spredningsplott, residualer mot predikerte verdier ggplot(residualer, aes(x = predikert, y = residualer)) + geom_point() # Autokorrelasjonsplott acf(reg1$residuals) # Histogram for å sjekke normalantakelsen ggplot(residualer, aes(x = residualer)) + geom_histogram() # QQ-plott, observasjonene ligger langs en rett linje hvis de er normalfordelte ggplot(residualer, aes(sample = residualer)) + stat_qq() + stat_qq_line() # Regner ut og plotter Cooks avstand: infl &lt;- influence.measures(reg1) # Lager enkelt plott av Cooks avstand cooks_avstand &lt;- data.frame(obs = 1:nrow(df), cook = infl$infmat[, &quot;cook.d&quot;], cook_infl = infl$is.inf[, &quot;cook.d&quot;]) ggplot(cooks_avstand, aes(x = obs, y = cook)) + geom_col() Alle plottene over kan justeres og pyntes, for eksempel ved å gjøre endringer som i R-videoen om plotting. Se også forelesningsscriptet for flere tips om presentasjon av residualplott. "],["tidsrekker.html", " 5 Tidsrekker", " 5 Tidsrekker Den neste MET4 handler om tidsrekker. I motsetning til modulene vi har jobbet med frem til nå, der vi separerte teorivideoene og dataøvingene, er disse to elementene nå blandet sammen. Vi har delt stoffet opp i en serie overskrifter, der du finner videosnuttene fulgt av en guide til relevante R-funksjoner, programmeringsøvelser og noen regneoppgaver. Dataøvingen til denne modulen er også laget for å passe inn i BED4-opplegget. I øvingen skal vi lage prognoser av noen tidsrekker som kommer til å bli nyttige i en av casene i BED4. Hvis du ikke tar BED4 dette semesteret så er det også helt greit – dataøvingen vår står støtt på egne bein. Faglærer og studentassistenter vil være tilgjengelig på vanlig måte for konsultasjon, diskusjon og problemløsning. Lykke til! "],["intro.html", "5.1 Introduksjon til tidsrekker", " 5.1 Introduksjon til tidsrekker 5.1.1 Kontrollspørsmål Hvilke forskjeller er det mellom en tidsrekke og et sett med samtidige observasjoner? Nevn noen typiske mønstre som vi kan se etter i en tidsrekke. Hvorfor er det nyttig å identifisere slike mønstre? Hvorfor kan det være nyttig å glatte en tidsrekke? Beskriv kort hvordan man regner ut et glidende gjennomsnitt. Hvorfor kan vi ikke bruke det glidende gjennomsnittet til å predikere neste observasjon i en tidsrekke? Beskriv kort hvordan eksponensiell glatting fungerer, og hvorfor denne teknikken kan brukes til prediksjon. 5.1.2 Oppgaver fra lærebok Keller: Statistics for Management and Economics, 11. utg a) Regn ut et glidende gjennomsnitt med vindusstørrelse 3 for følgende tidsrekke: t Y 1 48 2 41 3 37 4 32 5 36 6 31 7 43 8 52 9 60 10 48 11 41 12 30 Løsning t Glidende gjennomsnitt 1 NA 2 (48+41+37)/3 = 42.00 3 (41+37+32)/3 = 36.67 4 (37+32+36)/3 = 35.00 5 (32+36+31)/3 = 33.00 6 (36+31+43)/3 = 36.67 7 (31+43+52)/3 = 42.00 8 (43+52+60)/3 = 51.67 9 (52+60+48)/3 = 53.33 10 (60+48+41)/3 = 49.67 11 (48+41+30)/3 = 39.67 12 NA b) Regn ut et glidende gjennomsnitt med vindusstørrelse 5 for tidsrekken over. Løsning t Glidende gjennomsnitt 1 NA 2 NA 3 (48 +41+37+32+36)/5 = 38.8 4 (41+37+32+36+31)/5 = 35.4 5 (37+32+36+31+43)/5 = 35.8 6 (32+36+31+43+52)/5 = 38.8 7 (36+31+43+52+60)/5 = 44.4 8 (31+43+52+60+48)/5 = 46.8 9 (43+52+60+48+41)/5 = 48.8 10 (52+60+48+41+30)/5 = 46.2 11 NA 12 NA c) Tegn inn tidsrekken over med de to glattingene inn i samme figur. Løsning “Manuelt” i R: # Les først inn dine utregninger time &lt;- seq(12) y &lt;- c(48, 41, 37, 32, 36, 31, 43, 52, 60, 48, 41, 30) glatt3 &lt;- c(NA, 42, 36.67, 35, 33, 36.67, 42, 51.67, 53.33, 49.67, 39.67, NA) glatt5 &lt;- c(NA, NA, 38.8, 35.4, 35.8, 38.8, 44.4, 46.8, 48.8, 46.2, NA, NA) # Med base-R plot plot(time, y) lines(time, glatt3, col = &quot;red&quot;) lines(time, glatt5, col = &quot;blue&quot;) # eller med ggplot library(ggplot2) df &lt;- data.frame(time = rep(time, 2), y = rep(y, 2), glatting = c(glatt3, glatt5), vindulengde = factor(c(rep(&quot;3&quot;, 12), rep(&quot;5&quot;, 12)))) ggplot(df) + geom_point(aes(x = time , y = y)) + geom_line(aes(x = time, y = glatting, color = vindulengde)) d) Regn ut eksponensiell glatting for tidsrekken under med glattefaktor \\(w = 0.1\\): t Y 1 38 2 43 3 42 4 45 5 46 6 48 7 50 8 49 9 46 10 45 Løsning t Eksponensiell glatting 1 38.00 2 0.1(43) + 0.9(38) = 38.50 3 0.1(42) + 0.9(38.50) = 38.85 4 0.1(45) + 0.9(38.85) = 39.47 5 0.1(46) + 0.9(39.47) = 40.12 6 0.1(48) + 0.9(40.12) = 40.91 7 0.1(50) + 0.9(40.91) = 41.82 8 0.1(49) + 0.9(41.82) = 42.53 9 0.1(46) + 0.9(42.53) = 42.88 10 0.1(45) + 0.9(42.88) = 43.09 e) Gjenta oppgaven over med glattefaktor \\(w = 0.8\\). Løsning t Glidende gjennomsnitt 1 38 2 0.8(43) + 0.2(38) = 42.00 3 0.8(42) + 0.2(42.00) = 42.00 4 0.8(45) + 0.2(42.00) = 44.40 5 0.8(46) + 0.2(44.40) = 45.68 6 0.8(48) + 0.2(45.68) = 47.54 7 0.8(50) + 0.2(47.54) = 49.51 8 0.8(49) + 0.2(49.51) = 49.10 9 0.8(46) + 0.2(49.10) = 46.62 10 0.8(45) + 0.2(46.62) = 45.32 f) Tegn tidsrekken over inn i samme figur som de to glattede versjonene. Ser det ut til å være en trend i denne tidsrekken? Løsning “Manuelt” i R: # Les først inn dine utregninger time &lt;- seq(10) y &lt;- c(38, 43, 42, 45, 46, 48, 50, 49, 46, 45) exp01 &lt;- c(38, 38.50, 38.85, 39.47, 40.12, 40.91, 41.82, 42.53, 42.88, 43.09) exp08 &lt;- c(38, 42, 42, 44.40, 45.68, 47.54, 49.51, 49.10, 46.62, 45.32) # Med base-R plot plot(time, y) lines(time, exp01, col = &quot;red&quot;) lines(time, exp08, col = &quot;blue&quot;) # eller med ggplot library(ggplot2) df &lt;- data.frame(time = rep(time, 2), y = rep(y, 2), glatting = c(exp01, exp08), glattefaktor = factor(c(rep(&quot;0.1&quot;, 10), rep(&quot;0.9&quot;, 10)))) ggplot(df) + geom_point(aes(x = time , y = y)) + geom_line(aes(x = time, y = glatting, color = glattefaktor)) Det ser ut til at det er en stigende trend som avtar mot de siste observasjonene i tidsrekken. bonusspørsmål) Hva blir prediksjonen av \\(Y_{11}\\) når du bruker modellen i henholdsvis oppgave d) og e)? Løsning Våre prediksjoner av \\(Y_{11}\\) blir da henholdsvis \\(43.09\\) og \\(45.32\\). 5.1.3 R-øving Vi har lastet ned den daglige prisen på Eqinoraksjen over en 5-års periode fra Oslo Børs’ hjemmeside. Vi laster inn datasettet som før ved hjelp av readxl-pakken, og henter ut den aktuelle kolonnen. Legg merke til at vi bruker rev()-funksjonen til å reversere rekkefølgen til observasjonene slik at den første verdien kommer først: library(readxl) equinor &lt;- read_excel(&quot;equinor.xlsx&quot;) pris &lt;- rev(equinor$Siste) Du kan så lage et raskt plott av tidsrekken: plot(pris, type = &quot;l&quot;) Både glidende gjennomsnitt og eksponensiell glatting har flere ulike implementeringer i R. For glidende gjennomsnitt skal vi bruke funksjonen rollmean() i pakken zoo. Du må først installere pakken og laste den inn; install.packages(&quot;zoo&quot;) library(zoo) Hvis du leser litt på dokumentasjonen til rollmean() ved å kjøre ?rollmean vil du se at du kan regne ut f.eks et glidende gjennomsnitt for Equinoraksjen med vindusstørrelse 5 ved å kjøre pris_glatt5 &lt;- rollmean(pris, k = 5, fill = NA) Da får vi ut en ny vektor med lik lengde som den vi hadde, og som inneholder den glattede versjonen. Den fyller opp verdiene i starten og slutten som vi ikke kan regne ut med et glidende gjennomsnitt med NA, slik at vi kan tegne inn den glattede versjonen i samme plott som vi viste selve tidsrekken: lines(pris_glatt5, col = &quot;red&quot;) Dersom du er interessert kan du lese mer her om hvordan det glidende gjennomsnittet blir brukt som en investeringsstrategi. Tanken er at det glidende gjennomsnittet representerer den langsiktige trenden. Dersom tidrekken ligger under det glidende gjennomsnittet tolkes det som at aksjen er på vei nedover, og motsatt: dersom prisen ligger over det glidende gjennomsnittet, så er det et tegn på at aksjen er på vei oppover. Når de to seriene krysser hverandre går “alarmen”, og man tar stilling til om man skal kjøpe eller selge. Vindusstørrelsen velger man ut fra hvor hyppig man handler. For profesjonelle investorer som driver med handel i høy hastighet kan kanskje 5-dagersviduet som vi regnet ut over være nok. Andre med mellomlang og lang sikt vil gjerne bruke et vindu på 50 eller 200 dager. Oppgave: Regn ut et glidende gjennomsnitt med vindusstørrelse 200 for Equinoraksjen, og tegn det inn i figuren du har laget. Hjelper denne figuren deg til å lage en investeringsstrategi? Løsning Forutsatt at du har gjort det over kan du skrive pris_glatt200 &lt;- rollmean(pris, k = 200, fill = NA) plot(pris, type = &quot;l&quot;) lines(pris_glatt200, col = &quot;red&quot;) Nå kan vi jo ikke se det glidende gjennomsnittet for de siste observasjonene, så ved bare å sammenligne de siste observasjonene hvor vi også har det glidende gjennomsnittet så ligger prisen under det glidende gjennomsnittet, altså bør vi ikke investere (evt. selge, shorte etc.). Et problem med analysen over er at vi trenger fremtidige observasjoner til å regne ut den glattede tidsrekken. Det betyr at vi kjenner den glattede versjonen av tidsrekken ved tid \\(t\\) først ved tid \\(t+200\\). Vi kan enkelt lage en annen variant der vi glatter tidsrekken ved tid \\(t\\) ved å ta gjennomsnittet av \\(Y_{t-200}, Y_{t-199}, \\ldots, Y_{t-1}\\) i stedet for \\(Y_{t-100}, \\ldots, Y_{t}, \\ldots, Y_{t+100}\\), altså at vi bare bruker fortiden. Det gjør du i R ved å legge til det ekstra argumentet align = \"right\" i funksjonen rollmean. Fordelen nå er at vi ved hvert tidspunkt kjenner både prisen på aksjen og den glattede varianten. Oppgave: Regn ut et glidende gjennomsnitt med vindusstørrelse 200 for Equinoraksjen som hele tiden bruker tidligere observasjoner i glattingen. Tegn glattingen inn i figuren. Hvordan ser investeringsstrategien din ut nå? Løsning pris_glatt200 &lt;- rollmean(pris, k = 200, fill = NA, align = &quot;right&quot;) plot(pris, type = &quot;l&quot;) lines(pris_glatt200, col = &quot;red&quot;) I denne figuren ligger de siste prisene over det glidende gjennomsnittet noe som er indikasjon på en stigende trend. Altså kan en strategi være å investere i aksjen. Eksponensiell glatting har også et annet navn: Holt Winters Metode. En funksjon for å gjennomføre den finnes innebygget i R, og heter HoltWinters(). I denne funksjonen er vektparameteren \\(w\\) representert ved argumentet alpha. Funksjonen har noen flere argumenter som ikke vi skal bruke, så dersom vi ønsker å regne ut den eksponensielle glattingen for Equinoraksjen med \\(w = 0.5\\), kjører vi: pris_exp1 &lt;- HoltWinters(pris, alpha = .5, beta = FALSE, gamma = FALSE) For å hente ut den glattede versjonen skriver vi pris_exp1$fitted[,&quot;xhat&quot;] Oppgave: Lag en ny figur der du tegner inn aksjeprisen, samt den eksponensielle glattingen med hhv. \\(w = 0.5\\), \\(w = 0.01\\) og \\(w = 0.99\\). Løsning pris_exp1 &lt;- HoltWinters(pris, alpha = .5, beta = FALSE, gamma = FALSE) pris_exp2 &lt;- HoltWinters(pris, alpha = .01, beta = FALSE, gamma = FALSE) pris_exp3 &lt;- HoltWinters(pris, alpha = .99, beta = FALSE, gamma = FALSE) plot(pris, type = &quot;l&quot;) lines(pris_exp1$fitted[,&quot;xhat&quot;], col = &quot;red&quot;) lines(pris_exp2$fitted[,&quot;xhat&quot;], col = &quot;blue&quot;) lines(pris_exp3$fitted[,&quot;xhat&quot;], col = &quot;green&quot;) "],["trend-og-sesong.html", "5.2 Trend og sesong", " 5.2 Trend og sesong 5.2.1 Kontrollspørsmål Hvilke tre komponenter kan en tidsrekke typisk bestå av? 5.2.2 R-øving 1. Data. I pakken fpp2 finnes en tidsrekke som heter ausbeer, som er den kvartalsvise produksjonen av øl i Australia fra 1956 til 2008. Du kan få tak i det og se på tidsrekken ved å kjøre følgende kommandoer: install.packages(&quot;fpp2&quot;) library(fpp2) plot(ausbeer) Vi ser at det er en klar trendkomponent, selv om den ikke er lineær, samt en årlig sesongvariasjon. 2. Dekomponering. Funksjonen stl dekomponerer tidsrekken i de tre komponentene: trend, sesong, og tilfeldig variasjon. For å få tilgang på denne funskjonen trenger vi pakken forecast: install.packages(&quot;forecast&quot;) library(forecast) Vi så kan kjøre funksjonen slik: dekomponert &lt;- stl(ausbeer, s.window = &quot;periodic&quot;) Vi kan hente ut de ulike komponentene ved å bruke dollartegnet: dekomponert$time.series. Pakken forecast har en egen plottefunksjon, autoplot som er spesialdesignet for tidsrekkeobjekter. Prøv å plotte de tre komponentene hver for seg ved å kjøre: autoplot(dekomponert) 3. Predikere. For predikering bruker vi funksjonen forecast(), som tar en estimert modell som input, og som bruker modellen til å skrive frem tidsrekken ved å estimere fremtidige verdier. Dekomponeringen over utgjør også en modell som vi kan bruke til å predikere fremtidige observasjoner med. Kodesnutten under viser hvordan man predikerer \\(10\\) tidssteg frem i tid ved å sette h = 10 i funksjonen. I tillegg kan funksjonen regne ut prediksjonsintervall med en gitt dekningsgrad, her velger vi level = 0.95 for \\(95\\%\\) prediksjonsintervall. Resultatet lagrer vi i objektet prediksjon. Dette objektet kan vi plotte ved bruk av autoplot-funksjonen: prediksjon &lt;- forecast(dekomponert, h = 10, level = 0.95) autoplot(prediksjon) "],["ar.html", "5.3 AR(p)", " 5.3 AR(p) 5.3.1 Kontrollspørsmål Hva er definisjonen på en Hvit-støy-prosess? Hva er definisjonen på en AR(1)-prosess? Hvilken effekt har parameteren \\(\\phi\\) på egenskapene til en AR(1)-prosess? Hva er forskjellen på en AR(1)-prosess og en generell AR(\\(p\\))-prosess? Hvorfor kan vi si at AR(\\(p\\)) er en utvidelse/generalisering av hvit støy? 5.3.2 R-øving 1. Simulere. La oss først se hvordan vi kan simulere noen realiseringer fra disse tidsrekkene. Hvit støy består av ukorrelerte trekninger som alle har samme forventningsverdi og varians, noe vi kan simulere i R ved å bare trekke \\(n\\) uavhengige observasjoner fra hvilken som helst fordeling og kalle det en tidsrekke. For eksempel har vi tidligere trukket standard normalfordelte observasjoner ved hjelp av rnorm()-funsksjonen. La oss gjøre det igjen, og plotte det som en tidsrekke. Merk at din trekning ikke vil være identisk som den under: n &lt;- 50 hvit_støy &lt;- rnorm(n) plot(hvit_støy, type = &quot;b&quot;) Vi kan bruke funksjonen arima.sim() til å simulere tidsrekker fra AR-modellen (og den mer generelle ARIMA-modellen, mer om det senere). Du kan for eksempel simulere \\(n\\) observasjoner fra en AR(1)-prosess med \\(\\phi = 0.95\\) ved hjelp av følgende kommandoer: ar1 &lt;- arima.sim(model = list(ar = 0.95), n) plot(ar1, type = &quot;b&quot;) I det siste eksempelet trekker arima.sim()-funskjonen hvit-støy-prosessen \\(u_t\\) fra rnorm()-funksjonen, men det kan vi endre på hvis vi vil, se hjelpesiden ?arima.sim. Videre kan vi bruke denne funksjonen til å simulere fra hvit støy ved å sette model-argumentet til en tom liste (model = list()), eller vi kan simulere fra en AR(2)-prosess med \\(\\phi_1 = 0.2\\) og \\(\\phi_2 = 0.1\\) ved å sette model = list(ar = c(0.2, 0.1)). 2. Estimere. La oss i første omgang si at vi har observert tidsrekken ar1 som vi simulerte over, at vi mistenker at den følger en AR(1)-prosess \\(Y_t = \\phi Y_{t-1} + u_t\\), og at vi ønsker å estimere den ukjente parameteren \\(\\phi\\) ved hjelp av observasjonene. Som vi antydet i AR-videoen kan vi i dette tilfellet betrakte det som et regresjonsproblem med \\(Y_t\\) som responsvariabel og \\(Y_{t-1}\\) som forklaringsvariabel. La oss lage en data.frame med disse to kolonnene, og se hva vi får når vi bruker lm()-funksjonen. . df &lt;- data.frame(Y = ar1[2:n], lagged_Y = ar1[1:(n-1)]) summary(lm(Y ~ lagged_Y, data = df)) ## ## Call: ## lm(formula = Y ~ lagged_Y, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8538 -0.6840 -0.1112 0.4233 2.5894 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.36830 0.19310 -1.907 0.0626 . ## lagged_Y 0.86468 0.06545 13.211 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9659 on 47 degrees of freedom ## Multiple R-squared: 0.7878, Adjusted R-squared: 0.7833 ## F-statistic: 174.5 on 1 and 47 DF, p-value: &lt; 2.2e-16 Vi ser at vårt estimat av \\(\\phi\\), som i regresjonutskriften er koeffisienten til lagged_Y, er i nærheten av den sanne verdien 0.95, men med så få observasjoner kan det godt hende at ditt estimat er noe forskjellig. Poenget er: vi kan bruke lineær regresjon til å estimere koeffisientene i en AR-modell basert på observasjoner. Som i forrige oppgave er pakken forecast svært nyttig for estimering og predikering: library(forecast) Denne pakken inneholder en funskjon Arima for å estimere koeffisientene i en AR-modell (egentlig den mer generelle klasssen av ARIMA-modeller som vi kommer tilbake til senere). Denne funksjonen kan vi andvende direkte på tidsrekken ved å skrive Arima(ar1, order = c(1, 0, 0)) ## Series: ar1 ## ARIMA(1,0,0) with non-zero mean ## ## Coefficients: ## ar1 mean ## 0.8935 -1.7696 ## s.e. 0.0613 1.1203 ## ## sigma^2 = 0.9728: log likelihood = -70.04 ## AIC=146.08 AICc=146.6 BIC=151.81 I første omgang kan vi legge merke til at vi har spesifisert hvilken modell vi ønsker å estimere gjennom argumentet order = c(1, 0, 0), der ett-tallet angir AR-modellens orden \\(p\\), som i dette tilfellet er 1. Dersom du mistenker at AR(2)-modellen gir en bedre beskrivelse av tidsrekken din, kan du endre til order = c(2, 0, 0). Vi kommer tilbake til spørsmålet om hvordan du kan velge den beste modellen for et gitt praktisk problem. Legg merke til at de to estimatene ikke er identiske selv om vi bruker det samme tidsrekken. Det er fordi arima()-funksjonen ikke bruker minste kvadraters metode til å regne ut estimatene (slik lm() gjør), men heller bruker en annen estimeringsteknikk som heter maximum likelihood. 3. Predikere. For predikering bruker vi funksjonen forecast(), som tar en estimert modell som input, og som bruker modellen til å skrive frem tidsrekken ved å estimere fremtidige verdier. I kodesnutten under bruker vi den simulerte tidsrekken, og estimerer en AR(1)-modell som over som vi lagrer i objektet ar1_estimat. Så bruker vi det som argument i forecast(), der vi også spesifiserer hvor mange tidssteg fremover vi ønsker å predikere, her velger vi h = 10. I tillegg kan funksjonen regne ut prediksjonsintervall med en gitt dekningsgrad, her velger vi level = 0.95 for \\(95\\%\\) prediksjonsintervall. Resultatet lagrer vi i objektet prediksjon. ar1_estimat &lt;- Arima(ar1, order = c(1, 0, 0)) prediksjon &lt;- forecast(ar1_estimat, h = 10, level = 0.95) Vi kan plotte resultatet i en pen liten figur ved å bruke funksjonen autoplot som under: # Plotter den opprinnerlige tidsrekken, sammen med prediksjon og # prediksjonsintervall autoplot(prediksjon) "],["stasjonaritet.html", "5.4 Stasjonaritet", " 5.4 Stasjonaritet 5.4.1 Kontrollspørsmål Hva er definisjonen på en stasjonær tidsrekke? Hva er poenget med å innføre stasjonaritet som et konsept i tidsrekkeanalyse? Er AR(1) prosessen \\(X_t = 1.5 X_{t - 1} + u_t\\) stasjonær? 5.4.2 Merk En AR-prosess kan vi definere også med et konstantledd \\(c\\), f.eks: \\(Y_t = c + \\phi Y_{t-1} + u_t\\). Vi kan ikke forvente at alle tidsrekkene vi observerer i praksis vil ligge å variere rundt null (dvs at E\\((Y_t) = 0\\)). Vi kan flytte den opp og ned ved å legge til den samme konstanten \\(c\\) i hver tidssteg. I forrige oppgavesett, der vi estimerte parameteren \\(\\phi\\) i en AR(1)-modell, kom det (på samme måte som når vi gjør regresjon) ut et estimat av et intercept, som altså er denne \\(c\\)’en. I den simulerte tidsrekken vi jobbet med der, var det ikke noe konstantledd (altså, \\(c = 0\\)), som vi ser igjen i estimatene ved at de ikke er signifikant forskjellige fra null. Vi kunne tvunget estimeringsfunksjonene til å sette \\(c = 0\\), f.eks ved å inkludere argumentet include.mean = FALSE i arima()-funksjonen. "],["autokorrelasjon.html", "5.5 Autokorrelasjon", " 5.5 Autokorrelasjon 5.5.1 Kontrollspørsmål/Diskusjonsspørsmål Formuler med egne ord: Hva er autokorrelasjon? Hva kan vi lære ved å se på autokorrelasjonsplottet til en tidsrekke? Kan du komme på noe vi ikke kan finne ut av ved å se på korrelasjoneplottet til en tidsrekke? 5.5.2 R-øving 1. Utregning av ACF I R bruker vi funsksjonen acf() til å lage autokorrelasjonsplott. La oss i første omgang gjenskape noen av figurene fra videoen ved hjelp av simuleringer. For eksempel kan vi laget til to tidsrekker som på forrige oppgavesett, en hvit støy og en AR(1): n &lt;- 50 hvit_støy &lt;- rnorm(n) ar1 &lt;- arima.sim(model = list(ar = 0.95), n) Autokorrelasjonsplottene til disse to tidsrekkene kan vi få frem ved å anvende acf()-funksjonen på dem: acf(hvit_støy) acf(ar1) Vi ser igjen mønsteret fra videoen: Hvit støy består av ukorrelerte observasjoner, mens AR(1)-modellen består av observasjoner som bygger på forrige observasjon, slik at det er en viss korrelasjon, og dermed avhengighet fra dag til dag. Det ser vi igjen i autokorrelasjonsplottet som gir tydelig utslag, og der korrelasjonen går gradvis mot null med økende avstand mellom observasjonene. 2. ACF som sjekk av modell En sjekk vi gjerne gjør for å se om en estimert tidsrekkemodell passer dataene våre, er å se autokorrelasjonen til residualene i modellen er liten. Det betyr nemlig at modellen plukker opp den (lineære) avhengigheten i tidsrekken. For en AR(1) modell er residualene f.eks gitt ved \\(\\hat{u}_t = Y_t - \\hat{\\phi}Y_{t-1}\\), men disse er tilgjengelig direkte fra modell estimeringen i R: library(forecast) ar1_estimat &lt;- Arima(ar1, order = c(1, 0, 0)) acf(ar1_estimat$residuals) 3. Oppgave: Prøv nå å plotte autokorrelasjonsfunksjonen for for følgende tre tidsrekker, og knytt en kort kommentar til hver av dem om hva du lærer om tidsrekken ved å se på autokorrelasjonsplottet til: Prisen på Equinor-aksjen, som vi jobbet med i det første oppgavesettet. Løsning library(readxl) equinor &lt;- read_excel(&quot;equinor.xlsx&quot;) pris &lt;- rev(equinor$Siste) acf(pris) Vi ser at det er høy positiv autokorrelasjon selv for store avstander (lag). En lav pris (historisk sett) vil være assosiert med lave priser de foregående dagene og tilsvarende for høye priser. Dette stemmer bra med teorien med at dersom markedet er effisient (ikke pensum, slapp av!) skal prisen følge en tilfeldig gang, selv om dette ikke alltid er tilfellet. Det kan altså se ut til at prisen idag er tilnærmet prisen i går pluss ny støy. Equinoraksjens prosentvise avkastning (som er tilnærmet lik diff(log(pris)) fra dag til dag. Løsning pr_avkastning &lt;- diff(log(pris))) acf(pr_avkastning) Her ser vi at autokorrelasjon er svært lav uansett lag. Det virker altså ikke å være noen lineær sammenheng mellom avkastningen fra en dag til den neste (og den om 2, 3, .. dagen). Dette stemmer bra med at prisen i teorien skal følge en tilfeldig gang og at det i et effisient marked ikke skal gå an å predikere avkastningen for en aksje, noe som i teorien hadde vært mulige hadde det vært en positiv autokorrelasjon. Tidsrekken som er igjen etter at du fjernet trend og sesong fra ølproduksjonstidsrekken i det andre oppgavesettet. Løsning library(fpp2) library(forecast) dekomponert &lt;- stl(ausbeer, s.window = &quot;periodic&quot;) acf(dekomponert$time.series[ ,3], na.action = na.pass) Figuren er litt misvisende siden ett lag i figuren svarer til ett år, og siden vi har kvartalsvise observasjoner har vi 4 acf verdier per år. Det kan se ut til at det er en årlig sesongvariasjon som ikke har blitt dekomponert fullt ut av tidsrekken pga av disse toppene i acf som kommer 1,2,3. .. året. Til slutt: husk at også autokorrelasjonsplottene må pyntes og ordnes på hvis vi skal vise dem til andre i rapporter, innleveringer etc. Du kan stort sett bruke de samme argumetene som i vanlige plott: xlab =, ylab =, main = osv. "],["ma.html", "5.6 MA(q)", " 5.6 MA(q) 5.6.1 Kontrollspørsmål/Diskusjonsspørsmål Hva er definisjonen på en MA(1)- og en MA(\\(q\\))-modell? Hvordan skiller definisjonen av en MA-prosess seg fra definisjonen av en AR-prosess? På hvilken måte er autokorrelasjonsfunksjonene til AR- og MA-prosesser forskjellige? Kan du, med egne ord, beskrive en type reelle fenomener som kan modelleres som en MA-prosess? 5.6.2 R-øving 1. Estimering og predikering. På samme måte som for AR-prosessen kan vi nå simulere og estimere en MA(1)-prosess med \\(\\theta = 0.95\\): library(forecast) # Trengs for estimering n &lt;- 100 # Antall observasjoner ma1 &lt;- arima.sim(model = list(ma = 0.95), n) # Simuler tidsrekken plot(ma1, type = &quot;b&quot;) # Lag et plott Arima(ma1, order = c(0,0,1)) # Estimer theta Stemmer estimatet overens med den sanne \\(\\theta\\)? Sjekk ut dokumentasjonen ?Arima og se hva du må gjøre for å spesifisere at modellen ikke har noe konstantledd \\(c\\). Prøv også å modifisere koden fra AR-oppgavene slik at du predikerer den simulerte MA(1)-tidsrekken 10 steg frem. 2. Analyse av global temperatur. La oss når ta for oss eksempelet fra videoen der vi ser på den globale månendlige gjennomsnittstemperaturen fra 1880 til 2016. Last ned temp.csv, som er en CSV-fil med datasettet. Se på de første par radene: temp &lt;- read.csv(&quot;temp.csv&quot;) head(temp) ## Date Mean ## 1 1880-01-06 0.0009 ## 2 1880-02-06 -0.1229 ## 3 1880-03-06 -0.1357 ## 4 1880-04-06 -0.0499 ## 5 1880-05-06 -0.0738 ## 6 1880-06-06 -0.1692 Første kolonne inneholder informasjon om tidspunkt, og temperaturen er inneholdt i andre kolonne. La oss plotte både temperaturrekken og den differensierte temperaturrekken (dvs. forskjellen fra dag til dag). Hvis vi avslører at den differensierte tidsrekken kan regnes ut ved å kjøre difftemp &lt;- diff(temp$Mean), skulle det nå være grei skuring å produsere følgende to enkle plott: difftemp &lt;- diff(temp$Mean) plot(temp$Mean, type = &quot;l&quot;) plot(difftemp, type = &quot;l&quot;) Lag videre autokorrelasjonsplottet som vist i videoen for den differensierte tidsrekken: I autokorrelasjonsplottet ser vi nettopp et slikt MA-mønster som vi så i videoen; nemlig at autokorrelasjonen plutselig blir null (eller omtrent null) for et gitt lag. I dette tilfellet har vi at første ordens autokorrelasjon er klart forskjellig fra null, men at den fra og med \\(k = 2\\) nesten ikke har utslag. Hvis de differensierte temperaturmålingene faktisk er MA(1), kan den skrives slik: \\[Y_t = c + \\theta u_{t-1} + u_t,\\] der \\(\\theta\\) er en ukjent parameter. Vi kan bruke datasettet vårt til å estimere \\(\\theta\\) ved å bruke Arima()-funksjenen på samme måte som da vi estimerte en AR(1)-modell. Den eneste forandringen vi må gjøre er å endre order-argumentet fra c(1, 0, 0) til c(0, 0, 1): Arima(difftemp, order = c(0, 0, 1)) ## Series: difftemp ## ARIMA(0,0,1) with non-zero mean ## ## Coefficients: ## ma1 mean ## -0.4988 0.0005 ## s.e. 0.0251 0.0012 ## ## sigma^2 = 0.009078: log likelihood = 1532.11 ## AIC=-3058.23 AICc=-3058.21 BIC=-3042.01 Hvis du har tid til slutt og vil ha litt ekstra trening kan du prøve deg på følgende oppgave: Prediker den differensierte temperaturrekken tre måneder frem i tid. Lag en figur der du plotter de 12 siste månedene i den observerte tidsrekken sammen med prediksjonene dine med prediksjonsintervaller. Bonuspoeng: Husk at vi nå har predikert forandringen i den globale gjennomsnittstemperaturen fra måned til måned. Kan du heller lage en figur med selve temperaturserien og bruke prediksjonene dine til å heller plotte inn de tilhørende predikerte temperaturene? Pynt så figuren slik at du kan sende den fra deg. "],["arma-arima.html", "5.7 ARMA og ARIMA", " 5.7 ARMA og ARIMA 5.7.1 Kontrollspørsmål/Diskusjonsspørsmål Hva er sammenhengen mellom AR-, MA-, og ARMA-modellene? Hva er en ARIMA-modell? Hvilken modell er dette: \\[y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\theta u_{t-1} + u_t\\] 5.7.2 R-øving 1. Data Vi tar en ny titt på den daglige prisen på Eqinoraksjen over en 5-års periode som vi så på i introduksjonen til tidsrekker. Vi laster inn datasettet som før ved hjelp av readxl-pakken, og henter ut den aktuelle kolonnen. Legg merke til at vi bruker rev()-funksjonen til å reversere rekkefølgen til observasjonene slik at den første verdien komme først: library(readxl) equinor &lt;- read_excel(&quot;equinor.xlsx&quot;) pris &lt;- rev(equinor$Siste) plot(pris, type = &quot;l&quot;) Vi kan lage en figur av den differensierte tidsrekken på følgende måte: # Sjekk av differanse diff_pris &lt;- diff(pris) plot(diff_pris, type = &quot;l&quot;) Oppgave: Vurder om en ARIMA modell er bedre egnet enn en ARMA modell ut fra de to figurene over. Løsning I forhold til prisen ser den differensierte prisen stasjonær ut og det er derfor rimelig med en ARIMA model. I et effisient marked skal prisen i teorien følge en tilfeldig gang slik at den differensierte tidsrekken bare er støy, altså at prisen følger en ARIMA(0, 1, 0) modell. 2. Estimering av ARIMA modeller Vi bruker den samme funksjonen Arima fra forecast pakken til å estimere både ARMA og ARIMA modeller og spesifisering av modellen gjør vi via argumentet order. Skal du estimerer en ARMA(1,1) modell setter du f.eks dette argumentet til c(1, 0, 1). Elementet i midten av denne vektoren spesifiserer hvor mange ganger tidsrekken skal differensieres i ARIMA modellen. Estimering av en ARIMA modell med en enkelt differensiering og ett MA og AR ledd kan gjøres slik: library(forecast) arima111 &lt;- Arima(pris, order = c(1, 1 , 1)) 3. Hvordan skal vi velge p, d og q i en ARIMA(p,d,q) modell? Etter å ha tilpasset en ARIMA modell kan vi bruke modellen til å predikere de samme observasjonene vi har brukt til å tilpasse modellen. Vi kan så sammenligne hvor nær prediksjoner fra forskjellige modeller er de sanne dataene. Dette heter på godt norsk å gjøre en “in-sample” vurdering av modellen. Når du har tilpasset en modell, kan du ved å bruke summary funksjonen få opp flere mål på hvor god modellen er in-sample under fanen “Training set error measure”: summary(arima111) ## Series: pris ## ARIMA(1,1,1) ## ## Coefficients: ## ar1 ma1 ## 0.5121 -0.5780 ## s.e. 0.1680 0.1584 ## ## sigma^2 = 6.614: log likelihood = -2958.16 ## AIC=5922.32 AICc=5922.34 BIC=5937.72 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.03973471 2.568719 1.949446 0.009091475 1.251353 0.9955157 ## ACF1 ## Training set 0.02035596 Her er f.eks \\(RMSE = \\sqrt{1/T\\sum_{t = 1}^T (\\hat{y}_t - y_t)^2}\\) et slags gjennomsnittlig avvik mellom prediksjonene og observasjonene. Litt lenger oppe i summary utskriften er det også en størrelse som heter AIC som måler hvor sannsynlig hver observasjon er gitt modelvalget ditt. Sammenligner du flere modeller er du på jakt etter den modellen som har minst RMSE og/eller AIC. Det krever en del arbeid skal du sammenligne mange ARIMA(p,d,q) modeller ettersom det er så mange måter å kombinere p,d og q på selv om du bestemmer en maksverdi for hver av dem. Det finnes heldigvis en veldig smart R funksjon kalt auto.arima som følger med pakken forecast som estimerer mange modeller og gir deg ut den modellen med minst AIC: arima_best_AIC &lt;- auto.arima(pris) summary(arima_best_AIC) ## Series: pris ## ARIMA(0,1,2) ## ## Coefficients: ## ma1 ma2 ## -0.0421 -0.0947 ## s.e. 0.0282 0.0281 ## ## sigma^2 = 6.581: log likelihood = -2955.04 ## AIC=5916.09 AICc=5916.1 BIC=5931.48 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.0398931 2.56232 1.948444 0.009249629 1.249956 0.9950042 ## ACF1 ## Training set -0.001297121 Hva slags modell har auto.arima valgt her? 4. Prediksjon Prediksjon gjøres som tidligere med forecast funksjonen, så hvis en vil predikere 10 tidssteg frem i tid gjør man følgende: pred_arima111 &lt;- forecast(arima111, h = 10) autoplot(pred_arima111, include = 100) merk at i autoplot har vi valgt å bare vise 100 observasjoner av tidsrekken sammen med prediksjonene. Kommentar: I et effisient marked skal prisutviklingen i teorien følge en tilfeldig gang og dagens pris vil da være det beste du kan tippe på for morgendagens pris. Vi ser her at dette blir reflektert i prediksjonen gjort av Arima(1,1,1) modellen. "],["modellbygging-1.html", "5.8 Modellbygging", " 5.8 Modellbygging Vi har allerede sett på hvordan vi kan sammenligne modeller “in-sample”. For å sammenligne tidsrekkemodeller bruker en ofte også å sammenligne hvor godt modellene predikerer observasjoner som ikke har vært inkludert når man tilpasser modellen. Dette heter på godt norsk å vurdere “out-of-sample” egenskapene ved modellen. Det finnes mange varianter for å undersøke dette, og under skal vi ta en titt på en enkel variant. Oppgave Vi viser hvordan dette gjøres for en modell med eksponensiell glatting. Du skal gjenta prosedyren, men for en ARIMA-modell (velg den med best AIC, hint: auto.arima). Sammenlign så out-of-sample egenskapene til til disse to modellene. 5.8.1 R-øving 1. Data Vi skal i denne øvingen prøve å finne en god modell for dax-indeksen: library(forecast) dax &lt;- EuStockMarkets[ ,1] plot(dax) 2. Trening og test data Vi ønsker f.eks å teste hvor god modellen er til å predikere de 10 siste observasjonene i datasettet. Vi deler derfor dataene inn i et treningssett bestående av alle observasjoner utenom disse 10 siste observasjonene, og et testsett bestående av de 10 siste observasjonene: trening &lt;- head(dax, length(dax) - 10) test &lt;- tail(dax, 10) 3. Estimering og prediksjon Vi tilpasser så en modell til treningssettet ved bruk av eksponensiell glatting og predikerer 10 tidssteg frem for å få prediksjoner av testsettet: fit_exp &lt;- HoltWinters(trening) pred_exp &lt;- forecast(fit_exp, h = 10) Merk at når vi ikke spesifiserer noen argumenter i HoltWinters vil en mer avansert modell bli tilpasset, samtidig som glattingsparameteren faktisk vil bli estimert ved å minimerer MSE. 3. Out-of-sample vurdering Vi kan så sammenligne disse prediksjonene pred_exp med de faktiske observasjonene test ved å “måle” hvor langt disse er fra hverandre. Funksjonen accuracy som kommer med forecast pakken regner ut flere forskjellig mål på avstand: accuracy(pred_exp, test) ## ME RMSE MAE MPE MAPE MASE ## Training set 1.251117 35.82454 23.42652 0.03438446 0.8276219 0.04295353 ## Test set -317.477645 350.63504 317.47765 -5.82900543 5.8290054 0.58210884 ## ACF1 Theil&#39;s U ## Training set 0.07394258 NA ## Test set 0.62441059 3.510695 Hver kolonne i utskriften over representerer et slikt mål, og det er rad nummer to med navn “Test set” som vi er interessert i siden det er utregningen av disse målene mellom prediksjonene og testsettet (Den første raden representerer in-sample egenskapene). Jo mindre disse verdiene er jo mindre er avstanden mellom prediksjonene og de sanne verdiene i vårt testsett. "],["oppgaver-2.html", "5.9 Oppgaver", " 5.9 Oppgaver Første ordens autokorrelasjon til en tidsrekke \\(Y_t\\) er lik \\(-0.5\\). Andre ordens autokorrelasjon er lik \\(0.3\\). Tegn opp grafen til autokorrelasjonsfunksjonen til \\(Y_t\\). Forklar med ord hva det vil si at autokorrelasjonsfunksjonen til \\(Y_t\\) ser slik ut for observerte verdier av \\(Y_t\\). Forklar hva som er forskjellen på autokorrelasjonsfunksjonen til \\(Y_t\\) og den empiriske autokorrelasjonen til \\(Y_t\\) som vi har regnet ut for eksempel ved hjelp av 100 observerte realiseringer av tidsrekken. Tegn opp en graf som kunne vært den empiriske autokorrelasjonsfunksjonen til \\(Y_t\\) regnet ut for eksempel ved hjelp av 100 observerte realiseringer av tidsrekken. Løsning Graf av autokorrelasjonen: Første ordens autokorrelasjon er lik \\(-0.5\\), noe som betyr at korrelasjonen mellom påfølgende observerte verdier av \\(Y_t\\) er lik \\(-0.5\\). I praksis betyr det at to påfølgende observasjoner vil ha en tendens til å være ganske forskjellige fra hverandre. Hvis \\(Y_t\\) er “liten” (på en eller annen relevant skala), så vil \\(Y_{t+1}\\) ha en tendens til å være “stor”. Og hvis \\(Y_{t+1}\\) er “stor”, ja da vil \\(Y_{t+2}\\) igjen ha en tendens til å være “liten”. Andre ordens autokorrelasjon er lik \\(0.3\\), noe som betyr at korrelasjonen mellom to observasjoner som er observert med ett tidssteg mellom seg er lik \\(0.3\\). I praksis betyr det at dersom \\(Y_t\\) er “liten”, så vil \\(Y_{t+2}\\) også ha en tendens til å være “liten”, men denne tendensen er noe svakere enn den som er mellom to påfølgende observasjoner, der absoluttverdien til autokorrelasjonen er \\(0.5\\). For eksempel kan vi tenke oss at en realisering av \\(Y_t\\) ser slik ut: Dette er samme type spørsmål som vi kjenner til med forventningsverdi/gjennomsnitt, samt teoretisk og empirisk varians. En stokastisk variabel \\(X\\) har en forventningsverdi \\(\\mu\\) og en varians \\(\\sigma^2\\), men disse er i praksis ukjente. Vi kan derimot estimere forventningsverdien ved å ta et gjennomsnitt \\(\\overline X\\), og vi kan estimere den teoretiske variansen ved å regne ut den empiriske variansen \\(s^2\\). Store talls lov garanterer at dette er gode estimater, ved at \\(\\overline X \\rightarrow \\mu\\) og \\(s^2 \\rightarrow \\sigma^2\\) når antall observasjoner går mot uendelig. Vi har nøyaktig det samme forholdet mellom empiriske og teoretiske autokorrelasjoner. Autokorrelasjonsfunksjonen til \\(Y_t\\) som vi beskrev over er den teoretiske, og den er ikke kjent i praksis med mindre vi simulerer fra en kjent tidsrekkemodell. For et gitt datasett kan vi derimot regne ut den empiriske autokorrelasjonsfunksjonen, som er et estimat av det sanne, teoretiske autokorrelasjonsfunksjonen. Vi vet, fra store talls lov, at dette er et godt estimat som vil konvergere mot sannheten etter hvert som vi får flere og flere observasjoner. Den empiriske autokorrelasjonsfunksjonen vil ligne på den sanne autokorrelasjonsfunksjonen som vi tegnet opp over, men det vil være noe estimeringsfeil i tillegg som kommer av at vi bare har et endelig antall observasjoner. Laggene med null korrelasjon vil ikke måles til å ha eksakt null autokorrelasjon, men bare om lag 5% av dem vil havne utenfor forkastningsgrensene på 5% signifikansnivå. Vi tegner også inn noen tenkte forkastningsgrenser i figuren. Vurdere hvilke(n) tidsrekkemodell(er) som passer til de empiriske autokorrelasjonsplottene under, regnet ut ved hjelp av \\(n=500\\) observasjoner. Løsning Vi har et statistisk signifikant negativt utslag på første lag, og et statistisk signifikant positivt utslag på andre lag. Det er ingen flere lags der autokorrelasjonsfunksjonen er stiatistisk signifikant forskjellig fra null. Det kan tyde på en MA(2)-prosess, med \\(\\theta_1 &lt;0\\) og \\(\\theta_2&gt;0\\). Autokorrelasjonsfunksjonen går sakte mot null med økende lag. Det kan tyde på en autoregressiv prosess, muligens en AR(1)-prosess med \\(\\phi_1 &gt; 0\\). Autokorrelasjonsfunksjonen alternerer mellom positive og negative verdier, men i absoluttverdi ser de ut til å avta sakte mot null. Det kan tyde på en autoregressiv prosess, muligens en AR(1)-prosess med \\(\\phi_1 &lt; 0\\). Det er ikke mulig å entydig fastslå hvilken modell som er “korrekt” ut fra et autokorrelasjonsplott. I alle tilfellene kan det være AR- eller MA-ledd som har for små koeffisienter, til at vi klarer å estimere dem til å være statistisk signifikant forskjellig fra null ved hjelp av datasettet som vi har. Det er også vanskelig å visuelt skille høyere ordens prosesser fra hverandre kun ved å se på autokorrelasjonsplottet. I det midterste plottet over for eksempel, kunne det like gjerne vært med andre- og tredjeordens AR-ledd, uten at vi ville klart å se så stor forskjell på plottet. (I denne oppgaven er datasettene 500 observasjoner simulert fra henholdsvis en MA(2)-modell med \\(\\theta_1 = -0.2279\\) og \\(\\theta_2 = 0.2488\\), en AR(1)-modell med \\(\\phi_1 = 0.588\\), og en AR(1)-modell med \\(\\phi_1 = -0.5\\).) Under har vi anvendt funksjonen auto.arima fra forecast-pakken for fire tidsrekker. Identifiser hvilken modell som er plukket ut i hvert tilfelle, og skriv den opp: ## Series: ts ## ARIMA(1,0,1) with zero mean ## ## Coefficients: ## ar1 ma1 ## 0.4307 -0.6955 ## s.e. 0.2427 0.1922 ## ## sigma^2 = 0.8962: log likelihood = -135.48 ## AIC=276.97 AICc=277.22 BIC=284.78 ## Series: ts ## ARIMA(2,0,1) with zero mean ## ## Coefficients: ## ar1 ar2 ma1 ## 0.3037 0.4736 -0.5945 ## s.e. 0.1447 0.0871 0.1489 ## ## sigma^2 = 0.9683: log likelihood = -139.01 ## AIC=286.01 AICc=286.43 BIC=296.43 ## Series: ts ## ARIMA(1,1,0) ## ## Coefficients: ## ar1 ## 0.3568 ## s.e. 0.0933 ## ## sigma^2 = 1.01: log likelihood = -141.96 ## AIC=287.92 AICc=288.04 BIC=293.13 ## Series: ts ## ARIMA(2,1,2) with drift ## ## Coefficients: ## ar1 ar2 ma1 ma2 drift ## -0.0001 0.4345 0.2207 0.8386 -0.7286 ## s.e. 0.0996 0.1023 0.0609 0.0730 0.3520 ## ## sigma^2 = 1.027: log likelihood = -142.71 ## AIC=297.43 AICc=298.33 BIC=313.06 Løsning En ARMA(1,1)-modell: \\[Y_t = 0.4307Y_{t-1} - 0.6955u_{t-1} + u_t,\\] En ARMA(2,1)-modell: \\[Y_t = 0.3037Y_{t-1} + 0.4736Y_{t-2} - 0.5945u_{t-1} + u_t,\\] En ARIMA(1,1,0)-modell: \\[\\Delta Y_t = 0.3568\\Delta Y_{t-1} + u_t,\\] der \\(\\Delta Y_t\\) er tidsrekken \\(Y_t\\) differensiert en gang. Med andre ord er førstedifferansen til \\(Y_t\\) en AR(1) prosess med \\(\\phi_1 = 0.3568\\). En ARIMA(2,1,2)-modell: \\[\\Delta Y_t = -0.0001\\Delta Y_{t-1} + 0.4345\\Delta Y_{t-2} + 0.2207u_{t-1} + 0.8386u_{t-2} + u_t,\\] Med andre ord er førstedifferansen til \\(Y_t\\) en ARMA(2,2)-modell. I alle tilfellene er \\(u_t\\) hvit støy. Regn ut forventning og varians til følgende tidsrekkemodeller og avgjør om de er stasjonære. I alle tilfeller er \\(u_t\\) hvit støy der \\(E(u_t)=0\\) og \\(Var(u_t)=\\sigma^2\\). \\(Y_t = t + u_t\\) \\(Y_t = 3 + u_t\\) \\(Y_t = t\\cdot u_t\\) \\(Y_t = u_1 + u_2 + \\dots + u_t\\) Løsning \\[E(Y_t) = E(t + u_t) = E(t) + E(u_t) = t + 0 = t.\\] \\[Var(Y_t) = Var(t + u_t) = Var(t) + Var(u_t) = 0 + \\sigma^2 = \\sigma^2\\] Siden forventningsverdien ikke er konstant kan ikke dette være en stasjonær tidsrekke. \\[E(Y_t) = E(3 + u_t) = E(3) + E(u_t) = 3 + 0 = 3.\\] \\[Var(Y_t) = Var(3 + u_t) = Var(3) + Var(u_t) = 0 + \\sigma^2 = \\sigma^2\\] Her er både forventning og varians konstant i tid så dette kan være en stasjonær tidsrekke (vi må formelt også sjekke at autokorrelasjonen er konstant) \\[E(Y_t) = E(t\\cdot u_t) = t \\cdot E(u_t) = t\\cdot 0 = 0.\\] \\[Var(Y_t) = Var(t\\cdot u_t) = t^2\\cdot Var(u_t) = t^2\\sigma^2 \\] Siden varians ikke er konstant i tid kan ikke dette være en stasjonær tidsrekke. \\[E(Y_t) = E(u_1 + u_2 + \\dots + u_t) = E(u_1) + E(u_2) + \\cdot + E(u_t) = 0 + 0 + \\dots + 0 = 0.\\] \\[\\begin{align*} Var(Y_t) &amp;= Var(u_1 + u_2 + \\dots + u_t) \\\\ &amp; = Var(u_1) + Var(u_2) + \\dots + Var(u_t) = \\sigma^2 + \\sigma^2 + \\dots + \\sigma^2 = t\\sigma^2 \\end{align*}\\] Siden variansen ikke er konstant i tid kan ikke dette være en stasjonær tidsrekke. Individuell eksamen V21, oppgave 3 Løsning Løsningsforslag Individuell eksamen V20, oppgave 3 Løsning Løsningsforslag "],["relevante-r-tidsrekker.html", "5.10 Relevante R-kommandoer", " 5.10 Relevante R-kommandoer Antakelser om datasettet Vi skal nå ha rimelig grei kontroll på de ulike datastrukturene i R, og det er bra, fordi i denne siste modulen om tidsrekker så er vi nødt til å kunne administrere data i litt forskjellige formater. I hovedtrekke må vi kunne: Lese inn data fra en Excel- eller .csv-fil til en data frame på samme måte som tidligere, der vi i tidsrekker typisk vil ha en kolonne for dato eller tidspunkt, og en eller flere kolonner med tidsrekker nedover. Hente ut tidsrekkene i en data frame som en vektor. Akseptere at en del tidsrekkedata (spesielt de som følger med ulike pakker i R) er lagret i andre typer mer spesialiserte datatyper. Equinordatafilen (equinor.xlsx) følger den første formen og kan leses inn på vanlig måte ved hjelp av read_excel() i readxl-pakken. ausbeer-datasettet fra fpp-pakken, derimot, har en spesiell tidsrekke-klasse (prøv å skrive class(ausbeer) i konsollen), noe vi ser ved å bare skrive ut tidsrekken i konsollen. Da ser vi at datasettet er på en slags matrisestruktur, men kvartal kolonnevis og år radvis. Nødvendige R-funksjoner Nødvendige R-funksjoner for denne modulen sammenfaller eksakt med gjennomgangen under forelesningsvideoene, så det er ikke nødvendig å gjenta det her. "],["avansert-regresjon-og-maskinling.html", " 6 Avansert regresjon og maskinlæing", " 6 Avansert regresjon og maskinlæing I denne modulen tar vi en titt på noen litt mer avanserte statistiske metoder. De to første temaene, logistisk regresjon og “K-nearest-neighbor”-metoden (kNN), har til felles at de kan brukes når responsvariabelen er en kategorisk variabel med kun to kategorier. Begge disse metodene faller inn under det som populært kalles maskinlæring og er således en introduksjon til dette temaet. I det siste temaet, paneldata, skal vi se hvordan vi kan bygge regresjonsmodeller når hvert individ er observert flere ganger etter hverandre i tid. R-script til “Logistisk Regresjon” R-script til “KNN” R-script til “Paneldata” "],["logistisk-regresjon.html", "6.1 Logistisk regresjon", " 6.1 Logistisk regresjon 6.1.1 Videoforelesninger 6.1.2 Kontrollspørsmål I hvilke situasjoner bruker vi logistisk regresjon? Hva er det vi modellerer? Hvordan tolker vi èn enhets økning i forklaringsvariabelen? Hvilken metode brukes til å estimere en logistisk regresjonsmodell? Hva betyr klassifisering og hvordan gjøres dette? Hvis vi har flere modeller, hvilke(n) metode(r) kan vi bruker til å velge den beste? 6.1.3 Teori I denne forelesningen ser vi på situasjonen der vi ønsker å forklare utfallet av en binær variabel (en dummyvariabel) ved hjelp av et sett med forklaringsvariabler. Vi så at vanlig lineær regresjon ikke er særlig passende her fordi utfallet bare kan ta to verdier (0 eller 1, FALSE eller TRUE etc.), og fordi vi heller ikke kan tolke et kontinuerlig utfall direkte som en sannsynlighet fordi vi kan få ut verdier utenfor intervallet \\([0, 1]\\). Løsningen er å heller forklare log-oddsen til suksessansynligheten. Sagt på en annen måte: på venstresiden i regresjonsligningen plasserer vi en transformasjon av suksessansynligheten, som gir oss en kontinuerlig variabel som kun kan variere mellom 0 og 1. Pensumboken vår behandler desverre ikke logistisk regresjon. Heldigvis finnes det et meget godt alternativ, An Introduction to Statistical Learning (ISLR) av James m.fl. som kan lastes ned gratis her: An introduction to statistical learning (trykk på “Download the first edition”) Denne boken er for øvrig pensum i BAN404. Logistisk regresjon er omhandlet i kapittel 4.3 (avsnitt 4.3.5 er ikke pensum). Eksempelet vårt er tatt herfra, og datasettet er, som vist i forelesningsscriptet, inkludert i bokens egen R-pakke ISLR. Bruk litt tid på å lese gjennom disse sidene, konseptet er ganske godt forklart. Bli også kjent med R-syntaksen, som ligner på den vi allerede kan for vanlig lineær regresjon. Vi bruker f.eks. reg1 &lt;- glm(default ~ balance, data = Default, family = &quot;binomial&quot;) Når du er klar til å prøve selv, kan du se på oppg 10a, b og første del av d på s. 171 i ISLR. Dette datasettet er også inneholdt i ISLR-pakken. "],["introduksjon-til-maskinlring-med-knn.html", "6.2 Introduksjon til maskinlæring med kNN", " 6.2 Introduksjon til maskinlæring med kNN 6.2.1 Videoforelesninger 6.2.2 Kontrollspørsmål For hvilke typer responsvariabler bruker vi KNN? Hvordan fungerer KNN teoretisk sett? Hva er den praktiske tolkningen av KNN? Hvordan påvirker valget av \\(k\\) måten KNN fungerer på? Hvordan velger vi \\(k\\)? 6.2.3 Teori Kanskje har du allerede hørt om maskinlæring, “data science”, prediktiv modellering, “business analytics”, etc., og kanskje har du fått med deg at disse tingene virkelig er i vinden for tiden. Som akademisk institusjon skal vi selvsagt være på vakt mot å la popularitet være en avgjørende faktor for hva vi driver med, men, som en kollega så treffende uttrykte seg: “Internett er kommet for å bli.” Det skjer utrolig mye verdiskapning når vi får tak i den verdifulle informasjonen som ligger gjemt i de store datamengdene, og næringslivet skriker etter kompetanse. NHH har som svar på dette opprettet masterprofilen “Business Analytics (BAN)” (som ironisk nok er blitt superpopulær!), og det er naturlig å gi en liten smakebit på hva det går ut på i MET4. Det herlige er at vi ikke trenger å dykke så dypt i detaljene for å få brukbar innsikt i hva som skjer. Overgangen fra logistisk regresjon er naturlig. Vi bruker det vi kan fra regresjonsanalyse til å sette opp en modell der vi forklarer utfallet i en dummyvariabel ved hjelp av et sett forklaringsvariable i allerede observerte data. I første omgang kan vi si at den moderne anvendelsen av logistisk regresjon (kall det gjerne en form for maskinlæring) er å bruke data til å estimere sammenhengen mellom \\(X\\)-ene og responsvariabelen \\(Y\\), og så bruke denne sammengengen til å predikere \\(Y\\) for nye \\(X\\). Artikkelen To explain or to predict av Galit Shmueli forklarer distinksjonen mellom det å forklare og det å predikere godt, og skal være noenlunde lesbar for en interessert student. Eksempelet fra logistisk regresjon er et godt eksempel på en anvendelse: Vi predikerer sannsynligheten for at kunder vil misligholde gjelden i fremtiden, basert på karakteristika vi kan observere nå. Slike sannsynligheter kan vi mate inn i en strategisk analyse for å bestemme oss hvem som skal få innvilget nye lån, men på en systematisk måte der vi sørger for at vi oppnår nødvendige profittmarginer og håndterer risiko på en fornuftig måte, og kan ta hensyn til f.eks. etiske avveininger. Selv om vi ut fra eget behov for profitt og innenfor en akseptabel risikoprofil kan tilby nye lån til kunder med 15% sannsynlighet for å havne i betalingsproblemer, bør vi likevel gjøre det? Poenget her er at du ikke kan gjøre slike vurderinger før du faktisk kan estimere sannsynligheten for mislighold! Statistikken er bunnplanken, og blir mer og mer relevant etter hvert som vi innser at svarene ligger i å analysere data. Vi går videre til et annet eksempel. En teleoperatør med abonnementskunder ser at det er en systematikk i hvilke kunder som sier opp avtalene sine. Ved å se på spredningsplottet under (rød prikk = kunde som har sagt opp abonnementet), ser det ut til at nye kunder med dyre abonnementer har en tendens til å forlate oss. Kan vi sette opp en klassifiseringsregel der som vi kan anvende på alle kundene våre, som automatisk plukker ut kunder som har f.eks. mer enn 50% sannsynlighet for å si opp? Denne listen kan vi så sende videre til markedsavdelingen, som kan sette i verk forebyggende tiltak (f.eks. lokke de inn i bindende avtaler…?), og vi kan oppnå en umiddelbar gevinst. Figur 6.1: Røde prikker er kunder som har sagt opp abbonnementet sitt, svarte prikker er kunder som ikke har gjort det. Finn den optimale avveiningen mellom systematikk og tilfeldig variasjon. Vi kan angripe dette datasettet på to måter: Vi estimerer sannsynligheter ved hjelp av logistisk regresjon. Den stramme strukturen gjør at klassifiseringsgrensen alltid utgjør en rett linje i koordinatsystemet. Vi ser også på en annen klassifiseringsregel: kNN (k nearest neighbours), som ikke bruker sannsynlighetsmodeller eller regresjonsparametre til å klassifisere, men heller er en enkel regel basert på følgende prinsipp: Hvis et flertall av kundene som er mest lik meg har sagt opp, er det mer enn 50% sannsynlig at også jeg vil si opp. Her bruker vi litt tid på detaljer, men det handler i grunn bare om å lage en presis definisjom om hvem vi definerer som de kundene som ligner mest på meg, og svaret er de \\(k\\) kundene som ligger nærmest meg i koordinatsystemet. På samme måte som for logistisk regresjon kan vi lese mer om kNN i ISLR. På s. 39–42 står det hvordan teknikken fungerer, og i forelesningsnotatene og det medfølgende scriptet ser vi hvordan det kan gjøres i praksis. Når vi forstår hvordan kNN fungerer, er neste steg å reflektere litt over hvordan vi har tenkt å velge parameteren \\(k\\) i praksis. Vi så i forelesningen at: Vi kan ikke velge \\(k\\) for liten. Da ser vi for mye på støy og tilfeldigheter. Vi kan enkelt tenke oss at jeg er en lavrisikokunde, selv om de to kundene som er nærmest meg i koordinatsystemet sa opp av en eller annen grunn. Hvis vi velger \\(k = 3\\), vil jeg likevel bli klassifisert som høyrisiko og bli bombardert med unødvendig reklame (som i seg selv kan gjøre stor skade!) Hadde vi heller valgt \\(k = 50\\) eller \\(k=500\\) ville disse to raringene ikke bli tatt hensyn til, men blitt dominert av alle andre i området som faktisk ikke har sagt opp. Altså: vi kan ikke henge oss for mye opp i detaljene og den tilfeldige variasjonen! Vi kan heller ikke velge \\(k\\) for stor, for det vil til slutt nærme seg en situasjon det det bare blir en avstemning mellom alle kundene i datasettet. Det er flest kunder som ikke sier opp avtalen, så da blir alle kunder klassifisert som lavrisiko. Altså: vi vil heller ikke ignorere variasjonen i datamaterialet! Hele poenget er jo å lære noe nyttig fra hvordan prikkene fordeler seg i koordinatsystemet. I Figur 6.1 kan du prøve følgende: En liten \\(k\\) svarer til å se nøye på figuren (putt hodet ditt helt inntil skjermen!), og virkelig legge merke til hvor hver eneste en av de røde prikkene befinner seg. Å velge en større \\(k\\) svarer til å trekke lenger bort, og kanskje begynne å myse litt, slik at du får øye på systematikken, nemlig at det røde dominerer nede til høyre i figuren. Til slutt står du i rommet ved siden av med lukkede øyne, og da ser du plutselig ingenting! Et eller annet sted i mellom der ønsker vi å være. Kryssvalidering er en systematisk og generell måte å velge k for KNN (og tilsvarende parametre i andre maskinlæringsmetoder), som litt lenger enn å bare dele datasettet inn i trenings- og testdata ISLR behandler temaet på s. 181–186, men det er forholdsvis teknisk og skrevet i lys av noen metoder som vi ikke har sett på i MET4. "],["paneldata.html", "6.3 Paneldata", " 6.3 Paneldata 6.3.1 Videoforelesninger 6.3.2 Kontrollspørsmål Hva er paneldata? Hva må vi ta hensyn til når vi analyserer paneldata? Hvordan ser en generell modell for paneldata ut? Hva er den konseptuelle forskjellen mellom faste og tilfeldige effekter? Når kan vi bruke faste effekter? Når kan vi bruke tilfeldige effekter? Finnes det en måte å formelt teste om man skal bruke faste eller tilfeldige effekter? (obs: se helt nederst på denne siden for svaret på denne.) 6.3.3 Teori og R I denne forelesningen introduserer vi en ny datastruktur. Vi observerer flere individer (tversnittsdimensjonen) gjentatte ganger (tidsdimensjonen), og et slikt datasett kaller vi et panel, eller paneldata. Fordelen ved å jobbe med slike data er åpenbar: vi har mer informasjon og kan gjennomføre mer presise statistiske analyser. På den annen side må vi akseptere at en mer kompleks datastruktur gjør det nødvendig å innføre mer kompleks metodikk. I gjennomgangen under bruker vi et liten del av dataene fra eksempelet som er beskrevet i videoene. Ønsker du å følge R-gjennomgangen laster du ned følgende datasett: panel_liten.csv 6.3.3.1 Struktur på Paneldata Til nå har vi typisk observert \\(n\\) individer en gang. Hvis vi holder oss til eksempelet fra videoforelesningen, kan vi tenke oss at vi har spurt \\(n\\) arbeidstakere om hvor mange timer de jobbet forrige år (\\(X\\)), og hvor mye de hadde i timelønn (\\(Y\\)). Da ville datasettet sett omtrent slik ut: Her er \\(y_i\\) timelønn til arbeidstaker nummer \\(i\\), og \\(x_i\\) er antall timer jobbet for arbeidstaker nummer \\(i\\). Hvis vi så ønsker å se om det er en sammenheng mellom disse to variablene, kan vi sette opp en enkel regresjonsmodell som vi har gjort før: \\[\\begin{equation} y_i = \\alpha + \\beta x_i + \\epsilon_i, \\label{p-ols} \\end{equation}\\] der vi gjør de vanlige antakelsene om homoskedastisitet, uavhengige feilledd, og selvsagt at forklaringsvariabelen er eksogen, dvs at de stokastiske variablene \\(X\\) og \\(\\epsilon\\) er uavhengige fra hverandre. Hvis vi aksepterer det, så kan vi estimere \\(\\beta\\) ved hjelp av minste kvadreters metode (OLS - orinary least squares), som vi kan tolke som forventet økning i timelønn ved å jobbe en time ekstra. For paneldata har vi ikke lenger kun observert \\(n\\) arbeidstakere 1 gang, men spurt \\(N\\) arbeidstakere \\(T\\) ganger, slik at vi trenger to indekser til å identifisere hver enkelt observasjon: \\(y_{i,t}\\) er timelønn til arbeidstaker nummer \\(i\\) ved tidspunkt \\(t\\). Våre observerte \\(X\\)er og \\(Y\\)er kan vi samle i en tabell som før, se illustrasjonen under. Legg merke til at det bare er de to første kolonnene for \\(X\\) og \\(Y\\) som utgjør de faktiske observasjonene, mens de to neste kolonnene sier hvilket individ som er observert, og ved hvilket tidspunkt observasjonen er utført, og viser bare indeksene til \\(X\\)- og \\(Y\\)-observasjonene. Kall det gjerne metadata, og vi trenger den informasjonen når vi skal utføre paneldatateknikker. Formatet i tabellen over kalles gjerne et langt format, og omtrent samtlige R-pakker og funksjoner som brukes til å analysere panel data forventer at dataene er organisert på denne måten. Vi kan ta en titt på hvordan dette ser ut i R for eksempelet vårt: df &lt;- read.csv(&quot;panel_liten.csv&quot;) # Leser inn datasettet head(df) # Ser på datasettet ## X lnhr lnwg kids age disab id year mlhr mlwg dlhr dwg ## 1 5241 7.71 3.19 0 30 0 1 1979 7.742 3.218 -0.032 -0.028 ## 2 5242 7.64 3.14 0 32 0 1 1980 7.742 3.218 -0.102 -0.078 ## 3 5243 7.71 3.11 0 33 0 1 1981 7.742 3.218 -0.032 -0.108 ## 4 5244 7.72 3.00 0 34 0 1 1982 7.742 3.218 -0.022 -0.218 ## 5 5245 7.72 2.94 0 35 0 1 1983 7.742 3.218 -0.022 -0.278 ## 6 5246 7.73 3.12 1 35 0 1 1984 7.742 3.218 -0.012 -0.098 Her svarer lnwgtil responsvariabelen (log) lønn og lnhr til forklaringsvariabelen (log) antall timer jobbet. Legg merke til at det er en egen kolonne med navn id som forteller oss hvilket individ observasjonene gjelder for. Dette svarer til \\(i\\)-indeksen i notasjonen over. Det er også en egen kolonne kalt year som forteller oss hvilket år observasjonen er fra, og dette svarer til \\(t\\)-indeksen. F.eks er første rad observasjoner gjort for individ nr. 1 i år 1979. 6.3.3.2 Hva må vi ta hensyn til? Hovedmotivasjonen for å analysere paneldata er ennå å undersøke sammenhengen mellom respons og forklaringsvariabelen. En slik måte å samle inn data på gir f.eks mer innformasjon om sammenhengen mellom timelønn og antall arbeidstimer fordi vi har flere observasjoner enn om vi bare betraktet en observasjon per individ. Men siden vi har gjentatte observasjoner over tid kan responsvariablene være avhengige. Vi kan se for oss to grunner til dette: En utvikling i tid som er felles for alle individene; Det er f.eks tenkelig at det er en generell utvikling i lønnsnivået over tid, eller at det f.eks finnes gode år hvor alle tjener spesielt godt. De gjentatte observasjonene for ett gitt individ vil typisk være avhengige; Har et individ høy inntekt det ene året er det tenkelig at det også har høy inntekt det neste året. Denne effekten kan misforstås som en effekt av forklaringsvariabelen. Altså må vi både ta hensyn til at det kan være en generell utvikling i tid og at det kan være individuelt forskjellige lønnsnivå. Disse aspektene kan nemlig påvirke vårt estimat av effekten av å jobbe mer dersom vi bruker den tradisjonelle regresjonsmodellen og OLS. La oss inspisere de \\(3\\) individene vi har data for i vårt lille datasett ved å lage et figur med lønnsutvikling (lnwg) langs y-aksen og år (year) langs x-aksen for å se om det finnes et slags felles mønster i lønnsutviklingen (ref. punkt 1. over): library(ggplot2) ggplot(df) + geom_point(aes(x = year, y = lnwg, color = factor(id))) her ser vi f.eks at alle individene har et dårlig år i 1983, mens 1985 virker å være et godt år. Det også tydelig at lønnen holder seg relavivt lik lønnen det foregående året. Det er altså rimelig å tro at det er fellestrekk i lønnen til individene for gitte år. La oss så lage et spredningsplott mellom lnhr og lnwg hvor vi fargelegger hvilket individ observasjonene kommer fra: library(ggplot2) ggplot(df) + geom_point(aes(x = lnhr, y = lnwg, color = factor(id))) Ser vi på disse dataene samlet sett ser du til å være en klart positiv korrelasjon mellom lønn og antall timer jobbet. Men legg merke til at individ nr. 1 ligger på et høyere lønnsnivå og jobber mer enn de to andre individene. Det er dette som i stor grad skaper et bilde av en sterk positiv sammenheng mellom variablene. Hvis vi ser på de individuelle observasjonene (hver fargesky) hver for seg, virker ikke sammenhengen å være like sterk, og det har kanskje ikke like mye å si for lønnen om du individuelt velger å jobber mer. Dette svarer til fenomenet beskrevet i punkt 2. over. 6.3.3.3 Generelt oppsett av modell Effektene av de fenomene vi beskrev over kan vi ta hensyn til ved å inkludere dem i regresjonsmodellen på følgende måte: \\[y_{it} = \\beta_0 + \\beta_1 x_{it} + v_t + \\alpha_i + \\epsilon_{it} \\] hvor vi nå har lagt til to nye ledd, \\(v_t\\) og \\(\\alpha_i\\), i modellen: Her representerer \\(v_t\\) den generelle utviklingen i tid som er felles for alle individene (det er ingen “i”-indeks i denne). Det kan f.eks være en lineær trend (\\(v_t = \\delta t\\)) eller helt unike årlige effekter \\(v_t\\), som fanger opp gode og dårlige år. Leddene \\(\\alpha_i\\) representerer så de individuelle lønnsnivåene (disse er “i” indeksert). Har individ \\(1\\) høyere lønn enn individ \\(2\\) så vil \\(\\alpha_1\\) blir estimert til å være større enn \\(\\alpha_2\\). Vi justerer altså for at individer kan ligge på et forskjellig lønnsnivå, og at det er en felles årlige variasjoner i lønn. I praksis betyr dette at vi justerer regresjonslinjen vertikalt slik at den tilpasser seg lønnsnivået til hvert individ. Effekten \\(\\beta_1\\) av å jobbe mer er derimot antatt lik for hvert individ og den estimerte effekten vil da bli et slags gjennomsnitt av hvor mye det individuelt lønner seg å jobbe mer. I utgangspunktet kan vi betrakte \\(v_t\\) og \\(\\alpha_i\\) som kategoriske variabler som kan estimeres ved hjelp av dummyvariabler slik vi har lært før. Problemet er at i et tradisjonelt paneldatasett så er \\(N\\) (antall individer) et stort tall, mens \\(T\\) (antall observasjoner per individ) et relativt lite tall. Dette fører til svært mange kategoriske variabler \\(\\alpha_i\\) å estimere, og i praksis må vi derfor betrakte andre metoder. Vi noterer oss følgende: Egenskapene til \\(\\alpha_i\\) bestemmer typen paneldatamodell og vi deler disse modellene grovt sett inn i modeller med faste effekter og tilfeldige effekter. Selve ligningene vil altså se like ut, men tolkning og estimering er forskjellig. Leddene \\(v_t\\) vil vi i de fleste tilfeller klare å estimere som kategoriske variabler og i fortsettelsen ser vi bort fra dette leddet For enkelthets skyld betrakter vi bare èn forklaringsvariabel, men det kan selvsagt være flere forklaringsvariabler i en regresjonsmodell for panel data også. 6.3.3.4 Forskjellige parameteriseringer Merk at det både i lærebøker og i forskjellige R-pakker veksles mellom å to typer formuleringer av den generelle modellen over. Hvis vi ser bort fra \\(v_t\\) leddet, er modellen vi til nå har betraktet formulert som: \\[\\begin{align} y_{it} = \\beta_0 + \\beta_1 x_{it} + \\alpha_i + \\epsilon_{it} \\tag{6.1} \\end{align}\\] hvor \\(\\beta_0\\) er inkludert. Her kan \\(\\beta_0\\) tolkes som det gjennomsnittlige skjæringspunktet med y-aksen blant de individuelle regresjonslinjene, mens \\(\\beta_0 + \\alpha_i\\) vil være skjæringspunktet med y-aksen for individ nr. \\(i\\). Men det er også svært vanlig (og kanskje litt lettere) å formulere modellen uten \\(\\beta_0\\): \\[\\begin{align} y_{it} = \\beta_1 x_{it} + \\alpha_i + \\epsilon_{it} \\tag{6.2} \\end{align}\\] og da vil \\(\\alpha_i\\) være det individuelle skjæringspunktet med y-aksen for individ nr. \\(i\\). Forskjellen er rett og slett tolkningsmessig. I videoen for faste effekter går vi igjennom estimeringen av begge modellene, men under betrakter vi bare estimering av sistnevnte modell når vi bruker faste effekter. Dette gjør vi siden modellen i R er definert på denne måten. Når vi så ser på tilfeldige effekter vil vi av samme grunn betrakte modell (6.1). 6.3.3.5 Faste effekter I en modell med faste effekter betrakter vi \\(\\alpha_i\\) leddene som faste størrelser som må estimeres. Denne modellen er omtrent alltid gyldig og kan brukes selv om vi tror de individuelle forskjellene \\(\\alpha_i\\) er relativt store og at det er avhengighet mellom forklaringsvariabelen(e) og \\(\\alpha_i\\). I modellen vår som er formulert som \\[y_{it} = \\beta_1 x_{it} + \\alpha_i + \\epsilon_{it} \\] skal vi altså estimere \\(\\alpha_1, \\alpha_2, ..., \\alpha_N\\) samt effekten av det å jobbe mer \\(\\beta_1\\). Det finnes en rekke estimeringsteknikker og vi vil her (og i videoen) bare beskrive en metode. Vi begynner med å ta tidsgjennomsnittet av ligningen over for hvert individ: \\[1/T \\sum_{t = 1}^T y_{it} = 1/T \\sum_{t = 1}^T \\beta_1 x_{it} + 1/T \\sum_{t = 1}^T \\alpha_i + 1/T \\sum_{t = 1}^T \\epsilon_{it}\\] Her vil tidsgjennomsnittet av \\(\\alpha_i\\) bare være \\(\\alpha_i\\) siden dette leddet ikke varierer med tiden. Altså kan vi skrive ligningen over som: \\[\\begin{align} \\overline{y}_{i} = \\beta_1\\overline{x}_i + \\alpha_i + \\overline{\\epsilon}_i \\tag{6.3} \\end{align}\\] Her er henholdsvis \\(\\overline{y}_i\\) og \\(\\overline{x}_i\\) den gjennomsnittlige lønnen og antall timer jobbet for individ nr. i, og dette er størrelser vi kan regne ut. Vi tar så vår originale modell å trekker fra denne ligningen: \\[y_{it} - \\overline{y}_{i} = \\beta_1(x_{it} - \\overline{x}_i) + \\alpha_i - \\alpha_i + \\epsilon_{it} - \\overline{\\epsilon}_i\\] Nøkkelen her er at \\(\\alpha_i\\)-leddene kansellerer hverandre og vi kan formulere en ligning helt uten disse leddene: \\[\\tilde{y}_{it} = \\beta_1\\tilde{x}_{it} + \\tilde{\\epsilon}_{it},\\] hvor \\(\\tilde{y}_{it} = y_{it} - \\overline{y}_{i}\\), \\(\\tilde{x}_{it} = x_{it} - \\overline{x}_{i}\\) og \\(\\tilde{\\epsilon}_{it} = \\epsilon_{it} - \\overline{\\epsilon}_{i}\\). Siden vi nå ikke lenger har \\(\\alpha_i\\) i ligningen, og siden \\(\\tilde{\\epsilon}_{it}\\) bare er et nytt feilledd sentrert rundt null, kan vi estimere \\(\\beta_1\\) ved vanlig OLS, altså velge den verdien \\(\\hat{\\beta}_1\\) som minimerer: \\[\\sum_{i=1}^N\\sum_{t=1}^T(\\tilde{y}_{it} - \\beta_1\\tilde{x}_{it})^2\\] Gitt et estimat av \\(\\beta_1\\) kan vi få estimater av \\(\\alpha_1, \\dots, \\alpha_N\\) ved å og ta forventning av den tidsgjennomsnittlige modellen (6.3) (da forsvinner \\(\\overline{\\epsilon}_{it}\\) leddet), erstatte \\(\\beta_1\\) med estimatet \\(\\hat{\\beta}_1\\) og løse ligningen m.h.p \\(\\alpha_i\\). \\[\\begin{align} \\hat{\\alpha}_1 &amp;= \\overline{y}_{1} - \\hat{\\beta}_1\\overline{x}_1\\\\ \\hat{\\alpha}_2 &amp;= \\overline{y}_{2} - \\hat{\\beta}_1\\overline{x}_2\\\\ &amp;\\vdots\\\\ \\hat{\\alpha}_N &amp;= \\overline{y}_{N} - \\hat{\\beta}_1\\overline{x}_N\\\\ \\end{align}\\] Sluttproduktet er derfor \\(N\\) individuelle regresjonslinjer: \\[\\begin{align} y_{1t} &amp;= \\hat{\\alpha}_1 + \\hat{\\beta}_1 x_{1t} \\\\ y_{2t} &amp;= \\hat{\\alpha}_2 + \\hat{\\beta}_1 x_{2t} \\\\ &amp;\\vdots\\\\ y_{Nt} &amp;= \\hat{\\alpha}_N + \\hat{\\beta}_1 x_{Nt} \\\\ \\end{align}\\] som har individuelle skjæringspunkt med \\(y\\)-aksen (\\(\\hat{\\alpha}_i\\)) for å justere for forskjellig lønnsnivå, men hvor alle observasjonene har blitt brukt til å estimere det felles stigningstallet \\(\\hat{\\beta}_1\\). Det er verdt å merke seg at denne metoden ikke kan brukes dersom forklaringsvariabelen ikke varierer med tid (eksempelvis kjønn). Da vil nemlig \\(x_{it} - \\overline{x}_i = x_{it} - x_{it} = 0\\), og vi har derfor ingen mulighet til å estimere \\(\\beta_1\\) med OLS. Det finnes flere pakker som kan estimere slike modeller i R, men en veldig enkel pakke å bruke heter plm. Det første vi gjør er å laste pakken forså å “oversette” dataene våre til paneldata. På denne måten forstår plm funksjonen hva som er individ indeksen og hva som er tidsinndeksen: library(plm) # Pakke for å estimere faste effekter # Oversetter til panel data frame p.df &lt;- pdata.frame(df, index = c(&quot;id&quot; ,&quot;year&quot;)) Syntaksen for å bruke plm funksjonen er svært lik den for lm og glm. For å bruke en modell med faste effekter må man huske å spesifiserer argumentet model = \"within\": reg.fe &lt;- plm(lnwg ~ lnhr, data = p.df, model = &quot;within&quot;) summary(reg.fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = lnwg ~ lnhr, data = p.df, model = &quot;within&quot;) ## ## Balanced Panel: n = 3, T = 10, N = 30 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.270016 -0.042538 -0.011427 0.051738 0.336355 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## lnhr 0.36293 0.26210 1.3847 0.1779 ## ## Total Sum of Squares: 0.37546 ## Residual Sum of Squares: 0.34967 ## R-Squared: 0.068678 ## Adj. R-Squared: -0.038782 ## F-statistic: 1.91731 on 1 and 26 DF, p-value: 0.17792 Vi ser at den estimerte \\(\\beta_1\\) effekten av å jobbe mer estimeres til \\(0.36\\). Vi kan også få ut estimatene for de faste effektene \\(\\alpha_1\\), \\(\\alpha_2\\) og \\(\\alpha_3\\): fixef(reg.fe) ## 1 2 3 ## 0.408228 -0.062905 -0.279994 Det kan være nyttig å ta en visuell innspeksjon på hvordan de individuelle regresjonskurvene passer til dataene: plot(reg.fe) Denne figuren viser også hvordan regresjonskurven ville sett ut dersom vi hadde sett bort fra paneldatastrukturen og brukt en helt vanlig (pooled) regresjonsmodell. Sammenlignet med modellen med faste faste effekter (within) så ville vi da estimert effekten av jobbe mer til å være større. Selv om produktet her er \\(3\\) individuelle regresjonsmodeller, presiserer vi at dette ikke er \\(3\\) uavhengige analyser; alle observasjonene er brukt til å finne et estimat på \\(\\beta_1\\). Vi husker at det også kunne være en felles tidskomponent \\(v_t\\) i en slik modell og at denne kunne betraktes som en kategorisk variabel siden det typisk ikke er så mange (\\(T\\)) av disse. Vi kan derfor bare legge til year som en ekstra (kategorisk) forklaringsvariabel forutsatt at den er koded som en factor: is.factor(p.df$year) ## [1] TRUE reg.fe &lt;- plm(lnwg ~ lnhr + year, data = p.df, model = &quot;within&quot;) summary(reg.fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = lnwg ~ lnhr + year, data = p.df, model = &quot;within&quot;) ## ## Balanced Panel: n = 3, T = 10, N = 30 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.1740253 -0.0689048 0.0054976 0.0668643 0.2081334 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## lnhr 0.182181 0.317431 0.5739 0.5735 ## year1980 0.030310 0.095869 0.3162 0.7557 ## year1981 0.015762 0.095752 0.1646 0.8712 ## year1982 -0.050297 0.095805 -0.5250 0.6064 ## year1983 -0.063590 0.103765 -0.6128 0.5481 ## year1984 0.031862 0.102964 0.3094 0.7607 ## year1985 0.162429 0.095752 1.6964 0.1081 ## year1986 0.066977 0.095869 0.6986 0.4942 ## year1987 0.082429 0.095752 0.8609 0.4013 ## year1988 0.087881 0.095682 0.9185 0.3712 ## ## Total Sum of Squares: 0.37546 ## Residual Sum of Squares: 0.23334 ## R-Squared: 0.37853 ## Adj. R-Squared: -0.060163 ## F-statistic: 1.03543 on 10 and 17 DF, p-value: 0.45624 For dette lekedatasettet kunne vi strengt tatt også betraktet \\(\\alpha_i\\)-leddene som kategoriske variabler, men som sagt er som regel \\(N\\) for stor til at dette lar seg gjøre. 6.3.3.6 Tilfeldige effekter Hovedmotivasjonen for analysen av paneldataene er å finne ut hva effekten av å jobbe mer er (\\(\\beta_1\\)). Vi vet at det eksisterer individuelle effekter \\(\\alpha_i\\) som en konsekvens av datastrukturen, men vi er ikke nødvendigvis interessert i disse verdiene i seg selv. I motsetning til en modell med faste effekter hvor vi betraktet \\(\\alpha_i\\) som faste størrelser, vil vi i en modell med tilfeldige effekter betrakte disse som tilfeldige variabler. I en slik modell estimerer vi derfor heller fordelingen til disse effektene. En vanlig antagelse er da at \\(\\alpha_i\\sim N(0, \\sigma_{\\alpha}^2)\\) dersom vi bruker parameteriseringen (6.1) med \\(\\beta_0\\) inkludert. \\(\\alpha_i\\sim N(\\beta_0, \\sigma_{\\alpha}^2)\\) dersom vi bruker parameteriseringen (6.2) uten \\(\\beta_0\\). Variansen \\(\\sigma_{\\alpha}^2\\) er da et mål på uobservert heterogenitet; i dette tilfellet hvor stor variasjon det i lønnsnivået mellom individene. Det også vanlig å anta at \\(\\epsilon_{it}\\sim N(0, \\sigma_{\\epsilon}^2)\\), så i en slik modell skal vi altså estimere \\(\\beta_1\\) (og \\(\\beta_0\\)), \\(\\sigma_{\\alpha}^2\\) og \\(\\sigma_{\\epsilon}^2\\). Vi bruker altså bare èn ekstra parameter (\\(\\sigma_{\\alpha}^2\\)) for å justere for forskjellen i lønnsnivåene. Sammenligner vi dette med modellen med faste effekter der vi trengte \\(N\\) (\\(\\alpha_1, \\alpha_2,\\dots,\\alpha_N\\)) ekstra parametre er dette en mye enklere modell. En modell med tilfeldige effekter brukes dersom det er rimelig å anta at de individuelle effektene \\(\\alpha_i\\) er små og uavhengig av forklaringsvariabelen(e). Dersom dette er oppfylt viser det seg at en kan få mer presise estimat av effekten vi egentlig er interessert i , \\(\\beta_1\\). Det finnes en rekke estimeringsmetoder og vi skal ikke gå detaljer på noen her, men nevner: Varianter av minstekvadraters metode som kan minne om det vi gjorde for faste effekter. Sannsynlighetsmaksimeringsestimering. Bayesiansk estimering. De to siste metodene er mye brukt for å estimere tilfeldige effekter. I R kan vi fortsatt bruke plm, men må endre model argumentet til \"random\": # tilfeldig effekt modell reg.re &lt;- plm(lnwg ~ lnhr, data = p.df, model = &quot;random&quot;) summary(reg.re) ## Oneway (individual) effect Random Effect Model ## (Swamy-Arora&#39;s transformation) ## ## Call: ## plm(formula = lnwg ~ lnhr, data = p.df, model = &quot;random&quot;) ## ## Balanced Panel: n = 3, T = 10, N = 30 ## ## Effects: ## var std.dev share ## idiosyncratic 0.01345 0.11597 0.356 ## individual 0.02429 0.15586 0.644 ## theta: 0.771 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.182643 -0.082132 -0.017109 0.046306 0.421926 ## ## Coefficients: ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept) -1.35059 2.17414 -0.6212 0.53446 ## lnhr 0.54307 0.28506 1.9051 0.05677 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 0.53409 ## Residual Sum of Squares: 0.47281 ## R-Squared: 0.11475 ## Adj. R-Squared: 0.083133 ## Chisq: 3.62944 on 1 DF, p-value: 0.056766 I motsetningen til modellen vi estimerte med faste effekter er nå parameteriseringen gjort med \\(\\beta_0\\) leddet inkludert (modell (6.1)) og vi ser at dette blir estimert til å være \\(-1.35\\). Videre estimeres effekten av å jobbe mer til \\(\\hat{\\beta}_1 = 0.54\\). Litt lenger oppe i utskriften ser vi at \\(\\hat{\\sigma}^2_{\\alpha}= 0.0242\\) (“individual”), mens \\(\\hat{\\sigma}^2_{\\epsilon}= 0.0134\\) (“idiosyncratic”). 6.3.3.7 Hausman test En modell med faste effekter vil være mulig å bruke i alle tilfeller, mens en modell med tilfeldige effekter har strengere krav til hvordan disse individuelle effektene oppfører seg. Dersom disse er oppfylt er det en fordel å bruke en modell med tilfeldige effekter. En strategi for å finne riktig modell er å estimere begge modellene forså å utføre en såkalt Hausman test. Nullhypotesen er da at den riktige modellen er den med tilfeldige effekter. Forkaster vi bruker vi modellen med faste effekter, forkaster vi ikke bruker vi modellen med tilfeldige effekter. I R utfører vi testen slik: # Hausman test phtest(reg.fe, reg.re) ## ## Hausman Test ## ## data: lnwg ~ lnhr + year ## chisq = 6.6775, df = 1, p-value = 0.009764 ## alternative hypothesis: one model is inconsistent Siden vi får forkastning vil den beste modellen for dette (leke-) datasettet være modellen med faste effekter. "],["oppgaver-3.html", "6.4 Oppgaver", " 6.4 Oppgaver 6.4.1 Oppgaver om logistisk regresjon Kommentar: Oppgave 1 a) og oppgave 2) er svært like i hva du skal gjøre, bare at den siste er mer realistisk. Oppgave 1 Du har estimert en logistisk regresjonsmodell med to forklaringsvariabler \\(x_1\\) og \\(x_2\\). Koeffisientene i modellen er estimert til \\(\\hat{\\beta}_0 = 0.4\\), \\(\\hat{\\beta}_1 = -0.1\\) og \\(\\hat{\\beta}_2 = 0.3\\). Du observerer så et nytt individ med forklaringsvariablene \\(x_1 = 1\\) og \\(x_2 = 2\\). Prediker sannsynlighet for at \\(Y=1\\) for dette individet. Løsning \\[z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 = 0.4 -0.1\\times1 + 0.3\\times2 = 0.9\\] Den predikerte sannsynligheten er gitt ved følgende sammenheng: \\[P(Y=1|Z=z) = \\frac{e^z}{1+e^z} = \\frac{e^{0.9}}{1 + e^{0.9}} \\approx 0.71.\\] Klassifiser det nye individet. Løsning Siden vi estimerer \\(P(Y=1)\\) til å være \\(0.71\\) som er større enn \\(0.5\\) klassifiserer vi dette individet til \\(\\hat{y}=1\\). Avhengig av kontekst, kan det være relevant å bruke enn høyere eller lavere terskel enn \\(0.5\\), men dette er altså standardverdien. Oppgave 2 (Individuell Eksamen V2020 1 i)) Denne eksamensoppgaven handlet om luftforurensning der myndighetene bruker en målestasjon for å advare innbyggerne dersom konsentrasjonen av nitrogendioksid (NO\\(_2\\)) overstiger 100 \\(\\mu\\textrm{g}/\\textrm{m}^3\\). I en av deloppgavene tilpasses det en logistisk regresjonsmodell der responsvariabelen er en dummyvariabelen danger_warning, som indikerer om gjenomsnittskonsentrasjonen av NO\\(_2\\) den aktuelle dagen oversteg 100 \\(\\mu\\textrm{g}/\\textrm{m}^3\\). Dersom det skjer må myndighetene utstede et såkalt gult farevarsel. Den estimerte modellen er gitt i kolonne (2) i tabellen under. ….. I morgen er det lørdag 16. mai, og i den aktuelle byen er det meldt en gjennomsnittlig temperatur på 19 \\(^\\circ\\)C og en gjennomsnittlig relativ luftfuktighet på 47%. Bruk den logistiske regresjonsmodellen til å predikere sannsynligheten for at gjennomsnittlig NO\\(\\mathbf{_2}\\)-konsentrasjon overstiger 100 \\(\\mu\\textrm{g}/\\textrm{m}^3\\). Gi en kort vurdering om myndighetene bør utstede gult farevarsel. (Husk at luftfuktigheten er gitt på skala 0–100, og ikke 0–1). Løsning Den predikerte log-oddsen får vi ved å sette inn for variablene (lørdag, temperatur, fuktighet, vinterdummyen er null): \\[z = 5.052 + -2.292 - 0.086*19 - 0.044*47 = -0.942.\\] Den predikerte sannsynligheten er gitt ved følgende sammenheng: \\[P(Y=1|Z=z) = \\frac{e^z}{1+e^z} = \\frac{e^{-0.942}}{1 + e^{-0.942}} \\approx 0.28.\\] Den predikerte sannsynligheten er klart under 50%, som passer godt med den tidlighere analysen vår. Det er snakk om en forholdsvis varm lørdag i sommerhalvåret, og vi vil nok ikke utstede farevarsel. Det kan også være gode argumenter for at vi ikke nødvendigvis bruker 50% som terskel for farevarsel. Kanskje er det mer alvorlig å ikke utstede et farevarsel som burde vært sendt ut fordi det kan være farlig for folk, enn å utstede et unødvendig farevarsel. Føre var osv., og det kan tilsi at vi f.eks. bruker 40% eller 30% sannsynlighet som grense. Det kommer litt an på situasjonen, som vi ikke har full oversikt over her. 6.4.2 Oppgaver om KNN Oppgave 1 (Individuell hjemmeeksamen H2020, oppgave 3) Vi har følgende datasett med seks observasjoner bestående av en binær responsvariabel \\(y\\) og to forklaringsvariabler \\(x_1\\) og \\(x_2\\): Tabell 6.1: Datasett y x1 x2 0 3 4 0 4 5 1 5 3 0 3 6 1 4 3 1 6 2 Du observerer så forklaringsvariablene \\((x_1, x_2) = (3, 3)\\) for et nytt individ. Regn ut hva klassifiseringen av \\(y\\) blir for det nye individet ved å bruke k-nearest neighbor (KNN), med \\(k=3\\). Løsning Vi begynner med å regne ut den euklidske avstanden mellom \\((3,3)\\) og alle punktene \\((x_1, x_2)\\) i datasettet vårt. F.eks er avstanden mellom \\((3,3)\\) og \\((3,4)\\) \\[\\begin{equation*} d((3,3), (3,4)) = \\sqrt{(3 - 3)^2 + (3 - 4)^2} = 1 \\end{equation*}\\] Vi kan så legge disse avstandene inn i en egen kolonne i tabellen: Tabell 6.2: Utregnede avstander y x1 x2 avstand 0 3 4 1.000 0 4 5 2.236 1 5 3 2.000 0 3 6 3.000 1 4 3 1.000 1 6 2 3.162 Vi ser da at observasjon \\(1\\),\\(3\\) og \\(5\\) med avstander på h.h.v. \\(1\\), \\(2\\) og \\(1\\) er de tre nærmeste naboene, og blant dem er det 2 mot 1 i flertall for å klassifisere \\(y\\) som en 1’er. Altså er \\(\\hat{y}=1\\). Det går selvsagt an å løse oppgaven visuelt også. Hvordan fungerer KNN når \\(k = n\\), hvor \\(n\\) er antall observajoner i datasettet? Hva vil skje dersom \\(k = 6\\) for dette datasettet? Løsning Siden vi bare har seks observasjoner vil alle verdier av \\(k\\geq 6\\) fullstendig ignorere informasjonen som ligger i forklaringsvariablene. Klassifiseringen vil da bare være basert på om det totalt sett er mest 1’ere eller 0’ere. I dette spesifikke datasettet har vi totalt tre 1’ere og tre 0’ere, så enhver majortetsavstemning med \\(k\\geq 6\\) blir uavgjort. Altså vil det her ikke være mulig å oppnå flertall for verken 0’er eller 1’er for store verdier av \\(k\\). 6.4.3 Oppgaver om paneldata Oppgave 1 For et paneldata bestående av en responsvariabel \\(Y\\) og en forklaringsvariabel \\(X\\) har du estimert modellen \\[y_{it} = \\beta_1 x_{it} + v_t + \\alpha_i + \\epsilon_{it} \\] der du har betraktet \\(\\alpha_i\\) som faste effekter og \\(v_t\\) som kategoriske variabler. Estimatet av \\(\\beta_1\\) er \\(\\hat{\\beta}_1 = 1.5\\). For individ \\(4\\) i datasettet har du estimert \\(\\alpha_4\\) til å være \\(0.2\\). Videre er årseffekten for 2012, \\(v_{2012}\\), estimert til å være \\(-0.5\\). Prediker responsvariabelen til dette individet for 2012 dersom \\(x_{4,2012} = 2\\). Løsning \\[\\hat{y}_{4,2012}=\\hat{\\beta}_1 x_{4,2012} + \\hat{v}_{2012} + \\hat{\\alpha}_4 = 1.5\\times2 -0.5 + 0.2 = 2.7\\] Du tilpasser også en tilsvarende modell med tilfeldige effekter. En Hausman test gir en p-verdi på \\(0.23\\). Hvilken modell skal du da bruke? Løsning Forenklet sett har denne testen som nullhypotese at modellen med tilfeldige effekter er gyldig. Her er p-verdien veldig stor og vi har lite bevis for at denne nullhypotesen er feil. Vi kan altså bruke modellen med tilfeldige effekter. Dersom vi hadde fått forkastning ville det vært lurt å bruke modellen med faste effekter. Oppgave 2 Prøv deg på Oppgave 2 i den individuelle hjemmeeksamen H2020 som du finner i kapittel 9.1. Det er spesielt oppgave e) og f) som er relatert til paneldata, men vi bemerker at oppgave f) var “nøtten” i det oppgavesettet. "],["relevante-r-avansert-regresjon.html", "6.5 Relevante R-kommandoer", " 6.5 Relevante R-kommandoer Under følger en liste over hvilke oppgaver du skal klare i R fra denne modulen. Vår policy fra og med vårsemesteret 2022 er at R-kommandoene under er tilstrekkelige for å løse oppgavene i datalabber og hjemmeeksamen i MET4. Det er med andre ord ikke nødvendig å lære seg teknikker utover det som er listet opp eksplisitt i listen under. Eventuelle nye teknikker som trengs for å løse en bestemt oppgave vil bli oppgitt og forklart dersom det er nødvendig. Det antas i tillegg at du kan den grunnleggende R-syntaksen som er dekket under Introduksjon til R. Antakelser om datasett Du kan gjøre de samme antakelsene om datasettet som i den tilsvarende oversikten i modulen om hypotesetesting: Datasettene er inneholdt i Excel-filer (.xls eller xslx) eller .csv-filer, som kolonner av variabler, med variabelnavn i første rad. Tilpasse en logistisk regresjonsmodell Dersom vi ønsker å foklare variasjon i en binær variabel ved hjelp av en eller flere forklaringsvariabler. Vi kan bruke det samme datasettet som tilvarende seksjon i forrige modul: data-reg.xsls. I dette datasettet har vi en binær variabel i x3, samt to kontinuerlige variabler x1 og x2. Du må kunne lese inn datasettet, estimere en logistisk regresjonsmodell, og skrive ut resultatet på følgende måte: df &lt;- readxl::read_excel(&quot;datasett/data-reg.xlsx&quot;) model_logistisk &lt;- glm(x3 ~ x1 + x2, data = df, family = &quot;binomial&quot;) summary(model_logistisk) Dersom du skal bruke logistisk regresjon til å predikere verdien av den binære responsvariabelen for nye “individer” må du først være i stand til å dele opp datasettet i et testsett og et treningssett. Det holder å kunne gjøre en enkel splitt basert på rekkenummer. Her utgjør treningsdatasettet de første 70 observasjonene og testdatasettet resten: treningssett &lt;- df[1:70, ] testsett &lt;- df[71:nrow(df), ] Du vil da estimere modellen ved å bruke treningsdatasettet: model1 &lt;- glm(x3 ~ x1 + x2, data = treningssett, family = &quot;binomial&quot;) og du kan teste prediksjonsevnen ved å predikere utfallet i observasjonene i testdatasettet ved p bruke predict(): predikert &lt;- predict(model1, newdata = testsett, type = &quot;response&quot;) Resultatet blir en data frame med predikerte sannsynligheter for at verdien av x3 for den aktuelle observasjonen er lik 1. Du kan oversette sannsynlighetene til 0 eller 1 ved å velge en terskelverdi. Vi kan for eksempel velge at vi predikerer at x3 tar verdien 1 dersom den predikerte sannsynligheten for 1 er større enn 0.5 på følgende måte: sann &lt;- df$x3 # Den sanne verdien i testdataene pred_test &lt;- predict(model1, newdata = testsett, type = &quot;response&quot;) # Predikert sannsynlighet klassifisering &lt;- ifelse(pred_test &gt; 0.5, 1, 0) # Klassifisering av kundene table(sann, klassifisering) # Kontigenstabell Et par ekstra bemerkninger: - Du kan predikere responsvariabel for nye kombinasjoner av forklaringsvariablene ved å lage en data frame som du sender inn i newdata-argumentet til predict()-funksjonen på akkurat samme måte som for vanlig regresjon. Trene en kNN-modell Logistisk regresjon kan brukes både til statistisk inferens (vi bruker modellen til å forstå sammenhenger mellom respons- og forklaringsvariablene) og til prediksjon. Dersom vi først og fremst er interessert i prediksjon kan vi også bruke kNN, som kan gi bedre resultater. Vi antar at vi har med oss datasettet og oppsplittingen i treningssett og testsett fra seksjonen over. Du må kunne trene en kNN-modell, evaluere den på testsettet, og predikere status på nye individer. # Pass på at caret-pakken er installert library(caret) # Hvis vi vil sette k selv model2 &lt;- train(x3 ~ x1 + x2, data = treningssett, method = &quot;knn&quot;, tuneGrid = data.frame(k = 50)) # R-kode dersom vi vil velge k automatisk med kryssvalidering trControl &lt;- trainControl(method = &quot;cv&quot;, # 5-fold kryssvalidering number = 5) # Tilpasser modellen model3 &lt;- train(x3 ~ x1 + x2, data = treningssett, method = &quot;knn&quot;, trControl = trControl, metric = &quot;Accuracy&quot;) # Hvilken k valgte kryssvalideringen? k &lt;- model3$finalModel$k k predict()-funksjonen fungerer på samme måte som for lineær og logistisk regresjon. Paneldata Skriptfilen som følger denne modulen gir et greit sammendrag for hvilke R-kommandoer som er relevanter for å estimere paneldatamodeller. Datasettet som brukes her er panel_liten.csv. library(readr) # Lese inn csv-filer library(plm) # Paneldata # Innlesning av data: OBS! Linken i videoen fungerer ikke så du må laste ned # panel_liten.csv fra modulen selv og så lese inn på vanlig måte: df &lt;- read_csv(&quot;panel_liten.csv&quot;) # Oversetter til panel data frame med metadata for kolonner som skal brukes som # faste effekter. p.df &lt;- pdata.frame(df, index = c(&quot;id&quot; ,&quot;year&quot;)) # Faste effekter med utskrift av sammendrag reg.fe &lt;- plm(lnwg ~ lnhr, data = p.df, model = &quot;within&quot;) summary(reg.fe) fixef(reg.fe) # Tilfeldige effekter med utskrift av sammendrag reg.re &lt;- plm(lnwg ~ lnhr, data = p.df, model = &quot;random&quot;) summary(reg.re) # Hausmann-test phtest(reg.fe, reg.re) "],["datavinger.html", " 7 Dataøvinger", " 7 Dataøvinger Her finner du dataøvingene som skal gjennomføres i MET4. Se tidsplanen til kurset for en oversikt over når de ulike øvingene skal gjøres og hvilke uker studentassistentene gjennomfører øvingene på datasal. "],["dataving-1.html", "7.1 Dataøving 1", " 7.1 Dataøving 1 7.1.1 Innledning Velkommen til den første dataøvelsen i MET4. I denne øvelsen skal vi bli litt kjent med verktøyene R og Rstudio som brukes i datalabbene. Disse verktøyene er også essensielle for gjennomføringen av den obligatoriske innleveringen og på hjemmeksamen. Den første delen av øvingen inneholder praktisk informasjon om bruk av R og Rstudio etterfulgt av oppgaver. 7.1.2 Om R og Rstudio R er et program/programmeringsspråk som er spesialdesignet til å utføre statistiske analyser. R er basert på at du må skrive forskjellige kommandoer for å utføre utregninger og analyser. Gjennomsnittet av 3, 2 og 5 finner man for eksempel ved å skrive: mean(c(3,2,5)) Dette kan for mange være litt uvant i starten, men datalabbene vil gi deg god trening på denne type tankegang. Rstudio er et program som gjør det enklere å bruke R. På samme måte som Word kan hjelpe deg til å lage fine og oversiktlige tekster, kan Rstudio hjelpe deg til å utføre fine og oversiktlige statistiske analyser. Rstudio er et redigeringsprogram som vi i dette kurset skal bruke til å redigere og utføre R-kommandoer. 7.1.2.1 Installere R og Rstudio Bruker du din egen datamaskin kan du enkelt laste ned og installere R og Rstudio. Begge programvarene er gratis og kan installeres med å følge instruksene under. Får du problemer kan du få en av studentassistentene til å hjelpe deg. Start med å installere R: Gå til r-project.org Last ned versjonen som passer ditt operativsystem (Windows/Mac/Linux) Kjør installasjonsfilen og følg instruksene. Standard innstillingene skal være greie å bruke, så du kan trykke neste/ok til installasjonen er ferdig. Installer så RStudio: rstudio.com, og naviger deg frem til siden for RStudio. Du skal der laste ned desktop-versjonen av programmet (“Open source edition”) for ditt operativsystem og installere på vanlig måte. Kjør installasjonsfilen som lastes ned og følg instruksene 7.1.2.2 Vinduene i Rstudio og det å jobbe med R Første gang du åpner Rstudio vil du se tre vinduer. Et fjerde vindu åpner du med å klikke på File i menyen, så New File, og så R Script. Figur 7.1 viser en oversikt over de fire vinduene. Det er viktig at du forstår forskjellen på de to vinduene til venstre. Figur 7.1: Oversikt over vinduene i RStudio. Nederste vindu til venstre (b) viser R-konsollen og det er her alle utregninger blir gjennomført. I dette vinduet kan du for eksempel skrive (3*5 - 3/4)*(2 + 2) ## [1] 57 Her er (3*5 - 3/4)*(2 + 2) en såkalt kommando og det er programmet R som finner ut hva du mener med kommandoen og gir deg svaret 57 i retur. Du kan se at R tillater standard matteoperasjoner som gange, deling, pluss og minus (*, /, +, -). R er det vi kaller ‘objektbasert’, som betyr at du kan definere ‘objekter’ (ofte kalt variabler). Utregningen over kan for eksempel også regnes ut ved å skrive: a &lt;- 3*5 - 3/4 b &lt;- 2 + 2 a*b ## [1] 57 Her er a og b objekter/variabler som vi definerer ved bruk av ‘tildelingspilen’ &lt;- (du kan også bruke =). Det går an å lagre objekter i egne filer, men vi skal se at det stort sett er smartere å lagre ‘oppskriften’ (selve koden) på hvordan de lages i en egen .R-fil. Det øverste vinduet til venstre (a) viser en .R-fil (et skript). En .R-fil fungerer som et manuskript med R-kommandoer (kode) og kan lagres slik at du kan senere kan se hvilke kommandoer du har brukt i analysen og eventuelt fortsette der du slapp. I Del 2 av denne dataøvingen skal du selv lage en .R-fil som inneholder alle kommandoer som brukes i en enkel analyse. Når du vil at R skal utføre noen av kommandoene du har skrevet i .R-filen markerer du bare disse (eller lar pekeren stå i linjen du vil kjøre) og trykker Ctrl + Enter (Cmd + Enter på Mac): Figur 7.2: Utførelse av kommandoer du har skrevet i R filen. Marker eller la pekeren stå i linjen du vil kjøre og trykk ctrl + Enter (Cmd + Enter på Mac) Vinduet nederst til høyre (d) vil vise blant annet figurer du lager og hjelpetekst. Vinduet øverst til høyre (c) gir deg en oversikt over hvilke objekter du har laget og er spesielt nyttig hvis du vil ta en nærmere titt på et datasett du har lest inn. Det er viktig at du forstår forskjellen på de to vinduene til venstre, altså .R-filen og konsollen. R-kode du ønsker å ta vare på og som er en essensiell del av analysen skriver og lagrer du i .R-filen, mens små eksperimenter og undersøkelser som du ikke trenger senere kan du gjerne gjøre direkte i konsollen. For de av dere som er glad i hurtigtaster finnes det en oversikt i Rstudio som kommer opp dersom du trykker Alt + Shift + K (Option + Shift + K på Mac). Ofte vil man f.eks måtte skifte musepeker fra .R-filen til konsoll og motsatt, og hurtigtaster for å veksle mellom disse er Ctrl + 1 (R-fil) og Ctrl + 2 (konsoll). Hurtigtasten du kommer til å bruke desidert mest er Ctrl + Enter for å kjøre kode fra R-skriptet ditt i konsollen (På Mac erstatter du Ctrl med Cmd over alt). 7.1.2.3 Funksjoner, dokumentasjon og R-pakker R kommer med en rekke “innebygde” funksjoner som kan utføre ulike statistiske analyser. For eksempel kan en t-test utføres med å bruke en funksjon som heter nettopp t.test(). Alle slike funksjoner kommer med en dokumentasjon som viser hva funksjonen gjør og hvordan den skal brukes. For å tilgang til denne dokumentasjonen skriver man ? foran funksjonen i konsollen. Skriver du f.eks ?t.test ser du at det dukker opp en side i vinduet nede til høyre: Figur 7.3: Dokumentasjon av funksjoner dukker opp i et vindu nede til høyre. Dette vinduet kan åpnes til et større vindu som vist over Dokumentasjonen vil som hovedregel inneholder en kort beskrivelse av hva funksjonen gjør, hvilke argumenter funksjonen tar og hva den gir ut. Helt i slutten av dokumentasjonen er det ofte et eksempel på hvordan funksjonen kan brukes og er ofte svært nyttig å se på. Selv om det finnes mange funksjoner som allerede er innebygget i R, må man noen ganger installere ekstra ‘pakker’ for å få tilgang til spesielle funksjoner. I oppgave 2.2 i denne øvelsen vil vi gå gjennom hvordan dette gjøres for en bestemt pakke. 7.1.2.4 Skriv pen R-kode! Det er viktig at R-koden du skriver er veldokumentert og skrevet på en oversiktlig og pen måte. Hvis vi ønsker å skrive kommentarer til koder som står i .R-filen bruker vi tegnet # foran kommentaren. Dette gjør at R ikke prøver å evaluere kommentaren som en R-kode. Det finnes en rekke konvensjoner når det kommer til mellomrom, linjeskift, navngivning av objekter og lignende. Vi anbefaler tipsene som er oppsummert på (http://adv-r.had.co.nz/Style.html)[http://adv-r.had.co.nz/Style.html], men det er selvsagt lov å ha sine egne preferanser. Under ser du et eksempel på dårlig praksis ved R-koding. Her er det manglende dokumentasjon, dårlig navngivning og ingen ‘luft’ i form av mellomrom og linjeskift. Dette gjør at du eller andre vil måtte bruke unødvendig mye tid på å finne ut hva koden faktisk gjør på et senere tidspunkt. # Dårlig praksis: library(readxl) d&lt;-readxl(file=&quot;financedata.xlsx&quot;,sheetIndex = 1) %&gt;% na.omit() Ø95&lt;-mean(d$value)-qt(0.975,df=length(d$value)-1)*sd(d$value) N95&lt;-mean(d$value)+qt(0.975,df=length(d$value)-1)*sd(d$value) Følgende R kode gir det samme resultatet men er mye mer oversiktlig siden den er mer luftig, er brutt ned i biter, er godt dokumentert og har fornuftige objektnavn: # God praksis: # ---------- Analyse av data # Nødvendige pakker i analysen library(readxl) # Les data, fjern NA-verdier og hent ut gjeld my_data &lt;- readxl(file = &quot;financedata.xlsx&quot;, sheetIndex = 1) %&gt;% na.omit() debt &lt;- my_data$debt # Konfidensintervall n_obs &lt;- length(debt) # antall observasjoner alpha &lt;- 0.05 # signifikansnivå average &lt;- mean(debt) # gjennomsnitt st_dev &lt;- sd(debt) # standardavvik lower &lt;- average - qt(1 - alpha/2, df = n_obs - 1)*st_dev/sqrt(n) # nedre grense upper &lt;- average + qt(1 - alpha/2, df = n_obs - 1)*st_dev/sqrt(n) # øvre grense Vi oppfordrer deg til å prøve å skrive R-kode som er pen og oversiktlig i datalabbene fremover. Dårlige vaner kan være vonde å vende! 7.1.3 Oppgave 1: Interaktiv øvelse Her skal du bruke et læringsverktøy kalt swirl som vil ta deg gjennom en interaktive øvelse hvor du må utføre forskjellige oppgaver i konsollen. I flere av dataøvingene vil det være en slik interaktiv del. Her er tanken at du skal leke deg litt med R. Før du kan begynne må du installere swirl. Kopier derfor følgende tre linjer og lim dem inn i R-konsollen: install.packages(&quot;swirl&quot;) library(swirl) install_course(&quot;R Programming&quot;) For å starte swirl skriver du så følgende i konsollen: swirl() Du vil i starten bli bedt om å skrive inn ditt navn og så følger litt info om hvordan swirl fungerer. Du blir så bedt om å velge kurs. Her skal du velge alternativet ‘R Programming’ (1 og så enter). Du får så se alle modulene dette kurset inneholder: I denne øvingen skal du prøve deg på modul 1 ‘Basic Building Blocks’, modul 4 ‘Vectors’ (kun første halvdel), og modul 12 ‘Looking at Data’. I modul 1 vil du lære litt om de mest grunnleggende operasjonene som kan gjøres i R. Modul 4 ser nærmere på vektorer og her er første halvdel av modulen mest relevant. Modul 12 tar for seg det å utforske strukturen på et datasett. Start med modul 1 (1 og så Enter). Du vil bli bedt om å gjøre enkle operasjoner i R og av og til må du svare på multiple choice spørsmål: Merk at det helt til høyre vil står hvor langt du har kommet i prosent. Står du helt fast med et punkt kan du skrive skip() for å hoppe over dette punktet. Når du har fullført en modul blir du spurt om du vil motta ‘credit’ for å ha fullført modulen. Her kan du svare nei. Ønsker du å avbryte underveis skriver du bye(). Skriver du inn det samme navnet når du eventuelt starter swirl igjen kan du fortsette der du slapp. Husk å avslutt swirl (Esc) før du begynner på del to av øvingen. Lykke til! 7.1.4 Oppgave 2: Innlesning av data og deskriptiv statistikk i R I denne oppgaven skal vi lese inn noen data og produsere enkel deskriptiv statistikk av disse dataene. Dataene kommer fra et amerikansk forsøk hvor man ville undersøke påstanden om at voldelige dataspill fører til voldelig adferd ved la to grupper spille hvert sitt dataspill. I det “voldelige” dataspillet var oppdraget å skyte og drepe et romvesen, mens i den ikke-voldelige varianten skulle man finne og redde romvesenet fra fare. Utover det var spillene helt likt utformet, og i etterkant av en spilleøkt ble deltakernes aggresjonsnivå målt på en skala fra 1 til 9 ved hjelp av en standard psykologisk test. Dette datasettet ble brukt i eksamensoppgaven vårsemesteret 2019. Oppgave 2.1. Last ned filen violence.xslx. Denne filen lagrer du fortrinnsvis i en egen mappe der du ønsker at filer fra denne øvingen skal ligge. Åpne så RStudio, velg File -&gt; New File -&gt; R Script for å åpne et nytt Rscript. Lagre så scriptet ditt i samme mappen som du har lagt datasettet, slik at du nå har en mappe som ser ut som figuren under: Når vi skal lese inn data, lagre figurer og andre ting har R en standard ‘mappesti’ (working directory) den leter/lagrer i. Du kan se hva denne stien peker på ved å skrive getwd() i konsollen. Du skal nå spesifisere denne mappestien til mappen du har opprettet. Dette gjør du raskest ved å velge Session -&gt; Set Working Directory -&gt; To Source File Location. Neste gang du skal jobbe med dette prosjektet kan du åpne RStudio ved å dobbeltklikke på dataøving1.R, og mappestien skal da settes automatisk til riktig mappe. Lag gjerne en liten overskrift ved hjelp av kommentartegnet # slik at .R filen din ser omtrent slik ut: Oppgave 2.2. Du skal nå lese inn excel filen du lagret i over i R. Selv om det finnes mange funksjoner som allerede er innebygget i R, må man noen ganger installere ekstra ‘pakker’ for å få tilgang til spesielle funksjoner. For å lese inn en excel fil trenger du nettopp en slik ikke standard funksjon. Denne finnes i pakken readxl. Selve installeringen kan du gjøre direkte i konsollen med å skrive (hvis du ikke har gjort det allerede, for eksempel da du gikk gjennom forelesningsvideoen om R-pakker): install.packages(&quot;readxl&quot;) Pakken legger seg da i en bibliotekmappe der R er installert. For å gi R beskjed om å laste inn funksjonene til pakken du nettopp installerte bruker du funksjonen library(). Du har nå tilgang til en funksjon kalt read_excel() som du kan bruke til å lese inn excel filen: # MET4 - Dataøving 1 # ------------------ # les inn data library(readxl) violence &lt;- read_excel(&quot;violence.xlsx&quot;) Marker linjene du nettopp skrev i R-skriptet ditt og trykk Ctrl + Enter (Cmd + Enter), for å opprette objektet violence som inneholder datasettet. Funksjonen ls() lister opp alle objekter som har blitt definert. Du kan prøve selv å skrive følgende i konsollen: ls() ## [1] &quot;violence&quot; Du ser at det har kommet et nytt objekt som heter violence. En tilsvarende oversikt finner du i vinduet øverst til høyre i Rstudio (se Figur 7.1) hvor du også kan klikke på objektet for å se nærmere på det. Oppgave 2.3. Ta en titt på strukturen til datasettet du nettopp leste inn. Husker du kanskje noe fra den interaktive øvelsen ‘Looking at Data’? Når du gjør slike små utforskninger kan du gjerne jobbe direkte i konsollen, og det du gjør i dette punktet trenger nødvendigvis ikke være med i .R-filen din. Gå til konsollen og bruk funksjoner som class, dim, names, head og str for å utforske strukturen på dataene. Vi ser at det er 5 variabler: id er bare et tall som identifiserer forsøkspersonen. aggression_level er aggresjonsnivået som ble målt rett etter at forsøkspersonen hadde spilt en viss tid. violent_treatment er varianten av dataspillet som forsøkspersonen ble utsatt for; enten Violent eller Less Violent. difficulty_treatment er vanskelighetsgraden av spillet, som enten var Easy eller Hard. En mulig forklaring på aggressiv adferd er at vanskelige spill fører til høyere stressnivå, som igjen kan føre til aggressivitet. experienced_violence er svaret til forsøkspersonen på spørsmålet om vedkommende oppfattet spillet som Violent eller Less Violent. Forsøkspersonene visste ikke selv hva forssøket gikk ut på, eller at det var flere varianter av det samme spillet. Oppgave 2.4 Vi skal se nærmere på om aggresjonsnivået er forskjellig i de to gruppene. Da må vi trekke ut de aktuelle tallene fra datasettet. Vi ønsker å velge ut to vektorer for å gjøre denne sammenligningen: en vektor som inneholder aggresjonsnivået til gruppen som har spilt det voldelige dataspillet, og en vektor som inneholder aggresjonsnivået til gruppen som har spilt det ikke-voldelige dataspillet. La disse to vektorene få navn voldelig og ikke_voldelig, og lag dem ved å skrive følgende kodelinjer (se video om pipe-operatoren og enkel datavask for en forklaring av disse funksjonene. Den siste linjen, pull, gjør en dataframe med én kolonne om til en vektor) : # Laster inn dplyr-pakken for library(dplyr) # Vektorer med aggresjonsnivå til gruppen som har spilt voldelig/ikke-voldelig spill voldelig &lt;- violence %&gt;% filter(violent_treatment == &quot;Violent&quot;) %&gt;% select(aggression_level) %&gt;% pull ikke_voldelig &lt;- violence %&gt;% filter(violent_treatment == &quot;Less Violent&quot;) %&gt;% select(aggression_level) %&gt;% pull Sjekk nå at dette har fungert ved å skrive voldelig og ikke_voldelig inn i konsollen for å se at det faktisk er vektorer som inneholder tallene 1 – 9. Bruk også noen minutter til å prøve å forstå hva kodelinjene over faktisk gjør. (NB! Pass på at du skriver Violent og Less Violent helt riktig med store og små bokstaver, ellers vil det ikke fungere!) Oppgave 2.5. I eksamensoppgaven fra 2019 får vi oppgitt deskriptiv statistikk over aggresjonsnivået for de to gruppene i følgende tabell: Bruk funksjoner som min(), max(), median(), mean(), length() og summary() til å finne ut om tallene stemmer (Du vil se at datasettet ditt inneholder noen færre observasjoner enn det som er oppgitt i tabellen, som gjør at tallene er litt forskjellige). Oppgave 2.6. Vi skal nå lage et histogram av hver av gruppene du lagret som vektorer i tidligere. Vi kan først kikke på Figur 2.1 og koden som lagde disse figurene for å få en idé om hva vi må gjøre. Et svært viktig punkt er føgende: ggplot-funksjonen skal alltid ha hele datasettet (en data frame) som argument!! Det betyr at vi ikke skal bruke de to vektorene voldelig og ikke_voldelig, slik som i hist()-funksjonen, men bruke hele datasettet violence. Vi ser av oversikten over at variabelen som inneholder aggresjonsnivået er aggression_level, så det er den vi skal bruke som \\(x\\)-argument. Ved å ta utgangspunkt i koden som lagde Figur 2.1, kan vi gjøre et første forsøk (der vi husker å laste inn ggplot2-pakken først): library(ggplot2) ggplot(violence, aes(x = aggression_level)) + geom_histogram(bins = 9) Nesten! Det eneste problemet er at vi har ett histogram for alle observasjonene, mens det vi egentlig ønsket var å lage et histogram for hver av gruppene. Dette er såre enkelt i ggplot2. Det eneste vi trenger å gjøre er å identifisere den variabelen i datasettet som angir gruppetilhørighet (sjekk variabeloversikten over, svaret er violent_treatment), og så plusse på en funksjon som heter facet_wrap() som vist under. ggplot(violence, aes(x = aggression_level)) + geom_histogram(bins = 9) + facet_wrap(~ violent_treatment) Dersom vi i stedet ønsker et skalert histogram kan vi spesifisere y-argumentet på følgende vis: ggplot(violence, aes(x = aggression_level, y = ..density.. )) + geom_histogram(bins = 9) + facet_wrap(~ violent_treatment) Oppgave 2.7. Når man skal sammenligne sentrum og spredning i to grupper er et boxplott et ypperlig alternativ og vi kan da bruke funksjonen boxplot() i “base R”, eller funksjonen geom_boxplot() hvis vi heller ønsker å benytte ggplot2. Vi holder oss til det siste alternativet her, og ser at kodelinjene ligner på det vi laget over. Dersom vi ønsker å lage et enkelt boxplot av en variabel for å sammenligne spredingen i to eller flere grupper kan vi skrive ggplot(a, aes(x = b, y = c)) + geom_boxplot() Her må du selv erstatte bokstavene a, b og c i henhold til følgende regel: a er navnet på datasettet. b er variabelen som inneholder gruppeinndelingen. c er variabelen som inneholder målingene. De ferdige kodelinjene skal være med i .R-skriptet ditt. For å se boxplottet kan du som vanlig kjøre kommandoene med å trykke Ctrl + Enter. Ser det ut til å være noe forskjell på sentrum og spredning i de to gruppene? Bonusoppgave. Bytt ut geom_boxplot() over med geom_jitter() og geom_violin(). Hva viser disse plottene? "],["dataving-2.html", "7.2 Dataøving 2", " 7.2 Dataøving 2 7.2.1 Interaktiv øvelse Før vi tar fatt på dataanalysen begynner vi med litt R-trening i swirl. Har du allerede installert pakken swirl (skriv install.packages(\"swirl\") i konsoll hvis ikke) starter du opp swirl med å skrive følgende i konsollen: library(swirl) swirl() Du vil i starten bli bedt om å skrive inn ditt navn. Hvis du bruker samme navn som tidligere får du kanskje tilbud om å starte opp igjen der du slapp, men da kan du bare velge det nederste valget ‘No. Let me start something new’. Du velger så alternativet ‘R Programming’ hvor du får se alle modulene dette kurset inneholder. I denne øvingen skal du prøve deg på modul 6 ‘Subsetting Vectors’ og modul 8 ‘Logic’. Husk at det helt til høyre vil står hvor langt du har kommet i prosent. Står du helt fast med et punkt kan du skrive skip() for å hoppe over dette punktet. Når du har fullført en modul blir du spurt om du vil motta ‘credit’ for å ha fullført modulen. Her kan du svare nei. Ønsker du å avbryte underveis skriver du bye(). Skriver du inn det samme navnet når du eventuelt starter swirl igjen kan du fortsette der du slapp. Husk å avslutt swirl () før du begynner på neste del av datalabben. Lykke til! 7.2.2 Data til dataøvelsen I denne dataøvelsen skal vi ved hjelp av R gjennomføre en del av testene som vi har lært i praksis. Vi skal gjøre både ett- og to-utvalgs tester, og vi skal bruke \\(\\chi^2\\)-testen Vi skal jobbe med tre ulike datasett i denne øvingen, og alle sammen kan lastes ned ved å klikke på lenkene under: testdata.xls violence.xlsx roubik_2002_coffee_yield.xlsx Last ned disse filene og legg dem i en mappe på datamaskinen din. Åpne så RStudio, velg File -&gt; New File -&gt; R Script for å åpne et nytt Rscript, og lag gjerne en liten overskrift ved hjelp av kommentartegnet #. Lagre så scriptet ditt i samme mappen som du har lagt datasettene, slik at du nå har en mappe som ser ut som figuren under: Det neste du må gjøre er å sørge for at du har satt opp riktig mappesti (working directory) i RStudio, og det gjør du raskest ved å velge Session -&gt; Set Working Directory -&gt; To Source File Location. Neste gang du skal jobbe med dette prosjektet kan du åpne RStudio ved å dobbeltklikke på dataøving2.R, og mappestien skal da settes automatisk til riktig mappe. I alle tilfeller skal vinduet ditt se omtrent slik ut: 7.2.3 Oppgaver til øvingen: 7.2.3.1 Oppgave 1 Costa Rica er en stor kaffeprodusent med moderne produksjon. Kaffeprodusentene har over lengre tid benyttet en standardisert miks av sprøytemidler som skal ta knekken på ugress og skadelige insekter, men uten å skade avlingen eller miljøet ellers. En liten kaffeplantasje i Costa Rica har begynt å eksperimentere med en ny kombinasjon av sprøytemidler som skal være like effektiv mot ugress, men samtidig enda mer skånsom mot kaffeplantene, slik at avlingen blir større. Innehaveren av plantasjen ønsker å sette opp et eksperiment for å undersøke denne påstanden. Han velger ut 25 tilfeldige jordlapper fordelt på hele eiendommen der han bruker de nye sprøytemidlene gjennom en hel sesong. Lang erfaring har vist at avlingen ved bruk av gammel metode er normalfordelt med forventning \\(\\mu = 100\\) og standardavvik \\(\\sigma = 10\\), der vi har brukt en standardisert enhet for mengde avling per arealenhet. Hjelp bonden, ved å løse følgende oppgaver: Oppgave 1.1: Les inn datasettet testdatasdata.xsl i RStudio og se på de første par radene. Det kan du gjøre ved å kjøre følgende kodelinjer: library(readxl) # Pakke for å lese excel-filer data &lt;- read_excel(&quot;testdata.xls&quot;) # Leser inn datasettet data # Ser på datasettet ## # A tibble: 25 × 4 ## X1 X2 A1 A2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 122. 121. 1 1 ## 2 101. 105 1 1 ## 3 114. 108 1 1 ## 4 103 99.1 1 0 ## 5 97.6 96.2 0 0 ## 6 85.9 95.4 0 0 ## 7 93.5 101. 0 1 ## 8 92 97 0 0 ## 9 98.8 106. 0 1 ## 10 99.9 100. 0 1 ## # ℹ 15 more rows Det er kolonnen X2 som inneholder de observerte avlingene på de 25 forsøksseksjonene. Oppgave 1.2: Er forventet avling ved bruk av den nye metoden større enn forventet avling ved bruk av den gamle metoden? Hint: Forelesningsnotatene/scriptet inneholder koden du trenger for å løse denne og neste oppgave. Du kan også se på oversikten over relevante R-kommandoer for det du trenger. Oppgave 1.3: Det er viktig for kaffebonden at avlingen ikke varierer for mye mellom de ulike delene av farmen. En viktig måleparameter for denne type produksjon er derfor variansen. Kan vi slå fast at variansen til avlingen har forandret seg etter omlegging til ny metode? Oppgave 1.4: Kaffebonden er skeptisk til påstanden om at forventet avling med den gamle metoden er \\(\\mu = 100\\), og mener at det vil variere med for eksempel jordsmonn. For å ta høyde for dette gjennomførte han året i forveien tilsvarende målinger på de samme jordlappene, med med gammel sprøytemetode. Disse målingene finner du i kolonne X1 i datasettet. Test om avlingene er forskjellige, både med og uten paring av observasjonene. Kommenter resultatet. 7.2.3.2 Oppgave 2 Vi skal i denne oppgaven se på oppgave 1a og 1b som ble gitt på skoleeksamen i MET4 vårsemesteret 2019. Dette er det samme datasettet som vi så på i forrige dataøving. I et amerikansk forsøk ville man undersøke påstanden om at voldelige dataspill fører til voldelig adferd ved la to grupper spille hvert sitt dataspill. I det “voldelige” dataspillet var oppdraget å skyte og drepe et romvesen, mens i den ikke-voldelige varianten skulle man finne og redde romvesenet fra fare. Utover det var spillene helt likt utformet, og i etterkant av en spilleøkt ble deltakernes aggresjonsnivå målt på en skala fra 1 til 9 ved hjelp av en standard psykologisk test. I denne oppgaven skal vi i hovedsak finne ut om gruppen som spilte de voldelige dataspillet hadde signifikant høyere aggresjonsnivå enn kontrollgruppen. Oppgave 2.1: Les inn datasettet violence.xslx på samme måte som i forrige dataøving. Hvis du allerede har kjørt library(readxl) trenger du ikke gjøre det igjen med mindre du har startet RStudio på nytt. Gi datasettet et passende navn, f.eks violence &lt;- read_excel(&quot;violence.xlsx&quot;) Vi skal altså teste om aggresjonsnivået er forskjellig i de to gruppene. Da må vi trekke ut de aktuelle tallene fra datasettet. Som vi husker fra forelesningsnotatene trenger vi to vektorer for å gjøre en to-utvags \\(t\\)-test: en vektor som inneholder aggresjonsnivået til gruppen som har spilt det voldelige dataspillet, og en vektor som inneholder aggresjonsnivået til gruppen som har spilt det ikke-voldelige dataspillet. La disse to vektorene få navn voldelig og ikke_voldelig, og lag dem ved å skrive følgende kodelinjer (samme som forrige dataøving): voldelig &lt;- violence %&gt;% filter(violent_treatment == &quot;Violent&quot;) %&gt;% select(aggression_level) %&gt;% pull ikke_voldelig &lt;- violence %&gt;% filter(violent_treatment == &quot;Less Violent&quot;) %&gt;% select(aggression_level) %&gt;% pull Oppgave 2.2: Vi er nå klare til å gjøre en to-utvalgs \\(t\\)-test for om aggresjonsnivået er det samme i de to gruppene. Prøv å gjøre det nå, men vær bevisst på hvilke valg du gjør underveis, og som du mater inn i t.test()-funksjonen, f.eks: Antar du lik varians i de to gruppene? Hvorfor/Hvorfor ikke? Bruker du ensidig eller tosidig test? Hvorfor? Oppgave 2.3: En avgjørende detalj i studien som vi ser på i denne oppgaven er at forskerne også spurte forsøkspersonene hvorvidt de selv syntes spillet de spilte var voldelig. For å kunne trekke noen som helst lærdom fra et slikt forsøk er det viktig at den voldelige spillvarianten faktisk blir oppfattet som voldelig og vice versa. Vi ønsker dermed å undersøke nullhypotesen om at variablene violence_tratment og experienced_violence er uavhengige av hverandre. Den hypotesen er vi nødt til å forkaste for at forsøket skal være gyldig: hvis det ikke er noen sammenheng mellom opplevd og faktisk voldelighet er forsøket helt klart ugyldig. Første steg er å lage et nytt datasett der vi bare ta med oss de to kolonnene vi er interessert i. Kall det hva du vil, f.eks. violence_redusert. Vi bruker select()-funksjonen til å velge ut variablene vi trenger, se video om datavask dersom du trenger å repetere denne funksjonen. violence_redusert &lt;- violence %&gt;% select(violent_treatment, experienced_violence) Skriv violence_redusert i konsollen for å bekrefte at du har valgt ut de korrekte kolonnene. Vi fortsetter som i videoforelesningen og lager en krysstabell for disse variablene krysstabell &lt;- table(violence_redusert) krysstabell ## experienced_violence ## violent_treatment Less Violent Violent ## Less Violent 114 9 ## Violent 33 93 Oppgave 2.4: Heldigvis ser det ut til at det er en klar sammenheng mellom faktisk og opplevd voldelighet ved at de fleste forsøkspersonene havner på diagonalen i krysstabellen. Bruk funksjonen chisq.test() på samme måte som i forelesningen til å formelt teste nullhypotesen om uavhengighet. 7.2.3.3 Oppgave 3 Vi skal i denne oppgaven returnere til kaffeproduksjon. Vi skal gjøre statistiske tester i R som i de tidligere oppgavene i denne øvingen, men vanskelighetsgraden går opp fordi vi også må tenke nøye over hvordan vi anvender metodene korrekt i en gitt kontekst. I 2002 publiserte det prestisjetunge tidsskriftet Nature en kort artikkel skrevet av David W. Roubik1, som handler om den kjente kaffebønnen Arabica. Arabicabønnen kommer opprinnelig fra Afrika, og er en selvpollinerende plante. Det vil si at den ikke er avhengig av insekter for å formere seg, og man trodde lenge at den heller ikke hadde noen fordeler av insektspollinering. For å undersøke denne påstanden samlet Roubik inn historiske data over arabicaavlinger fra hele verden. Han delte verdens kaffeproduserende land inn i to kategorier: Old world som omfatter afrikanske og asiatiske land, og New world som omfatter land i Latin-Amerika. Han registrerte videre gjennomsnittlig årlig avling (målt i kg/hektar) i to perioder: 1961–80 og 1981–2001. Nøkkelen til analysen er at den afrikanske honningbien var en viktig pollinator i Afrika og Asia både i den første og andre perioden, men knapt eksisterte i Amerika før 1980. Etter 1980, derimot, økte utbredelsen av denne bien i Amerika, og ble fort naturalisert. Kan vi sette denne utviklingen i sammenheng med økt kaffeavling i Latin-Amerika etter 1980, og dermed skrote teorien om at kaffeplanter ikke drar nytte av insektspollinering? Oppgave 3.1: For å undersøke dette kan vi bruke datasettet som Roubik brukte, som finnes i filen roubik_2002_coffe_yield.xlsx. Last datasettet inn i R på vanlig måte, og se på det: yield &lt;- read_excel(&quot;roubik_2002_coffe_yield.xlsx&quot;) yield ## # A tibble: 28 × 4 ## world country yield_61_to_80 yield_81_to_01 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 new Costa_Rica 9139 14620 ## 2 new Bolivia 7686 8767 ## 3 new El_Salvador 9996 8729 ## 4 new Guatemala 5488 8231 ## 5 new Colombia 5920 7740 ## 6 new Honduras 4096 7264 ## 7 new Nicaragua 4566 6408 ## 8 new Brazil 4965 6283 ## 9 new Peru 5487 5740 ## 10 new Mexico 5227 5116 ## # ℹ 18 more rows Vi ser at det er fire kolonner i datasettet: world angir om det er snakk om New world (new) eller Old world (old). country angir navnet på landet. yield_61_to_80 angir avlingen i perioden 1961–80. yield_81_to_01 angir avlingen i perioden 1981–2001. Oppgave 3.2: Kall den første tidsperioden p1 og den andre tidsperioden p2. Lag så fire vektorer, en for hver kombinasjon av world og tidsperiode ved å bruke samme teknikk som i oppgave 2.2 over. Når du er ferdig, skal du ha laget følgende vektorer: new_p1: inneholder avling for alle land med world == new i første periode. new_p2: inneholder avling for alle land med world == new i andre periode. old_p1: inneholder avling for alle land med world == old i første periode. old_p2: inneholder avling for alle land med world == old i andre periode. Her må du bruke både filter() og select(), og du må avslutte med en pull for å oversette en en-kolonnes dataframe til en vektor. Dersom du har gjort det riktig, ser vektorene slik ut når du er ferdig: new_p1 ## [1] 9139 7686 9996 5488 5920 4096 4566 4965 5487 5227 2347 3089 1938 new_p2 ## [1] 14620 8767 8729 8231 7740 7264 6408 6283 5740 5116 4124 3240 ## [13] 2789 old_p1 ## [1] 4251 10522 3509 10028 5667 17064 5904 4001 6604 4738 5716 3824 ## [13] 3525 3393 3213 old_p2 ## [1] 13380 11561 9652 9593 8797 7869 7354 7288 6055 5432 5394 3576 ## [13] 3141 2391 2136 Oppgave 3.3: Bruk en paret \\(t\\)-test til å finne ut om kaffeavlingen i den gamle verden er signifikant forskjellig i de to tidsperiodene. Oppgave 3.4: Bruk en paret \\(t\\)-test til å finne ut om kaffeavlingen i den nye verden er signifikant forskjellig i de to tidsperiodene. Oppgave 3.5 (Diskusjonsopgave): Dersom du har gjort de to foregående oppgavene riktig vil du se at den gjennomsnittlige kaffeavlingen ikke har endret seg signifikant i den gamle verden, mens økningen i den nye verden er klart statistisk signifikant. Vi har brukt parrede \\(t\\)-tester, slik at vi “kontrollerer for” eventuelle landeffekter (denne terminologien blir skal vi bruke mer når vi skal jobbe med regresjon). Roubik omtaler funnet som følger: A substantial increase in Latin American coffee yield partly coincided with the establishment of African honeybees in those countries, although there was no such change in the Old World, where honeybees originated […]. This comparison underlines a possible cause-and-effect relationship between the presence of social bees and cofee yield. Dette er intet mindre enn en kortslutning, på minst to forskjellige måter. Hvorfor? Diskuter med dine medstudenter. Kan det gjennomføres en enkel test som gir et bedre bilde av situasjonen? David W. Roubik: The value of bees to the coffee harvest. Nature (2002)↩︎ "],["dataving-3.html", "7.3 Dataøving 3", " 7.3 Dataøving 3 7.3.1 Oppgave 1: Interaktiv øvelse Før vi tar fatt på dataanalysen begynner vi som vanlig med litt R-trening i swirl. Har du allerede installert pakken swirl (skriv install.packages(\"swirl\") i konsoll hvis ikke) starter du opp swirl med å skrive følgende i konsollen: library(swirl) install_course(&quot;Regression_Models&quot;) # legger til nytt kursmateriale om regresjon swirl() Du vil i starten bli bedt om å skrive inn ditt navn. Hvis du bruker samme navn som tidligere får du kanskje tilbud om å starte opp igjen der du slapp, men da kan du bare velge det nederste valget ‘No. Let me start something new’. Du velger så alternativet ‘Regression Models’ hvor du får se alle modulene dette kurset inneholder. I denne øvingen skal du prøve deg på modul modul 1 ‘Introduction’. Her vil du lære litt om hvordan du kan bruke R til å gjøre en regresjonsanalyse ved hjelp av et treningsdatasett. Noen av kommandoene som gjennomgås i denne modulen vil komme til nytte senere i datalabben. Husk at det helt til høyre vil står hvor langt du har kommet i prosent. Står du helt fast med et punkt kan du skrive skip() for å hoppe over dette punktet. Når du har fullført en modul blir du spurt om du vil motta ‘credit’ for å ha fullført modulen. Her kan du svare nei. Ønsker du å avbryte underveis skriver du bye(). Skriver du inn det samme navnet når du eventuelt starter swirl igjen kan du fortsette der du slapp. Husk å avslutt swirl (esc) før du begynner på del to av øvingen. Lykke til! 7.3.2 Oppgave 2: Regresjonsanalyse Et rock-and-roll museum åpnet i Atlanta i 1990. Museet lå i en sentral del av byen i nærheten av mange ulike butikker. Mot slutten av juli måned i 1992 startet en stor brann i en av disse butikkene som ødela hele kvartalet, inkludert museet. Heldigvis var museet forsikret, både mot selve brannskadene, og mot tapte billettinntekter i gjennoppbyggingsperioden. Vanligvis vil et forsikringsselskap beregne erstatningsbeløpet under antakelsen om at besøkstallene i gjenoppbyggingsperioden ville vært på samme nivå som besøkstallene i tiden før brannen. I dette tilfellet mente derimot eierne av museet at besøkstallene var økende, slik at de reelt sett hadde krav på et større erstatningsbeløp. Argumentet var basert på besøkstallene til en fornøyelsespark like ved. Fornøyelsesparken åpnet i desember 1991, slik at museet og parken opererte sammen i de siste fire ukene av 1991, og de første 28 ukene i 1992 før brannen ødela museet. Museet åpnet igjen i april 1995, men var da betydelig større enn det var opprinnelig. Data for besøkstall for museet og fornøyelsesparken finner vi i regnearket C16-01.xlsx. Som i de to foregående dataøvingene legger du denne filen i en mappe på maskinen din, og oppretter et tomt R-script der du lagrer koden for denne oppgaven. Oppgave 2.1: Kikk raskt på datasettet i Excel eller tilsvarende. Du ser at det er tre kolonner, en som angir ukenummer (Week, teller fra 1 til 205), en som angir ukentlig besøkstall på museet (Museum) og en som angir ukentlig besøkstall i fornøyelsesparken (A-Park). Legg merke til at besøkstallet i museet er null fra og med uke 33, til og med uke 179, som er perioden fra brannen til nyåpning. Oppgave 2.2: Last så datasettet inn i R som før ved hjelp av read_excel()-funksjonen. Gi det et passelig navn (f.eks visits), og sjekk raskt at det har gått bra ved å taste inn datanavnet i konsollen. Da skal det se omtrent slik ut: visits ## # A tibble: 205 × 3 ## Week Museum `A-Park` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 787 1379 ## 2 2 1179 1396 ## 3 3 4225 5332 ## 4 4 1336 1477 ## 5 5 2122 3717 ## 6 6 1136 1663 ## 7 7 2413 3573 ## 8 8 1399 2086 ## 9 9 1528 2503 ## 10 10 1788 2553 ## # ℹ 195 more rows Legg merke til følgende: Observasjonene ser ut til å være de samme som vi så da vi kikket på selve regnearket. Det er alltid en god vane å forsikre seg om at R har lest inn datasettet på riktig måte. Et av variabelnavnene har fått noen rare tødler rundt seg. Grunnen til det er at A-park inneholder en bindestrek, så for at R ikke skal tolke det tegnet som et minustegn (og dermed gi oss et mareritt med feilmeldinger), må vi alltid bruke disse tødlene når vi refererer til denne variabelen. (På tastaturet som forfatteren av disse ord skriver på, er det Shift + tasten til venstre for Backspace. Oppgave 2.3: Før vi går videre, må vi få et bedre begrep om problemet ved å kikke grafisk på observasjonene. La oss plotte observasjonene i et linjeplott for å se hvordan de utvikler seg over tid, ved å ha ukenummer på \\(x\\)-aksen og besøkstall på \\(y\\)-aksen. Vi kan lage et enkelt plott for besøkstall for museet ved å skrive # Laster først ggplot-pakken (det trenger vi bare gjøre en gang i skriptet) library(ggplot2) # Lager et enkelt linjeplott: ggplot(visits, aes(x = Week, y = Museum)) + geom_line() Du kan legge til besøkstall for fornøyelsesparken ved å plusse på en ny linje med geom_line(), men da må du spesifisere y-variabelen på nytt. Hele plottekommandoen blir da: ggplot(visits, aes(x = Week, y = Museum)) + geom_line() + geom_line(aes(y = `A-Park`)) Vi ser at det er en sterk sammenheng mellom besøkstallene til museet og parken, spesielt etter gjenåpningen i 1995, og det skal vi utnytte når vi senere skal beregne erstatningssummen. Oppgave 2.4: Juster på argumentene i geom_line()-funskjonene, og legg til flere “lag” på samme måte som vi gjorde for å pynte på figuren i oppgave 3 i kapittel 1.11 (det er 100% lov å Google). Dette ser bedre ut: Oppgave 2.5: La oss nå ta utgangspunkt i forsikringsselskapets påstand: besøkstallet i perioden der museet er stengt skal beregnes ved hjelp av observasjonene før brannen. Vi estimerer parametrene i en enkel regresjonsmodell \\[y_i = \\beta_0 + \\beta_1x_i + \\epsilon,\\] der responsvariabelen \\(y_i\\) er besøkstallet på museet på dag nr. i, og \\(x_i\\) er besøkstallet i fornøyelsesparken samme dag. Datasettet vi skal bruke er altså de 32 første radene i datasettet visits. Da kan vi enten lage en ny tabell som består av de 32 første radene (for eksempel ved hjelp av filter(Week &lt;= 32)), eller så kan vi bruket argumentet subset i lm()-funkesjonen til å spesifisere hvilke observasjoner som skal brukes for å estimere modellen: reg1 &lt;- lm(Museum ~ `A-Park`, data = visits, subset = 1:32) Oppgave 2.6: Pakken stargazer inneholder funksjoner for å lage pene regesjonstabeller automatisk fra regresjonsobjekter i R. Pakken må installeres og lastes på vanlig måte: install.packages(&quot;stargazer&quot;) library(stargazer) Inne i stargazer-pakken er det en funksjon som også heter stargazer(). Hvis du ikke har sett den brukt før (f.eks i forelesning), kan du lese mer om den ved hjelp av hjelpefunksjonen: ?stargazer. Bruk så stargazer() til å lage følgende regresjonsutskrift (hint: bruk argumentet type = \"text\"): =============================================== Dependent variable: --------------------------- Museum ----------------------------------------------- `A-Park` 0.693*** (0.018) Constant 16.229 (114.695) ----------------------------------------------- Observations 32 R2 0.979 Adjusted R2 0.979 Residual Std. Error 355.588 (df = 30) F Statistic 1,424.094*** (df = 1; 30) =============================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Oppgave 2.7: Lag tre diagnoseplott etter mal som er gitt i forelesningsnotatene: Et som viser residualene i regresjonsmodellen i et spredningsdiagram, et QQ-plott, og et histogram. Kan du gjøre en grov vurdering om hvorvidt forutsetningene for lineær regresjon er oppfylt? Oppgave 2.8: Bruk denne regresjonsmodellen til å beregne hva besøkstallet hadde vært dersom museet hadde vært åpent som vanlig. Dette kan vi gjøre ved å bruke predict()-funksjonen. Følg oppskriften under nå, så skal du prøve å gjøre det selv etterpå. # Vi lager et nytt datasett bestående av de ukene der museet var stengt: visits_pred &lt;- visits %&gt;% filter(Museum == 0) %&gt;% select(&quot;Week&quot;, &quot;A-Park&quot;) # Bruker predict()-funksjonen til å predikere tilhørende y&#39;er: predicted_visits1 &lt;- predict(reg1, newdata = visits_pred) # Til slutt legger vi til de predikerte verdiene som en ny kolonne i visits_pred: visits_pred$predikert1 &lt;- predicted_visits1 De predikerte besøkstallene er nå lagret som kolonne predicted1 i datasettet visits_pred. Oppgave 2.9 For å få bedre greie på hvordan prediksjonene egentlig ser ut kan vi legge dem til figuren vår fra over. La oss lage en blå stiplet linje, og det kan vi gjøre med å legge til enda et kall til geom_lines(). Denne gangen må vi bruke flere argumenter: Vi må bruke argumentet data til å si at tallene vi skal plotte for den nye linjen nå ligger i datasettet visits_pred, og ikke visits. Vi må bruke argumentet colour til å fortelle hvilken farge vi skal ha på linjen. Vi må bruke argumentet linetype til å fortelle at vi vil ha en stiplet linje. Ta for deg figuren du lagde i oppgave 2.4, og legg til følgende linjer (husk å få med en + mellom hvert lag): geom_line(aes(x = Week, y = predikert1), data = visits_pred, colour = &quot;blue&quot;, linetype = &quot;dashed&quot;) Da blir figuren min seende slik ut: Oppgave 2.10: Kommenter kort regresjonsutskriften fra oppgave 2.6 og figuren fra oppgave 2.9. Ser det fornuftig ut? Oppgave 2.11: Se på saken heller fra museets side. De mener at det er besøkstallene fra etter åpningen i 1995 som skal brukes til å estimere regresjonsmodellen. Det er lett å forstå hvorfor de ønsker det, for da ser det ut som at det er omtrent like mange besøkende på museet som i fornøyelsesparken. Repeter oppgave 2.5, men nå bruker du altså besøkstallene fra etter åpningen til å estimere regresjonskoeffisientene. Hint 1: Det eneste du må endre er hva som skal inn i subset-argumentet. Hint 2: Stigningstallet i den nye modellen skal være 0.97. Oppgave 2.12: Beregn hvilke besøkstall museet hadde hatt i perioden det var stengt ved å legge til grunn den nye regresjonsmodellen etter mønster fra oppgave 2.8, og legg dem inn i figuren etter mønster fra oppgave 2.9. Figuren blir skal da se omtrent slik ut hvis vi bruker en finn grønnfarge (forestgreen) til den siste linjen: Oppgave 2.13: Når vi ser hvor tett de to besøkstallene beveger seg etter nyåpningen er det ikke rart at de beregnede besøkstallene basert på den nye modellen (markert i grønt over) følger observasjoenen fra fornøyelsesparken. Anta at hver billett til museet koster $6.99. Hvor stor er differansen mellom erstatningskravet til museet og tilbudet til forsikringsselskapet? Oppgave 2.14 (Diskusjon): Det er ganske stor forskjell mellom tilbud og krav, men hvis vi tenker oss om skjønner vi fort at begger parter befinner seg i en klassisk catch-22. Hvis den ene partens argument fører til en utbetaling som er for stor eller for liten fordi de tar utgangspunkt i slutten eller starten på en stigende utvikling, må nødvendigvis det motsatte standpunkt også være galt av nøyaktig samme grunn. Kan du foreslå et kompromiss? "],["dataving-4.html", "7.4 Dataøving 4", " 7.4 Dataøving 4 En produsent som vil selge kraft til Nord Pool leverer salgsbud til kraftbørsen som spesifiserer, for hver time neste dag, hvor mange megawatt (MW) man er villig til å produsere til ulike priser. Fristen for å levere salgsbud er kl 12:00 dagen før produksjonen skal finne sted. For en vindkraftprodusent er det flere usikre faktorer man må ta stilling til når man skal levere salgsbud for neste dag: Timeprisene i spotmarkedet (Euro/MW) er ukjente (blir ikke offentliggjort før kl 12:45). For å vite hvor mange megawatt (MW) man kan produsere i en gitt time trenger man å vite vindstyrken. Selv med gode værprognoser vil det fremdeles være betydelig usikkerhet knyttet til vindstyrken i de ulike timene neste dag. Dersom den faktiske produksjonen avviker fra det man har meldt inn vil det påløpe en straffekostnad. I timer med mye vind vil man måtte selge den overskytende produksjonen til en lavere pris enn spotprisen, og i timer med lite vind vil man måtte kompensere fleksible produsenter for å dekke opp for den manglende produksjonen. Straffekostnaden (Euro/MW) for over- eller underproduksjon er en ukjent størrelse på budgivningstidspunktet. I denne dataøvingen skal vi konsentrere oss om å lage prognoser for spotprisen. I case 3 i BED4 kommer dere også til å få bruk for vindstyrken og straffekostnadene, men å lage prognoser for disse vil kreve ferdigheter utover det som gjennomgås i MET4. Hvis du ikke tar BED4 dette semesteret, så går det helt fint også. Dataøvingen står fint på egne bein, og du kan uansett komme tilbake til disse resultatene hvis du for eksempel skal ta BED4 på et senere tidspunkt. Du har kanskje lagt merke til at R-kodingen i tidsrekkemodulen har en litt forskjellig stil fra det vi har gjort tidligere i kurset. I denne øvingen vil derfor størsteparten av koden blir oppgitt i oppgaveteksten. Din oppgave blir å kjøre koden, få ut figurer og resultater, samt å kommentere og tolke resultatene. Vi starter med å laste inn noen pakker som vi kommer til å trenge. Som vanlig må du installere pakkene først dersom du ikke har gjort det allerede: library(forecast) library(ggplot2) 7.4.1 Oppgave 1: Last inn og se på datasettet Denne gang er datasettet pakket inn i en såkalt .Rdata-fil. Det er et enkelt filformat for å lagre R-objekter. Last ned p_da.Rdata, og last datasettet inn i R ved å kjøre følgende kommando (der du selvsagt har satt arbeidsmappen til der du har lagt datafilen): load(&quot;p_DA.Rdata&quot;) Du skal nå få to tidsrekker i minnet: n03 og no5, som i figuren under: Disse to tidsrekkene inneholder strømprisen i to prisområder i Norge hver time fra 1. januar 2022 til og med 21. september 2022. Tidsrekken no3 inneholder prisen for Midt-Norge (NO3) og no5 inneholder prisen for Vest-Norge (NO5). Vi konsentrerer oss om NO3 i første omgang, og kikker raskt på datasettet ved å plotte tidsrekken direkte med autoplot()-funksjonen som vi finner i forecast-pakken: autoplot(no3) På \\(x\\)-aksen har vi antall dager siden 1. januar 2022. Skriv en kort kommentar der du peker på noen viktige karakteristikker ved denne tidsrekken. Du må gjerne bruke xlim-argumentet i plot()-funksjonen for å zoome inn og se på mindre tidsperioder. Ser tidsrekken ut til å være stasjonær? 7.4.2 Oppgave 2: Gjem bort den siste dagen slik at vi kan sjekke prediksjonene våre For å kunne gjøre en vurdering av hvor gode prediksjonene våre er, deler vi nå datasettet vårt i to, der vi tar ut de siste 24 timene som vi kan bruke til å evaluere prediksjonene. Vi kaller disse to delene no3_train, som er den lange delen som vi skal bruke til å estimere en modell (trene en modell), og no3_test som er de siste 24 timene som vi skal bruke til å teste etterpå om modellen duger. Vi bruker funksjonen window() til å hente ut deler av tidsrekken, der vi må spesifisere start- og sluttverdier som vektorer med to elementer; en for dag og en for time. Det er 264 dager i datasettet vårt, så da får vi: no3_train &lt;- window(no3, start = c(1, 0), end = c(263, 23)) no3_test &lt;- window(no3, start = c(264, 0), end = c(264, 23)) Du kan nå dobbelsjekke at du har fått ut en enkelt dag i no3_test ved å kjøre autoplot(no3_test). 7.4.3 Oppgave 3: Hent ut sesong og trend Vi har lært at et første steg i tidsrekkeanalyse er å hente ut eventuelle sesong og trendkomponenter. Vi har også sett at det er en enkel funksjon i R som kan gjøre dette for oss, nemlig stl(), slik vi så i seksjonen om trend og sesong. dekomponert &lt;- stl(no3_train, s.window = &quot;periodic&quot;) autoplot(dekomponert) Det var ikke så lett å se detaljer i sesongkomponenten i dette plottet, så vi zoomer inn på en mindre del av x-aksen: autoplot(dekomponert) + xlim(245, 260) Gi en kort beskrivelse av de ulike komponentene i tidsrekken. 7.4.4 Oppgave 4: Prediker treningstidsrekken 24 steg frem og visualiser resultatene. Vi skal nå slippe veldig billig unna! Denne oppgaven består egentlig av flere steg: Finn en statistisk modell for residualtidsrekken. Bruk for eksempel auto.arima() for å finne den ARIMA-modellen som passer best til treningsdatasettet. Bruk denne modellen til å predikere residualtidsrekken 24 steg frem. Skriv trendserien 24 steg frem. Her må vi ha en fornuftig måte å ekstrapolere som vi ikke har dekket eksplisitt i materialet vårt. Hekt på en ny dag med den daglige sesongvariasjonen. Legg sammen prediksjonene av residualene, trend- og sesongkomponenten for å lage en prediksjon av prisserien. Vi kunne godt satt oss ned for å programmere disse stegene hver for seg. Heldigvis har noen gjort dette før oss, gjennom funksjonen forecast() i forecast-pakken. For å lage en prognose må vi sende inn den dekomponerte tidsrekken, sammen med en spesifikasjon av hvilken type statistisk tidsrekkemodell vi ønsker å tilpasse til residualtidsrekken (vi velger ARIMA, for det er den modellen vi har lært om), og hvor mange steg frem vi ønsker å predikere. Vi kan også legge til signifikansnivået for et prediksjonsintervall som vi vil ha på 95%: prognose &lt;- forecast(dekomponert, method = &quot;arima&quot;, h = 24, level = 95) Vi kan visualisere resultatet ved hjelp av autoplot(). For å kunne se noe fornuftig i plottet så tar vi bare med de 100 siste observerte tidsstegene i tillegg til de 24 prediksjonene: autoplot(prognose, include = 100) Gi en kommentar til dette plottet. 7.4.5 Oppgave 5: Sammenlign prediksjonene med de faktiske observasjonene Vi kan nå finne frem igjen no3_test, som er en tidsrekke som inneholder de faktiske observasjonene for dagen der vi har gjort prediksjoner. La oss sammenligne. En måte å visualisere dette på er å hente ut prediksjonene fra prognose-objektet (de ligger under $mean), og sette det sammen med de faktiske observasjonene (som vi har lagret i no3_test). Vi kan også hente ut prediksjonsintervallene, og sette alt inn i en data frame: # Setter prediksjoner, intervaller og observasjoner inn i samme data frame. prediksjoner &lt;- data.frame( x = 1:24, # Timer i døgnet for x-aksen prediksjon = prognose$mean, # Predikerte priser nedre = prognose$lower[,1], # Nedre og øvre prediksjonsintervaller ovre = prognose$upper[,1], observert = no3_test # De faktiske observasjonene ) # Lager plott ggplot(prediksjoner) + geom_line(aes(x = x, y = prediksjon), linetype = &quot;dashed&quot;) + # Prediksjoner geom_line(aes(x = x, y = nedre), colour = &quot;darkred&quot;) + # Nedre grense geom_line(aes(x = x, y = ovre), colour = &quot;darkred&quot;) + # Øvre grense geom_line(aes(x = x, y = observert), size = 1.5) + # Observert xlab(&quot;&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;24 timers prognose av strømpris&quot;) + theme_minimal() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning ## was generated. I dette plottet er prediksjonene av strømprisen vist som en stiplet linje, mens de prisene som faktisk ble observert er vist som en tykk heltrukken linje. Kommenter plottet. 7.4.6 Oppgave 6: Lag prediksjoner for neste dag Så langt har vi brukt alle dagene i datasettet vårt bortsett fra den siste til å predikere strømprisen på den siste dagen. Resultatene ser ut til å være gode. Du kan nå gjenta denne prosedyren, men i stedet for tidsrekken no3_train skal du nå bruke hele tidsrekken no3 til å predikere prisen for dagen etter det – den 22. september 2022 – en dag der vi ikke har de faktisk realiserte prisene i datasettet vårt. Du skal få et prediksjonsplott som ser slik ut: Du kan så samle prediksjonene i en data frame som i forrige oppgave, men der vi selvsagt ikke kan ha med en kolonne med observerte priser, siden vi ikke har dem tilgjengelige. Den endelige tabellen med observasjoner skal se slik ut: ## x prediksjon nedre ovre ## 1 1 36.02709 12.1495723 59.90460 ## 2 2 36.45012 3.3208432 69.57940 ## 3 3 37.28821 -3.5736943 78.15012 ## 4 4 38.53910 -7.6663095 84.74451 ## 5 5 39.89017 -9.5638644 89.34420 ## 6 6 42.27134 -9.0837471 93.62643 ## 7 7 43.55218 -8.9521015 96.05647 ## 8 8 49.43543 -3.8202856 102.69114 ## 9 9 54.36794 0.5712673 108.16460 ## 10 10 56.23131 2.0083818 110.45425 ## 11 11 54.74981 0.1657312 109.33389 ## 12 12 51.32547 -3.5811987 106.23213 ## 13 13 49.64904 -5.5564816 104.85456 ## 14 14 47.41288 -8.0764075 102.90216 ## 15 15 46.22697 -9.5361822 101.99013 ## 16 16 46.70673 -9.3236145 102.73708 ## 17 17 46.61878 -9.6740934 102.91166 ## 18 18 48.72382 -7.8282025 105.27583 ## 19 19 50.11119 -6.6974082 106.91979 ## 20 20 50.21319 -6.8499490 107.27634 ## 21 21 50.04209 -7.2739046 107.35808 ## 22 22 48.34032 -9.2270566 105.90769 ## 23 23 45.94396 -11.8734681 103.76139 ## 24 24 42.20995 -15.8563146 100.27621 Kommenter. 7.4.7 Oppgave 7: Lagre prediksjonene i en excel-fil. Hvis du tar BED4 dette semesteret (eller skal ta BED4 på et senere tidspunkt) så trenger du nå å eksportere disse prediksjonene til en Excel-fil. En pakke som kan gjøre dette er writexl (som du, igjen, er nødt til å installere før bruk: install.packages(\"writexl\")). Hvis du har lagret prediksjonene i en data frame som heter prediksjoner2, så kan du skrive den ut i en Excel-fil på følgende måte: library(writexl) write_xlsx(prediksjoner2, &quot;no3_prediksjoner.xlsx&quot;) Du skal nå ha en Excel-fil med prediksjonene dine i filen no3_prediksjoner.xlsx i arbeidsmappen din. 7.4.8 Oppgave 8: Gjør det samme for no5 Du kan nå gjenta øvelsen over for å få ut samme type prediksjoner for det andre prisområdet. Tidsrekken finner du i no5 og prediksjonsplottet ser slik ut: Data framen med prediksjonene skal se slik ut: prediksjoner3 &lt;- data.frame( x = 1:24, # Timer i døgnet for x-aksen prediksjon = prognose3$mean, # Predikerte priser nedre = prognose3$lower[,1], # Nedre og øvre prediksjonsintervaller ovre = prognose3$upper[,1] ) prediksjoner3 ## x prediksjon nedre ovre ## 1 1 346.3622 310.5934 382.1310 ## 2 2 338.6796 286.6895 390.6698 ## 3 3 344.5907 281.3350 407.8464 ## 4 4 358.3632 286.3178 430.4086 ## 5 5 378.5406 300.1708 456.9105 ## 6 6 402.4033 319.0857 485.7209 ## 7 7 424.7142 337.5252 511.9031 ## 8 8 441.2872 350.5347 532.0397 ## 9 9 441.0059 346.6958 535.3159 ## 10 10 417.9910 319.6357 516.3462 ## 11 11 393.0380 290.0513 496.0247 ## 12 12 369.8592 261.5774 478.1409 ## 13 13 352.9104 239.0015 466.8192 ## 14 14 346.8510 227.3225 466.3794 ## 15 15 350.4399 225.7535 475.1262 ## 16 16 367.2739 238.1157 496.4321 ## 17 17 390.4917 257.6416 523.3417 ## 18 18 415.6232 279.7278 551.5187 ## 19 19 433.2506 294.7538 571.7474 ## 20 20 437.4176 296.4963 578.3390 ## 21 21 429.5152 286.1074 572.9229 ## 22 22 412.5279 266.3558 558.7000 ## 23 23 390.6720 241.3325 540.0115 ## 24 24 368.3594 215.4343 521.2845 Du lagrer den som en Excel-fil slik: write_xlsx(prediksjoner3, &quot;no5_prediksjoner.xlsx&quot;) Du kan laste ned de ferdige Excel-filene her for å kontrollere at du har fått det til: no3_prediksjoner.xlsx og no5_prediksjoner.xlsx. "],["dataving-5.html", "7.5 Dataøving 5", " 7.5 Dataøving 5 7.5.1 Oppgave 1: Interaktiv øvelse Før vi tar fatt på dataanalysens begynner vi som vanlig med litt R-trening i swirl. Har du allerede installert pakken swirl (skriv install.packages(\"swirl\") i konsoll hvis ikke) starter du opp swirl med å skrive følgende i konsollen: library(swirl) swirl() Du vil i starten bli bedt om å skrive inn ditt navn og så følger litt info om hvordan swirl fungerer. Du blir så bedt om å velge kurs. Her skal du først velge alternativet ‘R Programming’. Merk at du kanskje må trykke alternativet ‘No. Let me start something new’ for å komme tilbake til hovedmenyen etter å ha brukt swirl tidligere. Du får så se alle modulene dette kurset inneholder. I denne øvingen skal du prøve deg på modul 9 ‘Functions’. I denne modulen vil du lære litt om funksjoner i R. Merk at det helt til høyre vil står hvor langt du har kommet i prosent. Står du helt fast med et punkt kan du skrive skip() for å hoppe over dette punktet. Når du har fullført en modul blir du spurt om du vil motta ‘credit’ for å ha fullført modulen. Her kan du svare nei. Ønsker du å avbryte underveis skriver du bye(). Skriver du inn det samme navnet når du eventuelt starter swirl igjen kan du fortsette der du slapp. Husk å avslutt swirl (esc) før du begynner på del to av øvingen. Lykke til! 7.5.2 Oppgave 2 - Maskinlæring: Logistisk regresjon og k nærmeste naboer I denne oppgaven skal vi se på de samme dataene som ble brukt i forelesningen om logistisk regresjon. Vi har data på 10000 kredittkortkunder og vi ønsker å kunne bygge og trene en best mulig modell til å predikere hvilke kunder som vil misligholde sin gjeld. Oppgave 2.1: Vi starter med å få tak i dataene. Disse er integrert i pakken ISLR. Last inn pakken og ta en titt på dataene ved bruk av følgende linjer (hvordan du kommenterer er opp til deg): library(ISLR) # Pakke som inneholder dataene head(Default) # Viser starten på dataframen str(Default) # Viser hvilke typer variabler dataframen inneholder Responsvariabelen er : Dette er en kategorisk variabel. Misligholdt kunden gjelden? Forklaringsvariabler: : Kategorisk : Kontinuerlig, størrelsen på gjelden ($) : Kontinuerlig, kundens årlige inntekt ($) Oppgave 2.2: Det neste vi gjør er å visuelt undersøke avhengigheten mellom det å misligholde (default) og hvor stor gjeld (balance) kunden har . Vanligvis når vi visuelt skal inspisere sammenhengen mellom to variabler lager vi et spredningsplott. Men når den ene variabelen er kategorisk er det mer informativt å sammenligne to boksplott av den kontinuerlige variabelen for hver av gruppene den kategoriske variabelen representer. Dette kan gjøres på følgende måte: boxplot(balance ~ default, data = Default, ylab = &quot;balance&quot;, xlab = &quot;default&quot;) Her er det formelen balance ~ default som gjør at boxplot() funksjonen lager to boksplott av balance; et for gruppen som misligholdt (“Yes”) og et for gruppen som ikke misligholdt (“No”). Reflekter over figuren og gjør deg opp en mening om sammenhengen mellom default og balance. Oppgave 2.3: Det er lurt å dele inn dataene i et treningssett og et testsett når vi driver med maskinlæring. Treningsettet bruker vi til å tilpasse (trene/lære) modellen, mens testsettet bruker vi til å se hvor godt forskjellige modeller presterer. Dette kan gjøres på flere måter, men vi velger her å bruke pakken dplyr som ble beskrevet i siste del av datalabb 2. Først legger vi til en unik id til hver kunde. Vi lar id-nummeret være lik radnummeret til kunden og til dette bruker vi funksjonen mutate: library(dplyr) my_data &lt;- Default %&gt;% mutate(id = row_number()) Siden vi nå skal trekke et utvalg av dataene våre kan det være lurt å sikre at resultatet er reprodusibelt ved å sette set.seed(123) foran koden som følger. Du kan gjerne velge et annet tall enn 123, men når en gjør dette i forkant av en tilfeldig trekning i R er trekningen bestemt. Så trekker vi et treningssett bestående av 70 % av dataene ved bruk av funksjonen sample_frac: train &lt;- my_data %&gt;% sample_frac(.70) De resterende kundene bruker vi som testsett ved bruk av funksjonen anti_join: test &lt;- my_data %&gt;% # Treningssettet er da de resterende 30 % av dataene anti_join(train, by = &#39;id&#39;) Koden over trekker ut alle kunder som ikke har lik id som i treningssettet som derfor svarer til de resterende 30 % av dataene. Oppgave 2.4: Vi forklarer i denne oppgaven hvordan en logistisk regresjonsmodell kan estimeres, tolkes og brukes. Du vil måtte lage nye modeller med tilsvarende koder i oppgavene som følger. Vi lager en modell hvor vi bruker variabelen balance (gjeld) som forklaringsvariabel. Vi bruker da funksjonen glm: model1 &lt;- glm(default ~ balance, data = train, family = &quot;binomial&quot;) Syntaksen til glm-funksjonen er veldig lik den vi bruker i regresjon (lm-funksjonen) bortsett fra at vi må spesifisere argumentet family = \"binomial\" for at å fortelle R at vi ønsker å gjøre en logistisk regresjon. Merk at vi bruker treningssettet til å estimere (trene) modellen ved å spesifisere argumentet data = train. Det kan være lurt å se om forklaringsvariabelen balance har en signifikant effekt på default ved å bruke summary funksjonen: summary(model1) For å tolke hvilken effekt balance (gjeld) har på default (mislighold) er det lurt å regne ut hva effekt en økning på 1 $ i balance har på oddsen for default (Se forelesning): exp(coef(model1)) ## (Intercept) balance ## 2.205746e-05 1.005567e+00 Vi ser at oddsen for default øker med en faktor 1.0056 (en 0.56 % økning) dersom balance øker med 1 $. Si at du ønsker å predikere sannsynligheten for hvorvidt to kunder med henholdsvis 1000 $ og 2000 $ i balance vil misligholde sitt lån. Da kan vi bruke predict2 på følgende måte: to_personer &lt;- data.frame(balance = c(1000, 2000)) pred &lt;- predict(model1, newdata = to_personer, type = &quot;response&quot;) pred ## 1 2 ## 0.005648303 0.593966756 Det første argumentet i funksjonen predict er hvilken modell vi skal bruke i prediksjonen (model1). Det andre argumentet newdata er hvilke kunder vi ønsker å predikere misligholdsannsynligheter for. Vi setter dette argumentet til data.frame’n vi har kalt to_personer hvor hver rad svarer til en kunde med et sett forklaringsvariabler (i dette tilfellet to kunder og derfor to rader). Det er viktig at den inneholder en (eller flere) kolonne(r) med kolonnenavn som svarer til navnet til forklaringsvariabelen(e) vi har brukt i modellen. Argumentet type = \"response\" gjør at vi får returnert sannsynligheten for mislighold og ikke bare verdien av det lineære leddet i modellen. Hvis vi ut fra disse sannsynlighetene ønsker en klassifiseringsregel som klassifiserer om kunden vil misliholde eller ikke (“Yes/No”) er det naturlig å tildele kunden “Yes” hvis misligholdsannsynligheten overstiger en hvis grense og “No” hvis ikke. Det kan det tenkes at kredittgiver vil være enten konservativ (sette grensen lavt, si 0.3) eller liberal (sette grensen høyt, si 0.7), men i eksempelet under bruker vi en “nøytral” grense på 0.5: ifelse(pred &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) ## 1 2 ## &quot;No&quot; &quot;Yes&quot; Som navnet tilsier, vurderer ifelse funksjonen en logisk test i første argumentet (pred &gt; 0.5 hvor pred er misligholdsannsynlighetene vi predikerte) og hvis testen har verdi TRUE gir den ut det du skriver i det andre argumentet (\"Yes\"), og det tredje argumentet (\"No\") ellers. Vi ser at individet med 1000 $ i gjeld blir klassifisert som “No”, mens individet med 2000 $ i gjeld blir klassifisert som “Yes”. Oppgave 2.5: Lag en ny modell ved navn model2 hvor du bruker den kategoriske variabelen student som forklaringsvariabel. Hvilken effekt har det å være student på for oddsen for mislighold? Prediker sannsynligheten for at en student og en ikke-student misligholder gjelden sin. Oppgave 2.6: Lag en tredje model ved navn model3 hvor du bruker alle forklaringsvariablene. Hvilken effekt har det å være student på for oddsen for mislighold nå? Sammenlign med forrige oppgave. Undersøk visuelt om det er en sammenheng mellom student og balance med å lage et boxplot over balance for studenter og et for ikke-studenter (hint: se Oppgave 2.2). Prediker sannsynligheten for mislighold for en ikke-student og en student med lik balance og income på henholdsvis 1500 $ og 10000 $. Sammenlign med prediksjonen du gjorde i Oppgav 2.4. Basert på model2 og model3, hvordan skal kredittgiver forholde seg til en student versus en ikke-student dersom a) Ingen informasjon om balance eller income er oppgitt og b) dersom en vet balance og income? Oppgave 2.7: I denne oppgaven skal vi trene opp en knn (k-nærmeste-naboer) modell til å gjøre en tilsvarende klassifisering som den logistiske regresjonsmodellen gjorde over. Funksjonen train som vi trenger er inneholdt i pakken caret (som må installeres ved hjelp av install.packages(\"caret\")). Vi velger å tilpasse en modell hvor antall naboer “k” velges automatisk med kryssvalidering vi argumentet “`trControl”: library(caret) # R-kode dersom vi vil velge k automatisk set.seed(200) trControl &lt;- trainControl(method = &quot;cv&quot;, # 5-fold kryssvalidering number = 5) # Tilpasser modellen model4 &lt;- train(default ~ balance + income + student, data = train, method = &quot;knn&quot;, trControl = trControl, metric = &quot;Accuracy&quot;) Vi kan sjekke hvilken k som ble valgt på følgende måte (siden kryssvalidering bruker tilfeldige trekninger kan resultatet bli noe foreskjellig fra gang til gang, selv om datasettet er det samme): # Hvilken k valgte kryssvalideringen? k &lt;- model4$finalModel$k k ## [1] 5 Som for de andre modellene bruker vi funksjonen predict når vi skal predikere og syntaksen er helt lik: to_kunder &lt;- data.frame(balance = c(1000, 2000), income = 10000, student = c(&quot;Yes&quot;, &quot;Yes&quot;)) predict(model4, newdata = to_kunder) ## [1] No Yes ## Levels: No Yes Merk at i motsetning til de logistiske regresjonsmodellene som predikerte sannsynligheter klassifiserer knn modellen kundene direkte som “Yes”/“No”. Oppgave 2.8: Vi ønsker å vurdere hvilken av model3 (logistisk regresjon) og model4 (knn) som er best. Vi kan da sjekke hvor godt de klarer å klassifisere testsettet vårt hvor vi vet hvem som har misligholdt lånene sine. Vi starter med å hente ut de sanne verdiene av default i treningssettet: sann &lt;- test$default # Den sanne verdien av default i testdataene Disse skal vi så sammenligne med hvordan modellene klassifiserer de samme kundene basert på de andre variablene. Vi gjør først klassifiseringen med den logistiske regresjonsmodellen: pred_logreg &lt;- predict(model3, newdata = test, type = &quot;response&quot;) # Predikert sannsynlighet klass_logreg &lt;- ifelse(pred_logreg &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) # Klassifisering av kundene En oversikt over hvor mange riktige/feil klassifiseringer modellen gjør kan lett oppsummeres med en kontigenstabell. For å lage en kontigenstabell i R bruker vi funksjonen table: logreg_tab &lt;- table(sann, klass_logreg) # Kontigenstabell logreg_tab ## klass_logreg ## sann No Yes ## No 2889 11 ## Yes 70 30 Her kan vi f.eks se at 2889 kunder blir riktig klassifisert som “No”, dvs at de ikke misligholdt lånet og modellen spår at de ikke vil misligholde lånet. Merk at diagonalen (2889 og 30) representerer korrekte klassifiseringer, mens av-diagonal (11 og 70) representer feil klassifiseringer. Det kan være en fordel å dele kontigenstabellen over med totalt antall kunder for å få andelel i stedet. Funksjonen prop.table gjør nettopp dette: logreg_tab_norm &lt;- logreg_tab %&gt;% prop.table %&gt;% # normaliser round(3) # rund av til 3 desimaler logreg_tab_norm ## klass_logreg ## sann No Yes ## No 0.963 0.004 ## Yes 0.023 0.010 Summen av diagonalen på denne tabellen (0.963 og 0.01) gir da totalt andel korrekt klassifiseringer: logreg_tot &lt;- sum(diag(logreg_tab_norm)) # Total andel korrekt klassfisering logreg_tot ## [1] 0.973 Oppgave 2.9: Gjør en tilsvarende klassifisering av kundene i testsettet med knn modellen model4 og sammenlign med resultatet over. Hvilken modell foretrekker du? Hvilke egenskaper ved klassifisering tror du kredittgiver vektlegger? Funksjonen predict er satt opp med litt forskjellige argumenter alt ettersom hvilken type modell vi bruker. Du kan lese dokumentasjonen ?predict.glm for å se hvordan den er satt opp for glm objekter↩︎ "],["seminar.html", " 8 Seminaroppgaver", " 8 Seminaroppgaver Her finner dere oppgavesettene som vi skal regne på oppgaveseminarene som vi i all hovedsak skal bruke torsdagstimene på. "],["seminar-1---grunnleggende-statistikk.html", "8.1 Seminar 1 - Grunnleggende statistikk", " 8.1 Seminar 1 - Grunnleggende statistikk (V20, OPPG 1) Grafen ble publisert av klima- og miljøminister Ola Elvestuen fra Venstre på Twitter 1. november 2019 (men senere tatt bort), og viser norske CO\\(_2\\)-utslipp (i 1000 tonn CO\\(_2\\)-ekvivalenter) som funksjon av tid. Venstre gikk inn i regjering sammen med Høyre og Fremskrittspartiet i januar 2018. Denne figuren ble kritisert for å være misvisende. På hvilken måte er den det, og hvordan ville du forandret den for at den skulle være mindre misvisende? (V20, OPPG 1) Figuren under ble publisert av Høyre på Facebook 1. november 2019, og viser norske CO\\(_2\\)-utslipp (i 1000 tonn CO\\(_2\\)-ekvivalenter) som funksjon av tid. Denne figuren ble også kritisert for å være misvisende. På hvilken måte er den det, og hvordan ville du forandret den for at den skulle blitt mindre misvisende? (V19, OPPG 3) I figuren under ser vi en graf over den norske styringsrenten siden 2013 og anslag over hvilken bane renten skal følge de neste tre årene med fire usikkerhetsintervaller. Bruk figuren til å anslå sannsynligheten for negativ styringsrente ved utgangen av 2022. Florida innførte i 2005 en såkalt “Stand Your Ground”-lov, som i større grad tillater folk å bruke dødelig makt i selvforsvar. Grafen under, som viser utviklingen av dødsoffer i skyteepisoder i Florida, illustrerte mange nyhetsreportasjer etter et drap i 2012, men fikk skarp kritikk i ettertid. Hvorfor det tror du? La oss anta at 50 avgangsstudenter som gikk ut av NHHs masterprogram i fjor svarte på et spørreskjema hvor mye de har i startlønn i sin første jobb. Gjennomsnittslønnen var 450.000, med et standardavvik på 120.000. Anta videre at lønnsfordelingen for årets studenter er den samme (og se bort fra inflasjon). Estimer sannsynligheten for at gjennomsnittlig startlønn for 50 responenter i år vil være større enn 450.000. Estimer sannsynligheten for at gjennomsnittlig startlønn for 50 responenter i år vil være større enn 500.000. La \\(X\\) være en stokastisk variabel med forventningsverdi lik 5 og varians lik 2. Definer to nye stokastiske variabler \\(Y = 2X-1\\) og \\(Z = X^2 + Y\\). Regn ut: E\\((Y)\\) og Var\\((Y)\\). E\\((Z)\\) og Var\\((Z)\\). La \\(X\\) være en standard normalfordelt variabel. Du får oppgitt at det medfører at E\\((X^3) = 0\\). La \\(Y = X^2\\). Vis at korrelasjonen mellom \\(X\\) og \\(Y\\) er lik null. Er \\(X\\) og \\(Y\\) uavhengige? "],["seminar-2---hypotesetesting.html", "8.2 Seminar 2 - Hypotesetesting", " 8.2 Seminar 2 - Hypotesetesting Sett opp en generell regel/oppskrift som du kan følge alle gangene du skal gjennomføre en hypotesetest. Tegn opp en figur som viser hvordan de samme verdiene av \\(\\overline{X}\\) og \\(\\mu\\) i to ett-utvalgs \\(t\\)-tester likevel kan føre til motsatt konklusjon av testen. Det er desverre et stort problem at folk som bruker begrepet p-verdi i ulike sammenhenfer ofte ikke forstår hva begrepet betyr. I den sammenheng publiserte The American Statistical Association i 2016 et skriv som i klare ordelag beskriver problemet. Les gjennom dette skrivet, og spesiel avsnitt 3 “Principles”. Oversett hver av disse seks overskriftene (prinsippene) til norsk med dine egne ord, og skriv så en setning, igjen med egne ord, om hvordan du forstår hvert av punktene. Prøv å definer begrepet p-verdi med så enkle ord som du klarer. Gjør oppgave 11.38 i læreboken: The club professional at a difficult public course boasts that his course is so tough that the average golfer loses a dozen or more golf balls during a round of golf. A dubious golfer sets out to show that the pro is fibbing. He asks a random sample of 15 golfers who just completed their rounds to report the number of golf balls they lost. Assuming that the number of golf balls lost is normally distributed with a standard deviation of 3, can we infer at the 10% significance level that the average (rett ord her er vel egentlig “expected”) number of golf balls lost is less than 12? Observasjonene er: 1, 14, 8, 15, 17, 10, 12, 6, 14, 21, 15, 9, 11, 4, 8 Anta at det sanne forventede antall golfballer som forsvinner i forrige oppgave er 10, hva er da styrken (power) til testen som du gjorde der? Hva er tolkningen til dette tallet? Gjør skoleeksamen V17, oppgave 1a og 1b. Gjør skoleeksamen V19, oppgave 1b. "],["seminar-3---regresjon-i.html", "8.3 Seminar 3 - Regresjon I", " 8.3 Seminar 3 - Regresjon I Se på den estimerte regresjonskurven under: Bestem hvilke av de følgende parameterestimatene som kan være riktig: \\(\\hat{\\beta}_0 = 5\\), \\(\\hat{\\beta}_1 = -3\\) \\(\\hat{\\beta}_0 = 10\\), \\(\\hat{\\beta}_1 = 4\\) \\(\\hat{\\beta}_0 = -4\\), \\(\\hat{\\beta}_1 = 2\\) \\(\\hat{\\beta}_0 = 10\\), \\(\\hat{\\beta}_1 = -2\\) Bestem hvilke av de følgende utsagn om den estimerte korrelasjonen mellom \\(X\\) og \\(Y\\) og andel forklart variasjon (\\(R^2\\)) som er riktig: Korrelasjonen er \\(-0.94\\) andel forklart variasjon er \\(-0.88\\). Korrelasjonen er \\(0.94\\) andel forklart variasjon er \\(0.88\\). Korrelasjonen er \\(-0.94\\) andel forklart variasjon er \\(0.88\\). Korrelasjonen er \\(1\\) andel forklart variasjon er \\(0.88\\). Under ser du et residualplott og et QQ-plot for en regresjonsanalyse. Hvilke brudd på antagelsene for lineær regresjon ser du? Dersom alle andre kriterier er oppfylt, hvilke konsekvenser har disse bruddene? En student ønsker å undersøke sammenhengen mellom pris og størrelsen på leiligheter (m^2). Han bestemmer seg for å samle inn data på solgte leiligheter ukentlig over et helt år. Hva kan være problematisk for en slik strategi? Anta \\(Y_i\\) er forandring i BNP fra kvartal \\(i-1\\) til kvartal \\(i\\), mens \\(x_i\\) er forandringen i arbeidsledighet fra kvartal \\(i\\) til kvartal \\(i-1\\). Du tilpasser så regresjonsmodellen \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\end{equation}\\] Forklar hva forskjellen på et prediksjonsintervall og konfidensintervall er i denne sammenhengen dersom \\(x=0\\), og hvorfor begge typer intervall er interessante. Eksamen Vår 2019, Oppgave 2d) og e). "],["seminar-4---regresjon-ii.html", "8.4 Seminar 4 - Regresjon II", " 8.4 Seminar 4 - Regresjon II Vi har lyst til å forstå forskjellen på den statistiske modellen \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\) der \\(\\epsilon\\) er normalfordelt med forventningsverdi 0 og varians \\(\\sigma^2\\), og det vi faktisk observerer, som er par av observasjoner \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\). Klarer du å illustrere denne forskjellen ved å fortsette på tegningen under? Her har vi satt \\(n=4\\). Et lite firma som selger babyutstyr ser god effekt av å markedsføre produktene sine på Youtube. For å analysere denne sammenhengen nøyere samler de inn historiske data på hvor mye de har brukt på Youtube-reklame (youtube) i løpet av en måned, og omsetningen den måneden (sales), og gjennomfører så en enkel lineær regresjon med youtube som forklaringsvariabel og sales som responsvariabel. Gi en kort fortolkning av regresjonsutskriften under: ## ## =============================================== ## Dependent variable: ## --------------------------- ## sales ## ----------------------------------------------- ## youtube 0.048*** ## (0.003) ## ## Constant 8.439*** ## (0.549) ## ## ----------------------------------------------- ## Observations 200 ## R2 0.612 ## Adjusted R2 0.610 ## Residual Std. Error 3.910 (df = 198) ## F Statistic 312.145*** (df = 1; 198) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 I figuren under har vi plottet inn datasettet, den estimerte regresjonslinjen (rød heltrukken strek), samt linjen \\(y = x\\) (sort stiplet linje). Hvordan skal babyutstyrforhandleren tolke denne figuren? Ved hjelp av den estimerte regresjonsmodellen lager babyutstyrforhandleren følgende plott av residualene mot de predikerte verdiene for hver observasjon i datasettet. Hva sier denne figuren oss om lineær regresjon som modell for sammenhengen mellom penger brukt på markedsføring og omsetning? Det er gjort mange forsøk på å forklare hvorfor noen land er rike, mens andre land er fattige, se f.eks hjemmeeksamen i MET4, V17. En mulig forklaring kan ligge in landenes fysiske og geografiske egenskaper. For eksempel, kan det hende at land som har ulendt terreng kan ha større vanskeligheter med å bygge infrastruktur og frakte varer enn land som er helt flate, og dermed ende opp som fattigere av den grunn? Vi ser på et lite datasett (lånt fra boken Statistical Rethinking) der vi har et mål på landenes rikdom (logaritmen av BNP, log_gdp), samt et mål på hvor ulendt terrenget er i det landet (rugged). Vi har også mer informasjon om landene, for eksempel om det ligger i Afrika eller ikke (cont_africa). Vi har gjort tre regresjonsanalyser i R, med følgende utskrifter: ## ## ======================================================================================= ## Dependent variable: ## ------------------------------------------------------------------- ## log_gdp ## (1) (2) (3) ## --------------------------------------------------------------------------------------- ## rugged 0.003 -0.067 -0.203*** ## (0.077) (0.064) (0.077) ## ## cont_africa -1.469*** -1.948*** ## (0.165) (0.227) ## ## rugged:cont_africa 0.393*** ## (0.132) ## ## Constant 8.513*** 9.030*** 9.223*** ## (0.136) (0.127) (0.140) ## ## --------------------------------------------------------------------------------------- ## Observations 170 170 170 ## R2 0.00001 0.322 0.357 ## Adjusted R2 -0.006 0.314 0.345 ## Residual Std. Error 1.170 (df = 168) 0.966 (df = 167) 0.944 (df = 166) ## F Statistic 0.001 (df = 1; 168) 39.715*** (df = 2; 167) 30.712*** (df = 3; 166) ## ======================================================================================= ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Skriv opp de tre estimerte regresjonsmodellene. Gi en kort fortolkning av modell (2). Gi en forklaring på hva vi lærer ved å gå fra modell (2) til modell (3). Fokuser spesielt på rollen til interaksjonsleddet (rugged:cont_africa). Hvordan kan det ha seg at rugged-variabelen nå plutselig er statistisk signifikant forskjelig fra null? Kan du tenke deg en praktisk fortolkning av disse resultatene? "],["seminar-5---tidsrekker.html", "8.5 Seminar 5 - Tidsrekker", " 8.5 Seminar 5 - Tidsrekker Vi skal jobbe oss gjennom en serie tidligere eksamensoppgaver om tidsrekker: V17, oppgave 1g V18, oppgave 2a–2b H19, opppgave m–n V20, oppgave 3 Oppgaveformuleringene finner du i seksjon 9.1. "],["seminar-6---avansert-regresjon-og-maskinlring.html", "8.6 Seminar 6 - Avansert regresjon og maskinlæring", " 8.6 Seminar 6 - Avansert regresjon og maskinlæring Nevn minst to grunner til at en ønsker å utføre en vanlig regresjonsanalyse. Reflekter så over hva hovedgrunnen er med å lage henholdsvis en KNN-modell og en regresjonsmodell. Diskuter i hvilken grad det er rimelig med komponenten \\(v_t\\) i en paneldatamodell \\[y_{it} = \\beta_0 + \\beta_1 x_{it} + ... + v_t + \\alpha_i + \\epsilon_{it} \\] dersom en skal analysere paneldata av følgende responsvariabler \\(y_{it}\\): Antall konkurser hver måned i ulike land. Timentlig energi-etterspørsel i norske kommuner. Lønn per år for forskjellige individer i et land. Kan du komme på en tidsinvariant forklaringsvariabel som er relevant for responsvariablene over? Gjør det noe om vi “glemmer” disse? Tegn et sett med observasjoner bestående av en dummy-variabel \\(Y\\) og en kontinuerlig variabel \\(X\\) i et xy-koordinatsystemet hvor en ville fått bedre prediksjoner av \\(Y\\) med KNN-metoden enn med logistisk-regresjon. Prøv deg på eksamen H21 oppgave 3 Oppgaveformuleringene finner du i seksjon 9.1. "],["eksamensoppgaver.html", " 9 Eksamensoppgaver", " 9 Eksamensoppgaver Her finner du tidligere eksamensoppgaver samt diverse (og lett redigert) korrespondanse mellom studenter og studentassistenter som kan være nyttig for senere årganger av studenter. "],["skoleeksamen.html", "9.1 Tidligere skoleeksamener", " 9.1 Tidligere skoleeksamener I tabellen under finner du en lang rekke eksamenssett som er gitt tidligere i MET4. Vi har lagt til en kolonne med oppgaver som åpenbart ligger utenfor pensum i dag. Utover det er selvsagt oppgavene relevante i større eller mindre grad for kurset i dag, uten at vi har mulighet til å gå nærmere inn på en slik detaljert rangering. Lenger nede på siden finner du noen kommentarer og rettelser til oppgavesett og løsningsforslag som har dukket opp i ettertid. Semester Oppgaver Løsningsforslag Ikke lenger pensum per 2025 Data 2017 - Vår Oppgaveformulering Løsningsforslag 1c 2017 - Høst Oppgaveformulering Løsningsforslag 1f-1h 2018 - Vår Oppgaveformulering Løsningsforslag 2018 - Høst Oppgaveformulering Løsningsforslag 1d, 1g 2019 - Vår Oppgaveformulering Løsningsforslag 1c 2019 - Høst Oppgaveformulering Løsningsforslag 2020 - Vår Oppgaveformulering Løsningsforslag 2020 - Høst Oppgaveformulering Løsningsforslag 2021 - Vår Oppgaveformulering Løsningsforslag 2021 - Høst Oppgaveformulering Løsningsforslag 2022 - Vår Oppgaveformulering Løsningsforslag 2022 - Høst Oppgaveformulering Løsningsforslag 2023 - Vår Oppgaveformulering Løsningsforslag 2023 - Høst Oppgaveformulering Løsningsforslag 2024 - Vår Oppgaveformulering Løsningsforslag 2024 - Høst Oppgaveformulering Løsningsforslag 2025 - Vår Oppgaveformulering Løsningsforslag Materiale Rettelser og kommentarer til oppgaver og løsningsforslag Her vil du finne en del kommentarer og rettelser som er spesifikke for de enkelte oppgavesettene. Dette er basert på et dokument som Jarle Møen startet på i sin tid, og som senere er oppdatert av Benjamin Narum. Curipod tok over for denne oversikten, så man finner andre spørsmål og svar der som også er mer nylig. Søk eller bruk “tags” for årgangene av oppgaver. Vi lar spørsmålene under stå i tilfelle de også er av nytte. Overskriftene under kan trykkes på, så ekspanderes innholdet for hver eksamen. Vår 2017 Oppgave 2c: Eksamen V17, oppgave 2c): Man blir bedt om å regne ut om koeffisienten er signifikant, hvorfor er frihetsgraden 282 (\\(v = n - k - 1\\)) og ikke 286 (\\(n-2\\))? Den generelle regelen for lineær regresjon er at man trekker fra antall frihetsgrader som svarer til antall parametre man har estimert først, som så brukes i beregningen av standardavviket. I oppgave 2c på analyse (1) er det brukt 6 estimerte parametre (5 koeffisienter og ett konstantledd) dermed blir det \\(n - 5 - 1\\). I sliden du viser er det en koeffisient og ett konstantledd, dermed \\(n - 2\\). Høst 2017 Oppgave 1b: “Viser resultatet av testen at studentene i gruppe A kanskje har plagiert?”. F-testen vil jo kun teste om variansene er den samme for gruppe A og gruppe B, eller ulik? Hvordan kan vi svare på dette spørsmålet ved hjelp av en F-test? I fasiten står det at studentene i gruppe A har kanskje plagiert. Er dette fordi variansen i gruppe A er minst, dermed har de mer like respons, noe som kan indikere plagiat? Ja, oppgave a og b henger tett sammen. Det stemmer at F-testen bare kan si om de er ulik. Man må tolke hva ulike standardavvik må bety utfra konteksten av “eksperimentet” for å komme frem til at det er juks på gang. Det stemmer som du sier at lavt standardavvik betyr plagiat. Poenget her er at gruppe B skal være representativt for studenter generelt, så ettersom standardavviket er signifikant forskjellig fra det det skulle vært (statistisk lik gruppe B) er det noe som foregår. Vår 2018 Oppgave 1b: I fasiten står det at man også kan bruke T-test for å sammenligne forventningsverdiene i de to datasettene, er det fordi n er stor og dermed vil normal og t-fordeling være omtrent det lik? Vi kan gjøre to forskjellige argumenter når \\(n\\) er stor i dette tilfellet: Når \\(n\\) er stor er det ikke så farlig med normalitetsantakelsen for observasjonene, siden sentralgrenseteoremet sørger for at testobservatoren er tilnærmet normalfordelt uansett, og da kan vi bruke Z-test eller t-test avhengig av om vi kjenner det/de sanne standardavviket/standardavvikene eller ikke. I tillegg ser vi at når \\(n\\) blir stor så er de kritiske verdiene for hypotesetesten nesten like. Det følger av at vi da kan estimere standardavviket mer presist, så de empiriske standandardavvikene \\(s\\) (eventuelt (\\(s_1\\) og \\(s_2\\)) er nærme de sanne verdiene \\(\\sigma\\) (eventuelt \\(\\sigma_1\\) og \\(\\sigma_2\\)). Høst 2018 Oppgave 1c: I oppgaven skal man se hvorvidt “… the trimmed mean of the offers is smaller in 2008 than in 2006.” De setter opp H0 = Mean(Libor2006) = Mean(Libor2008), men så setter de opp H1 : Mean(Libor2006) &lt; Mean(Libor2008). Burde det ikke være omvendt her ettersom vi skal se om gjennomsnittet er lavere i 2008 enn i 2006? altså at H1: Mean(Libor2006) &gt; Mean(Libor2008). Videre konkluderer man med at “The test indicates that the Libor in 2006 is lower than the Libor in 2008”. Men i summary tabellen får man oppgitt at gjennomsnittlig libor i 2006 = 3.09 og 2.00 i 2008. Hvordan kan det ha seg da at man konkluderer med at libor er lavere i 2006 enn i 2008? De har snudd ulikheten. Det er feil i fasiten og det skal egentlig være Mean(Libor2006) &gt; Mean(Libor2008). I konklusjonen skal det følgelig konkluderes med at Libor i 2006 er større enn i 2018. Høst 2019 Oppgave 1a: Oppgaven spør om hvilke tester man kan utføre for å finne ut hvilket land som har den signifikant største andelen kjempelykkelige land. Er det ikke z-test man da bruker? Videre blir vi bedt om å gi et datatransformasjon for Norge, hva mener de med dette? Hvorfor har fasiten kun brukt det som står under brøkstreken i testobservatoren? Det stemmer at de bruker en z-test i fasit, men det er også mulig å bruke en chi-squared goodness-of-fit test. Poenget her er at man skal finne ut hvem som er mest lykkelig i forhold til hverandre og dermed må man teste to og to land mot hverandre ved bruk av flere z-tester. Tanken med “transformasjonen” er at man tar tallene fra tabellen og beregner noe man enkelt kan sammenligne to og to land basert på, det blir da estimatet av “sannsynlighet for kjempelykkelig” med et empiriske standardavvik. Det empiriske standardavviket er da det som står i nevneren for testobservatoren (se kap 12-3c i boken). Siden testen i teorien må gjennomføres 3 ganger er det bedre å regne p og empirisk standardavvik for så å sette disse tallene inn i uttrykket for testobservatoren etterpå. Oppgave 1b: Etter at \\(z = 5.33\\) er regnet ut dukker det opp en \\(z = 0.11\\) under. Hvor kommer denne fra og hva forklarer den? Den blir ikke kommentert videre. I oppgave B har de først regnet ut z dersom man antar pooled standardavvik, deretter har de beregnet z med hvert sitt standardavvik og testet om disse to z-verdiene er ulike. Det er en litt knotete måte å gjøre det på. De kunne bare brukt den sistnevnte z og testet den ulik null heller enn lik førstnevnte z. Konklusjonen blir den samme. Se side 482, Case 2. I eksemplene i boken beregner de ikke D først slik de har gjort i fasit, men sier at den er kjent. Oppgave 1j: Vi blir bedt om å skissere regresjonslinjen for råalders samlede påvirkning på lykkenivået. Jeg forstod fasiten, men dersom oppgaven hadde bedt om å skissere regresjonslijen for rettferdighet samlede påvirkning på rettferdighet, hadde linjen bare vært lineær med en stigningstall på 0.03? Måtte vi gjøre alle de beregningene igjen siden variabelen var logaritmen til alder? Det stemmer. Rettferdighet-til-lykke-forholdet er linært, så det hadde bare blitt en linje. Her spørres det om en skisse nettopp fordi forholdet skal være ikke-lineært, så du ville nok ikke fått samme spørsmål for “rettferdighet” med mindre begge skulle inn i figuren samtidig som sammenligning. Beregningene var for å støtte at plottet ble riktig. I det lineære tilfellet kunne du jo bare ha funnet to punkter og dratt en linje imellom, det går jo ikke i det ikke-lineære tilfellet. Vår 2020 Oppgave 3c: Jeg er litt usikker ifm oppgave c) hvor vi blir spurt om redisualserien er stasjonær. Jeg forstår resonnementet i fasiten, men er litt usikker når det kommer til denne figuren. Her ser det jo ut til at variansen øker med tiden? Et av forutsetningene for stasjonær tidsrekke er jo at variansen skal være konstant og uavhengig av t? Hvordan ser vi forresten om forventningen er konstant eller ikke? Visuell inspeksjon kan gi deg en del innsikt, men noen ganger kan det være vanskelig å konkludere eksakt kun fra figuren. Om det er tvil må man støtte seg videre på beregninger. For residualserien er den litt kort til å konkludere bare fra plottet om variansen øker eller ikke. De laveste residualverdiene (på bunnen av plottet) ser ikke ut til å endre seg slik som det kanskje kan se ut for de høyeste (på toppen av plottet). Om du hadde plottet denne over lenger tid ville du kanskje ikke lenger tenkt at variansen endrer seg. Forventningen er konstant, og lik 0, om det er ca like mange punkter over som under 0-linjen over tid. Det ser ut til å være tilfelle her. "],["tidligere-hjemmeeksamen.html", "9.2 Tidligere hjemmeeksamen", " 9.2 Tidligere hjemmeeksamen Her finner du oppgavene som er gitt ved hjemmeeksamen etter at eksamensformen ble lagt om i 2017. Merk at besvarelsen skal leveres som en sammenhengende rapport, og at løsningsforslagene under ikke oppfyller det kravet, men heller er en skisse av kodesnutter som kan brukes til å besvare spørsmålene. For vår 2018 har vi et sett med eksempelbesvarelser på ulike karakternivåer, med sensors kommentarer. Semester Oppgaver Løsningsforslag Data, relevante artikler, etc. 2017 - Vår Oppgaveformulering Ikke tilgjengelig Materiale 2017 - Høst Oppgaveformulering Løsningsforslag Materiale 2018 - Vår Oppgaveformulering Løsningsforslag Materiale, Eksempelbesvarelser 2018 - Høst Oppgaveformulering Løsningsforslag Materiale 2019 - Vår Oppgaveformulering Løsningsforslag Materiale 2019 - Høst Oppgaveformulering Løsningsforslag Materiale 2020 - Vår Oppgaveformulering Løsningsforslag Materiale 2020 - Høst Oppgaveformulering Løsningsforslag Materiale 2021 - Vår Oppgaveformulering Løsningsforslag Data ikke lenger tilgjengelig. 2021 - Høst Oppgaveformulering Løsningsforslag Materiale 2022 - Vår Oppgaveformulering Løsningsforslag Materiale 2022 - Høst Oppgaveformulering Løsningsforslag Materiale 2023 - Vår Oppgaveformulering Løsningsforslag Materiale 2023 - Høst Oppgaveformulering Løsningsforslag Materiale 2024 - Vår Oppgaveformulering Løsningsforslag Materiale "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
