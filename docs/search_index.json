[["regresjon.html", " 4 Regresjon", " 4 Regresjon I forrige modul fokuserte vi på binære spørsmål av typen Er det en forskjell mellom disse to populasjonene, eller ikke?, Er disse kjennetegnene uavhengige, eller ikke?, og så videre. I denne modulen skal vi prøve å gå et steg lenger og tillate mer interessante spørsmål. I stedet for bare å spørre om en eller annen effekt er til stede (eller ikke), så vil vi heller finne ut hvor stor denne effekten er, hvilken retning den går, og kanskje om vi kan bruke kunnskapen vi får om statistiske sammenhenger til å si noe fornuftig om hva som vil skje for noe som vi enda ikke har observert. Da er det regresjon som gjelder, og mer spesifikt for vår del: lineær regresjon. Regresjon er et hovedtema i MET4. Vi innfører en statistisk modell som i sin enkleste form sier at en forklaringsvariabel \\(X\\) henger sammen med en responsvariabel \\(Y\\) på en helt bestemt måte, nemlig gjennom ligningen \\[Y = \\beta_0 + \\beta_1 X + \\epsilon.\\] Ligningen over sier at det er en lineær sammenheng mellom \\(X\\) og \\(Y\\), men at det i tillegg kommer en uforutsigbar støyvariabel \\(\\epsilon\\) som gjør at vi ikke vil kunne observere den lineære sammenhengen direkte. Det vi derimot kan gjøre, er å bruke de observerte \\(X\\)er og \\(Y\\)er til å finne ut hvilke verdier av \\(\\beta_0\\) og \\(\\beta_1\\) som passer best. Til det bruker vi minste kvadraters metode, som beskrevet i videoforelesningene i denne modulen. Vi deler arbeidet med regresjon inn i tre deler. I den første (og største) delen går vi grundig gjennom ulike sider vi den enkle lineære regresjonsmodellen over. I den andre delen ser vi på multippel regresjon som er en utvidelse av enkel regresjon der vi tillater flere forklaringsvariabler på høyre side av likhetstegnet, og i den tredje delen ser vi på ulike praktiske aspekter ved regresjonsmodellering og modellbygging. I videoforelesningene går vi gjennom noen slides, og vi skriver et R-skript. Du kan laste disse ned ved å klikke på lenkene under: Slides til Regresjon R-script til Regresjon TIPS: Hvis du ønsker å laste ned lysbildene som PDF trykker du på linken over, velger Skriv ut, og så skriver du ut som PDF. Før du gjør det bør du scrolle gjennom alle sidene slik at ligningene vises korrekt. "],["enkel-regresjon.html", "4.1 Enkel regresjon", " 4.1 Enkel regresjon 4.1.1 Videoforelesninger 4.1.2 Kommentarer Vi har sett på en del figurer som illustrerer noen pedagogiske poenger, og lærebokens kapittel 16 går detaljert til verks når de beskriver de ulike læringsmomentene: I kapittel 16.1 kan vi lese mer om den statistiske modellen som vi kaller enkel regresjon. I kapittel 16.2 introduseres minste kvadraters metode for å estimere regresjonskoeffisientene ved hjelp av data. De viser til og med hvordan det kan gjøres manuelt ved hjelp av bildatasettet, men det er selvsagt kun for å illustrere hvodan formlene ser ut. Vi estimerer ved hjelp av R, og vi har sett i videoforelesningen hvordan vi gjør det ved hjelp av lm()-funksjonen. Det som gjør regresjon til et statistisk problem er feilleddet \\(\\epsilon\\). Vi tenker oss at for en gitt verdi av \\(X\\), så vil «naturen» regne ut verdien av \\(Y\\) ved å regne ut den lineære sammenhengen \\(Y = \\beta_0 + \\beta_1 X\\), og så legge til støyvariabelen \\(\\epsilon\\) som trekkes fra en sannsynlighetsfordeling. Vi kan ikke observere direkte hvilke \\(\\epsilon\\) som «naturen» har «trukket» (for da ville vi med en gang kunne regnet oss frem til verdiene av \\(\\beta_0\\) og \\(\\beta_1\\)). For gitte estimater av regresjonskoeffisientene \\(\\widehat \\beta_0\\) og \\(\\widehat \\beta_1\\) (som vi kan finne f.eks. ved hjelp av minste kvadraters metode), så kan vi regne ut de observerte residualene \\[\\widehat\\epsilon_i = Y_i - \\widehat Y_i = Y_i - (\\widehat \\beta_0 + \\widehat \\beta_1 X_i).\\] Ved å analysere residualene kan vi si mer om f.eks Er det egentlig en lineær sammenheng mellom \\(X\\) og \\(Y\\)? Hvis det er mønstre og sammenhenger i de observerte residualene, tyder det på at den enkle lineære modellen ikke fanger opp hele sammenhengen mellom \\(X\\) og \\(Y\\). Vi kan gå mer spesifikt til verks: nøyaktig hvilke antakelser om residualene er ser ut til å være brutt? I senere økonometrikurs vil dere kunne lære mer om hvordan vi håndterer de ulike problemene. Hvor stor er variansen til \\(\\epsilon\\)? Det brukes videre til å sette opp den viktige signifikanstesten for om stigningstallet i regresjonen er forskjellig fra null. Alt dette behandles grudig i bokens kapittel 16.316.6. Her bør teksten leses godt. Kode til bileksempelet finnes i scriptet som følger med videoforelesningene. Når det gjelder enkel regresjon kan du sjekke om du har fått med deg det vesentligste ved å diskutere følgende spørsmål: Hva er responsvariabelen og hva er forklaringsvariabelen i enkel regresjon? Hva er fortolkningen av de to regresjonskoeffisientene? Hvilket prinsipp er det vi legger til grunn når vi skal bestemme (estimere) verdien av koeffisientene ved hjelp av data? Skriv opp formlene for koeffisientestimatene. Kan du gi en intuitiv fortolkning av disse? Er de rimelige? Kan du ved hjelp av formelen for \\(\\widehat\\beta_1\\) utlede sammenhengen mellom stigningstallet \\(\\beta_1\\) og korrelasjonskoeffisienten* mellom \\(X\\) og \\(Y\\)? Hvilken rolle spiller feilleddet (\\(\\epsilon\\))? Skriv opp de 4 + 1 forutsetningene. Når må den siste være oppfylt? Når kan vi klare oss uten? Hva er testobservatoren når vi tester H\\(_0: \\beta_1 = 0\\)? Kan du holde styr på de fire standardavvikene vi har jobbet med i denne forelesningen? Hva mener vi med å diagnostisere en regresjonsmodell? Hva er \\(R^2\\), og hva måler den? Hva sier \\(R^2\\) ikke noe om? Her er noen grunnleggende ferdigheter fra kapittel 16. Klarer du dette? Bruke til å tilpasse en enkel regresjonsmodell for et datasett? Bruke til å skrive ut oversiktlige regresjonstabeller? Tolke en regresjonsutskrift? Hente ut relevant informasjon etter en slik tilpasning? Bruke informasjon fra regresjonsutskriften til å regne ut antall stjerner for hånd? Lage diagnoseplott i ? Diagnistisere en modell? Identifisere innflytelsesrike observasjoner? "],["multippel-regresjon.html", "4.2 Multippel regresjon", " 4.2 Multippel regresjon 4.2.1 Videoforelesninger 4.2.2 Kommentarer I kapittel 17 utvides regresjonsbegrepet til multippel regresjon, som i prasis betyr at vi kan ha flere enn en forklaringsvariable: \\[Y = \\beta_0 + \\beta_1X_1 + \\cdots \\beta_kX_k + \\epsilon,\\] men utover dette er alle detaljene vi har snakket om de samme. For eksempel: Tolkningen av regresjonskoeffisienten: En endring på en enhet i forklaringsvariabelen \\(X_j\\) henger sammen med \\(\\beta_j\\) enhets endring i responsvariabelen \\(Y\\) (merk at jeg ikke brukker begrepet fører til, vi kan ikke uten videre fortolke sammenhengen som kausal!). Analysen av residualene \\(\\widehat \\epsilon_i = Y_i - \\widehat Y_i\\) er den samme og har samme formål 13 som over. \\(R^2\\) har samme fortolkning. R-kommandoen er den samme, vi bare sette pluss mellom forklaringsvariablene, f.eks reg &lt;- lm(Y ~ X1 + ... + Xk, data = x) I tillegg innfører vi noen nye begreper: Justert \\(R^2\\): Vi viste i forelesningen at vi vil alltid klare å øke \\(R^2\\) ved å legge til forklaringsvariable, selv om de ikke har noe med problemet å gjøre. Derfor innførte vi en justert \\(R^2\\) som tar høyde for nettopp dette, ved å bli større bare dersom den aktuelle forklaringsvariebelen faktisk forklarer en reell mengde av variasjonen i responsvariabelen. Se avsnitt 17-2f i læreboken. Multikolinearitet: Dersom en forklaringsvariabel er sterkt korrelert med en eller flere andre forklaringsvariabler har vi multikolinearitet. Det blir naturlig nok et problem å skille effekter fra hverandre når de i realiteten er helt eller nesten like. Ekstremtilfellet er perfekt multikolinearitet der en variabel er en eksakt lineær funksjon av en eller flere andre variable. Det typiske tilfellet er at vi har to kolonner der vi måler det samme fenomenet, men med to ulike enheter, f.eks. cm og m. Selvsagt kan vi ikke klare å identifisere en separat og uavhengig effekt av \\(X\\) på \\(Y\\) om vi skifter måleenhet, og vi vil få en feilmelding dersom vi prøver på det. Det er ekvivalent med å dele på null (every time you divide by zero, God kills a kitten!). Løsning: fjern en av kolonnene fra regresjonsanalysen. Verre er det om to variable måler nesten det samme, men ikke helt, som i skoledataeksempelet der vi kunne bruke både innbyggertall og antall femteklassinger i kommunen som forklaringsvariabler. De henger tett sammen, men selvsagt ikke eksakt, og det virker rart å kunne knytte separate efekter til disse to variablene. I dette tilfellet får vi likevel ikke feilmeldinger, men konsekvensen kan fort bli at standardavvikene (usikkerheten!) til koeffisientestimatene eksploderer, og at ingen av variablene blir signifikant forskjellige fra null, selv det det faktisk er en sterk sammenheng mellom kommunestørrelse og prøveresultat (husk at testobservatoren: \\(t = \\widehat \\beta_k/\\sigma_{\\beta_k}\\) blir liten når nevneren blir stor). F-test for multiple sammenligninger: Dette henger nøye sammen med variansanalyse (analysis of variance, ANOVA), som nå er tatt ut av pensum i kurset. For å forstå dette kan vi sette opp et eksempel, med to forklaringsvariabler: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2.\\] Etter å ha brukt miste kvadraters metode for å estimere de tre koeffisientene er vi kanskje interessert i å vurdere den statsistiske signifikansene til de to stigningstallene separat. Da tester vi de to nullhypotesene \\(\\beta_1 = 0\\) og \\(\\beta_2 = 0\\), som vi i praksis gjør ved å se på hvor mange stjerner de får i regresjonsutskriften. Men sett at ingen av koeffisientene er signifikant forskjellige fra null, kan vi da slutte at vi ikke kan forkaste hypotesen \\(\\beta_1 = \\beta_2 = 0\\), dvs at begge koeffisientene er lik null, og at ingen av forklaringsvariablene forklarer variasjon i \\(Y\\)? NEI, det kan vi ikke. Vi kan for eksempel lett tenke oss at vi på grunn av multikolinearitet ikke får separate forkastninger av de to nullhypotesene, men at ved å fjerne en variabel, så blir den andre signifikant. For å virkelig forstå dette problemet kan du godt lese starten på kapittel 14.1 samt kapittel 14.2 om multiple sammenligninger (som strengt tatt ikke er pensum), men essensen er altså: \\[\\textrm{Å forkaste H}_0: \\beta_1 = 0 \\textrm{ og H}_0: \\beta_2 = 0 \\textrm{ er ikke det samme som å forkaste H}_0: \\beta_1 = \\beta_2 = 0!\\] For å gjennomføre den siste testen må vi sette opp en egen testobservator, som viser seg å være \\(F\\)-fordelt. Læreboken lister opp noen detaljer i avsnitt 17-2f, og essensen er at vi setter opp en brøk på formen \\[F = \\frac{\\textrm{Variasjon i } Y \\textrm{ som fanges opp av regresjonsmodellen med } X_1 \\textrm{ og }X_2}{\\textrm{Variasjon i } Y \\textrm{ som fanges opp av regresjonsmodellen uten } X_1 \\textrm{ og }X_2}.\\] Dersom denne brøken viser seg å være stor (som definert av signifikansnivå og frihetsgrader, se lærebok), forkaster vi nullhypotesen om at begge koeffisientene begge kan være lik null. I en generell multippel regresjon med \\(k\\) forklaringsvariable rapporterer R F-statistic: etc, med verdien av \\(F\\)-observatoren i testen for \\[H_0: \\beta_1 = \\cdots = \\beta_k = 0,\\] og dersom den oppgitte \\(p\\)-verdien er mindre enn f. eks. 5%, kan vi slutte at ikke alle koeffisientene kan være null samtidig (selv om ingen av koeffisientene i seg selv nødvendigvis er signifikant forskjellig fra null). Som en såkalt fun fact kan vi nevne at det er enkelt å teste for signifikansen til grupper av variable på denne måten, f.eks hvis det er noen variable som måler lignende ting (si \\(X_2, X_4\\) og \\(X_5\\)). I R kan du estimere to modeller, en modell som inkluderer variablene (f.eks. reg_stor) og en modell der du tar bort de aktuelle variablene (f.eks. reg_liten). Du kan da kjøre kommandoen anova(reg_stor, reg_liten) for å teste \\[H_0: \\beta_2 = \\beta_4 = \\beta_5 = 0.\\] Kritikk av læreboken: Læreboken har en tabell på s. 701 som viser sammenhengen mellom ulike statistiske størrelser som vi kan regne ut for en regresjonsmodell. \\(R^2\\) kjenner vi som forklaringsgraden, \\(s_{\\epsilon}\\) er standardavviket til residualene, \\(F\\) er testobservatoren for modellgyldighet som vi definerte uformelt over, og som er definert formelt nederst på s. 700, mens SSE (Sum of Squares Error) henger nøye sammen med standardavviket, som vi også kan se på s. 700. På disse sidene ser vi mange ligninger som viser hvordan disse størrelsene formelt henger sammen, og i tabellen på s. 701 ser vi blant annet at dersom SSE er liten, er også \\(s_{\\epsilon}\\) liten, \\(R^2\\) er nær null, og \\(F\\)-observatoren er stor. Det er greit nok, men de har en ekstra kolonne som slår fast at regresjonsmodellen er good. Her menes det ikke at regresjonsmodellen er god i den forstand at vi skal reagere med glede eller lettelse (slik noen gjerne gjør), men at variasjonen i datamaterialet i stor grad lar seg forklare av modellen vår. I et tenkt eksempel der den sanne sammenhengen mellom \\(Y\\) og \\(X\\) er gitt ved \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), men der \\(\\beta_1\\) er forholdsvis liten og \\(s_{\\epsilon}\\) er relativt stor, vil f.eks. \\(R^2\\) bli liten, selv om den enkle lineære regresjonsmodellen repsesenterer sannheten og av alle tenkende mennesker må sies å være god. Det er desverre mange lærebøker som blander disse to fortolkningene, ikke gjør det! Her er enda noen grunnleggende begreper. Har du fått med deg dette? Hva mener vi med at en observasjon er innflytelsesrik? Hva er grunnen til at vi trenger justert \\(R^2\\) med flere forklaringsvariable? Hva er forskjellen på perfekt og tilnærmet multikolinearitet i lineær regresjon? Hva blir konsekvensen i hvert av tilfellene? Kan du gi en praktisk og intuitiv forklaring på hvorfor multikolinearitet nødvendigvis må være et problem? Hva er forskjellen på statistisk og økonomisk signifikans? Kan du sette opp konkrete eksempler der vi kan estimere statistisk signifikante, men ikke økonomisk signifikante effekter i multippel regresjon? Hva med den motsatte situasjonen, økonomisk signifikant, men ikke statistisk signifikant? Grunnleggende ferdigheter: Klarer du dette? Bruke R til å tilpasse en multippel regresjonsmodell for et datasett? Bruke R til å finne særlig innflytelsesrike observasjoner? Tolke en multippel regresjonsutskrift? "],["modellbygging.html", "4.3 Modellbygging", " 4.3 Modellbygging 4.3.1 Videoforelesninger 4.3.2 Kommentarer Kapittel 18 dekker de grunnleggende begrepene innen modellbygging. I kap. 18.1 snakkes det om polynomiske modeller, i kap. 18.2 behandles dummyvariabler. Kapittel 18.3 og 18.4 handler om hvordan vi i praksis kan jobbe for å velge ut variable i en gitt situasjon. Forelesningene dekker i grunn greit det vi skal få med oss her. Som en sjekk om du har fått med deg det vesentlige, kan du svare på følgende spørsmål: Vi har lært tre typer log-transformasjoner. Hva blir fortolkningen av koeffisientene for hver av disse? Kan du nevne tre gode grunner til at log-transformasjoner er nyttige? Hvorfor sier vi at følgende modell er lineær? \\(Y = \\beta_0 + \\beta_1X + \\beta_1X^2 + \\varepsilon\\) Vil vi ikke få problemer med multikolinearitet i modellen over? Nevn en veldig god grunn til at vi må være ytterst forsiktig med polynomtransformasjoner. Hva er en dummyvariabel? Hva er fortolkningen av regresjonskoeffisienten til en dummyvariabel? Hva er fortolkningen av regresjonskoeffisienten til et interaksjonsledd mellom målevariabelen \\(X\\) og dummyvariabelen \\(D\\)? Et utrolig viktig poeng, men bruk tid til å tenke over og formulere et svar: Hvorfor er det viktig å tenke på multippel testing i sammenheng med variabelutvelgelse? Grunnleggende ferdigheter: Klarer du dette? Bruke logtransformasjoner i R? Bruke poynomtransformasjner i R? Sette opp en fornuftig regresjonsmodell ved å ta utgangspunkt i et datasett og et analyseformål, og argumentere godt for dine valg? Denne ferdigheten har blitt testet på hver eneste hjemmeeksamen i manns minne! "],["oppgaver-2.html", "4.4 Oppgaver", " 4.4 Oppgaver 4.4.1 Regresjon med en forklaringsvariabel Oppgave 1 Forskere har brukt statistikk til å undersøke om TV-titting er forbundet med overvekt. De har samlet inn data fra 15 10-åringer om antall timer TV-titting per uke og antall kilo overvekt hos barnet (rapportert som differanse fra normalvekt). De innsamlede dataene er oppsummert i tabellen: TV_titting Overvekt 42 17 35 5 28 -1 34 0 37 13 38 15 32 5 33 7 18 -7 28 7 36 6 29 7 29 4 34 15 18 -5 Bruk R til å lage et spredningsplott av resultatene. Hva tror du om forholdet mellom de to variablene utfra figuren? Sett opp regresjonsuttrykket for å undersøke om overvekt er forbundet med TV-titting. La overvekt være responsvariabelen. Hva er betydningen av hver parameter i uttrykket? Bruk R til å estimere parameterne. Hva kan koeffisientene fortelle deg om forholdet mellom overvekt og TV-titting? Beregn et 95 % konfidensintervall for \\(\\beta_1\\). Hint: Bruk summary() på regresjonsmodellen for å finne \\(S(\\hat{\\beta}_1)\\) som da gitt i kolonnen Std. Error. Bruk R til å beregne et 95 % prediksjonsintervall for overvekt i kilo for et barn som ser på TV 30 timer i uken. Hva forteller intervallet deg? Bruk R til å beregne et 95 % konfidensintervall for gjennomsnittlig overvekt for barn som ser 30 timer på TV i uken. Hvordan er tolkningen av dette intervallet forskjellig fra det i oppgave d? Hva er forventet overvekt for barn som ser 0 timer på TV i uken utfra modellen? Hva kan være problematisk ved å gjøre denne type analyser av en regresjonsmodell? Løsning df_tv &lt;- data.frame( TV_titting = c(42, 35, 28, 34, 37, 38, 32, 33, 18, 28, 36, 29, 29, 34, 18), Overvekt = c(17, 5, -1, 0, 13, 15, 5, 7, -7, 7, 6, 7, 4, 15, -5)) plot(df_tv$TV_titting, df_tv$Overvekt, type = &quot;p&quot;, xlab=&quot;TV-titting&quot;, ylab=&quot;Overvekt&quot;) Plottet viser en ganske tydelig trend om at TV-titting og overvekt er relaterte. Dette kan vi undersøke nærmere. Vi setter overvekt som responsvariabel og TV-titting som forklaringsvariabel. Uttrykket blir da: \\[\\begin{equation} \\text{Overvekt} = \\beta_0 + \\beta_1 \\text{TV-titting} + \\epsilon \\end{equation}\\] Ser vi bort fra dataene ville \\(\\beta_0\\) være forventet overvekt for personer som ikke ser på tv. Av spredningsplottet ser det derimot ut som at vi ikke har data ved 0 TV-titting og det er derfor ikke fornuftig med en direkte tolkning av denne parameteren. Under antagelsen om at regresjonsmodellen er gyldig forteller \\(\\beta_1\\) hvor mye vi forventer at overvekten øker dersom man øker TV-tittingen med 1 time. Vi estimerer en regresjonsmodell fra dataene og skriver ut tabellen med resultatene: reg &lt;- lm(Overvekt ~ TV_titting, df_tv) summary(reg) ## ## Call: ## lm(formula = Overvekt ~ TV_titting, data = df_tv) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.1917 -2.6147 0.2795 2.6785 6.8083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -22.2124 5.1359 -4.325 0.000824 *** ## TV_titting 0.8942 0.1602 5.583 8.88e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.026 on 13 degrees of freedom ## Multiple R-squared: 0.7057, Adjusted R-squared: 0.683 ## F-statistic: 31.17 on 1 and 13 DF, p-value: 8.88e-05 # For en finere utskrift kan du bruke følgende: # library(stargazer) # stargazer(reg, type=&quot;text&quot;) Det er tydelig sigifikans for at hver av parameterne er ulik null. For \\(\\beta_1\\) betyr dette at trenden vi observerte i spredningplottet var signifikant. Videre kan vi tolke det positive fortegnet til \\(\\beta_1\\) som at flere timer TV-titting er relatert med økt overvekt. Merk at vi ikke kan si utfra dataene at mer TV-titting fører til overvekt. En mer riktig tolkning er å si at de forekommer samtidig i populasjonen. (Dersom vi ønsket å finne ut av kausaliteten måtte man på et tilfeldig utvalg av barn satt noen til å se mye på TV og noen til å se mindre på TV, og deretter undersøkt om dette resulterte i statistisk signifikant høyere overvekt hos en av gruppene. Dette krysser imidlertid noen etiske grenser om å påføre barn overvekt, dersom hypotesen er sann.) Fra R utskriften ser vi at \\(S(\\hat{\\beta}_1)=0.1602\\). Videre er \\(t_{0.025, 13} = 2.16\\). Et 95 % konfidensintervall for \\(\\beta_1\\) er da gitt ved: \\[\\left[\\hat{\\beta}_1 - t_{\\alpha/2, n - 2}S(\\hat{\\beta}_1), \\hat{\\beta}_1 + t_{\\alpha/2, n - 2}S(\\hat{\\beta}_1)\\right]\\\\ =\\left[0.8942 - 2.16\\times0.1602, 0.8942 + 2.16\\times0.1602\\right]\\\\ =\\left[0.5481, 1.2402\\right]\\] e) reg.pred &lt;- predict(reg, newdata = data.frame(TV_titting = c(30)), interval = &quot;predict&quot;) reg.pred ## fit lwr upr ## 1 4.614735 -4.380262 13.60973 # stargazer(reg.pred, type=&quot;text&quot;) Dersom vi trekker et nytt barn som ser 30 timer på TV per uke, vil barnet i 95 % av tilfellene være innenfor prediksjonsintervallet dersom vi hadde gjentatt dette mange ganger. reg.conf &lt;- predict(reg, newdata = data.frame(TV_titting = c(30)), interval = &quot;confidence&quot;) reg.conf ## fit lwr upr ## 1 4.614735 2.317582 6.911888 #evt # library(stargazer) # stargazer(reg.conf, type=&quot;text&quot;) Konfidensintervallet viser hvor gjennomsnittet av målinger på et nytt sett med barn som ser 30 timer på TV per uke vil ligge i 95 % av nye eksperimenter. reg.zero &lt;- predict(reg, newdata = data.frame(TV_titting = c(0)), interval = &quot;prediction&quot;) reg.zero ## fit lwr upr ## 1 -22.21237 -36.30997 -8.11477 # evt # library(stargazer) # stargazer(reg.zero, type=&quot;text&quot;) Beste gjetning er at barn som ser 0 timer på TV i uken er 22.2 kg under normalvekt (!). Et kjapt søk viser at barn på 10 år veier mellom 25.5 og 39 kg. Det høres urimelig ut at veldig lite TV-titting har sammenheng med ekstrem underernæring blant den samme populasjonen av barn som ser rundt 30 timer på TV i uken. Det er flere ting som kan gå galt når man gjør en slik analyse av en modell: Modellen kan være gal. Vi antar et lineær forhold mellom TV-titting og overvekt, noe som ikke trenger å være riktig ved 0 timer TV-titting. Datasettet dekker ikke den gruppen barn som ser 0 timer på TV, så med mindre modellen er helt riktig (noe den sjelden er) vil den ikke kunne generalisere så langt utenfor området vi estimerte parameterne på. Med mindre forholdet mellom TV-titting og overvekt er kausalt, kan det være at sammenhengen som er observert mellom dem i dataene ikke vil være det samme langt utenfor det aktuelle data-området. Oppgave 2 Vi skal undersøke om alder har sammenheng med hvor lang tid man bruker på et puslespill. Vi har data fra et tilfeldig utvalg av 210 voksne personer om alder og tid brukt på oppgaven. Uttrykk alder som \\(X\\) og tid i minutter som \\(Y\\). Følgende deskriptive statistikker er beregnet for datasettet: \\(s_{xy}^2= 8, s_x^2, = 110, s_y^2 = 42, \\bar{x} = 40, \\bar{y} = 20\\) hvor \\(\\bar{x},\\bar{y}\\) er gjennomsnittene. Sett opp regresjonsuttrykket for sammenhengen mellom \\(X\\), \\(Y\\) og støy \\(\\epsilon\\). Hva er antagelsen for sammenhengen mellom X og \\(\\epsilon\\)? Hva er uttrykket for beste gjetning/prediksjon \\(\\hat{Y}_i\\) gitt en verdi av forklaringsvariabelen \\(X_i\\) for regresjonsmodellen du har satt opp? Hva er uttrykket for residualene? Vis at hva uttrykket for regresjonsparameterne blir når de estimeres med minste kvadraters metode. Regn ut minste kvadraters estimat av parameterne. Løsning Regresjonsuttrykket er \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X + \\epsilon \\end{equation}\\] Vi antar at forklaringsvariabelen \\(X\\) er uavhenging støyen \\(\\epsilon\\). Dermed vil også kovariansen være null, \\[\\begin{equation} \\text{Cov}(X, \\epsilon) = 0. \\end{equation}\\] Beste gjetning er forvetningen betinget på utfallet av forklaringsvariabelen \\(X\\): \\[\\begin{align} \\hat{Y}_i &amp;= E[Y|X = X_i] = E[\\beta_0 + \\beta_1 X + \\epsilon|X = X_i] \\\\ &amp;= \\beta_0 + \\beta_1 X_i + E[\\epsilon|X = X_i] \\\\ &amp;= \\beta_0 + \\beta_1 X_i + E[\\epsilon] \\\\ &amp;= \\beta_0 + \\beta_1 X_i \\end{align}\\] Residualene er forskjellen mellom data og prediksjon \\[\\begin{equation} r_i = \\hat{Y}_i - Y_i \\end{equation}\\] Minste kvadraters metode minimerer summen av residualene kvadrert, så \\[\\begin{align} SSE &amp;= \\sum_{i} r_i^2 \\\\ &amp;= \\sum_{i} (\\beta_0 + \\beta_1 X_i - Y_i)^2. \\end{align}\\] Minimér ved å sette den deriverte for hver parameter til null: \\[\\begin{align} \\frac{dSSE}{d\\beta_0} &amp;= \\sum_{i} 2 (\\beta_0 + \\beta_1 X_i - Y_i) \\overset{!}{=} 0 \\\\ \\implies \\quad \\beta_0 &amp;+ \\beta_1 \\frac{\\sum_{i} X_i}{N} = \\frac{\\sum_{i} Y_i}{N} \\\\ \\implies \\quad \\beta_0 &amp;= \\frac{\\sum_{i} Y_i}{N} - \\beta_1 \\frac{\\sum_{i} X_i}{N}, \\end{align}\\] og \\[\\begin{align} \\frac{dSSE}{d\\beta_1} &amp;= \\sum_{i} 2 (\\beta_0 + \\beta_1 X_i - Y_i)X_i \\\\ &amp;= 2 \\beta_0 \\sum_{i} X_i + 2 \\beta_1 \\sum_{i} X_i^2 - 2\\sum_{i} X_i Y_i \\overset{!}{=} 0 \\\\ &amp;\\implies \\quad \\beta_0 \\frac{\\sum_{i} X_i}{N} + \\frac{\\sum_{i} X_i^2}{N} = \\frac{\\sum_{i} X_i Y_i}{N} . \\end{align}\\] Satt inn for \\(\\beta_0\\) blir det \\[\\begin{align} \\left ( \\frac{\\sum_{i} Y_i}{N} - \\beta_1 \\frac{\\sum_{i} X_i}{N} \\right ) \\frac{\\sum_{i} X_i}{N} + \\beta_1\\frac{\\sum_{i} X_i^2}{N} &amp;= \\frac{\\sum_{i} X_i Y_i}{N} \\\\ \\beta_1 \\left ( \\frac{\\sum_{i} X_i^2}{N} - \\left ( \\frac{\\sum_{i} X_i}{N} \\right )^2 \\right ) &amp;= \\frac{\\sum_{i} X_i Y_i}{N} - \\frac{\\sum_{i} X_i}{N}\\frac{\\sum_{i} Y_i}{N} \\\\ \\beta_1 S_X^2 &amp;= S_{XY}. \\end{align}\\] Da blir beta_1 &lt;- 8 / 110 beta_1 ## [1] 0.07272727 og beta_0 &lt;- 20 - beta_1 * 40 beta_0 ## [1] 17.09091 Oppgave 3 Figurene nedenfor viser residualene (feilleddene) vi får ut fra et par ulike modeller. Ser plottene ut til å tilfredsstille kravene for enkel regresjon? Hvis ikke, hva ser ut til å være galt for hver av figurene? Hvordan kan man håndtere brudd på betingelsene i de ulike situasjonene? Løsning Her ser støyleddet ganske normalfordelt ut. Det betyr at antagelsene er oppfylt. Støyen er symmetrisk om null, men kan se ut som den øker. Altså er det brudd på antagelsen om normalfordeling. Økningen ser ut til å være jevn, og en log-transformasjon kunne i dette tilfellet vært til hjelp. Her ser det ut som at vi har perioder med høy varians etterfulgt av perioder med lavere varians. Vi skal ikke se så mye nærmere på hvordan dette kan håndteres i MET4, men man kan komme over det i senere kurs. Her ser det ut som at støyen har en relasjon i tid, og at høye/lave verdier henger sammen med høye/laver verdier. Det er ikke åpenbart at det er dette som foregår, og vi burde undersøkt det nærmere. Senere i kurset skal vi håndtere dette ved bruk av tidsrekkemodeller. Merk at det ikke alltid er lett å se direkte fra figurene hva som er galt. Riktig bruk av statistiske tester og diagnoseplott kan hjelpe med å oppdage brudd på betingelsene. Figurene i denne oppgaven er generert ved simulering, vi kan derfor vite akkurat hva som er galt. For virkelige data vil kunne være vanskeligere å tolke fordi det ikke nødvendigvis er én ting som er galt om gangen. Man må også være på vakt for overtolkning av det som egentlig er tilfeldigheter. 4.4.2 Multippel regresjon og modellbygging Oppgave 1 Denne oppgaven skal gi et forhold til hvordan teorien funker i praksis. Vi skal simulere data fra konstruerte modeller gjøre analyser på dem. Fordelen med å begynne her er at man får er forhold til hvordan statistikk kan og bør tolkes utfra teorien. Med virkelige data vil antagelsene som ligger til grunn for modellene stort sett være brutt, i større eller mindre grad, og kjernen i god statistisk analyse er å vite hva man kan og ikke kan gjøre likevel. God kjennskap til hvordan det burde funke i teorien er en viktig byggestein for statistisk forståelse. Vi generer opp data som skal brukes til å estimere en model. rnorm trekker tall fra en standard normalfordeling. mutate() legger til nye kolonner i dataframe-en basert på allerede eksisterende kolonner. set.seed() setter startpunktet for pseudotilfeldige tall, og om du setter samme seed får du de samme tilfeldige tallene som vi har brukt. set.seed(4) df_mdl &lt;- data.frame( x1 = rnorm(100, sd = 2), x2 = rnorm(100, sd = 2), eps = rnorm(100) ) %&gt;% mutate( y = 2 * x1 + (-1) * x2 + eps ) Hva er de eksakte parameterne i modellen som passer til dataene vi har generert, og hva er fordelingen til hver av forklaringsvariablene og støy-leddet? Er betingelsen om uavhengig feilledd oppfylt? Generér opp dataene selv og estimer parameterne basert på dataene. Tolk estimatene. Lag 95 % konfidens- og prediksjonsintervaller for en prediksjon hvor \\(X_1=1, X_2=1\\) basert på estimatene dine. Hva er eksakt prediksjonsintervall basert på modellen vi har generert data fra? Ser dine prediksjonsintervaller rimelige ut utfra dette? Hvorfor er de ikke eksakt like? Stemmer konfidensintervallet overens med modellen? Tolk konfidensintervallet basert på at vi kan generere opp nye data (med et annet seed). Hva er P-verdien i F-test for at vi har en signifikant sammenheng mellom responsvariabelen og forklaringsvariablene? Løsning Fordelingen til forklaringsvariablene er \\[\\begin{align} X_1, X_2 \\sim N(0, 2) \\end{align}\\] fordi vi har trukket dem fra en normalfordeling med forventning 0 og standardavvik 2. Videre er fordelingen til støyleddet en standard normalfordeling \\(\\epsilon\\sim N(0,1)\\). Betingelsen om uavhengig feilledd er oppfylt siden vi ikke har brukt feilleddet til å generere forklaringsvariablene. Det er responsvariabelen, men den skal altså være avhengig av støyleddet. Modellen tar formen \\[\\begin{align} Y = 2 X_1 - X_2 + \\epsilon,\\quad \\epsilon \\sim N(0, 1), \\end{align}\\] hvor parameterne er eksakt fordi vi har laget dataene selv. reg &lt;- lm(y ~ x1 + x2, data = df_mdl) stargazer(reg, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 1.986*** ## (0.053) ## ## x2 -1.091*** ## (0.048) ## ## Constant -0.055 ## (0.096) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.958 ## Adjusted R2 0.957 ## Residual Std. Error 0.956 (df = 97) ## F Statistic 1,094.997*** (df = 2; 97) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Vi ser at estimatene av koeffisientene er signifikante, og at sanne verdier er innenfor konfidensintervallet for hver av dem. Det er ikke et konstantledd i modellen vår, noe som stemmer overens med at den ikke er signifikant forskjellig fra null i regresjonsmodellen. new_df &lt;- data.frame(x1 = 1, x2 = 1) reg_pred &lt;- predict(reg, newdata = new_df, interval = &quot;predict&quot;) reg_conf &lt;- predict(reg, newdata = new_df, interval = &quot;confidence&quot;) ## ## Prediksjonsintervall ## ==================== ## fit lwr upr ## -------------------- ## 1 0.840 -1.073 2.752 ## -------------------- ## ## Konfidensintervall ## =================== ## fit lwr upr ## ------------------- ## 1 0.840 0.600 1.080 ## ------------------- Eksakt prediksjonsintervall for modellen som genererte dataene er \\[\\begin{align} \\bar{Y}|X_1,X_2 \\pm \\alpha_{0.025} \\sigma_{\\epsilon} &amp;= 2 \\cdot 1 - 1 \\cdot 1 \\pm 1.96 \\cdot 1 \\\\ &amp;= 1 \\pm 1.96 \\\\ &amp;= [-0.96, 2.96]. \\end{align}\\] Estimert prediksjonsintervall er bredere enn det estimerte intervallet. Dette kommer av usikkerhet knyttet til estimatene av forventning og standardfeil. Det er benyttet T-test i estimatet av prediksjonsintervallet for å ta hensyn til denne usikkerheten. Man burde ikke få smalere prediksjonsintervaller enn det som faktisk er sant med mindre noen av modellantagelsene er usann. Merk at når vi generer opp data selv vet vi sannheten, det gjør vi ikke for virkelige data. Vi burde få at \\(\\bar{Y}|X_1,X_2 = 1\\) er innenfor konfidensintervallet, noe ser ut til å være oppfylt. Tolkningen er at dersom vi kjører analysen på mange ulike datasett vil \\(\\bar{Y}|X_1,X_2 = 1\\) havne innenfor konfidensintervallet i 95 % av tilfellene. Dette kan du prøve selv, bare pass på at du ikke setter seed-et før du generer opp et nytt datasett. Svaret gis i regresjonstabellen stargazer(reg, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 1.986*** ## (0.053) ## ## x2 -1.091*** ## (0.048) ## ## Constant -0.055 ## (0.096) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.958 ## Adjusted R2 0.957 ## Residual Std. Error 0.956 (df = 97) ## F Statistic 1,094.997*** (df = 2; 97) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 F-statistikken er angitt som \\(1094.997\\) med frihetsgrader \\(2, 97\\). P-verdien regnes som pf(1094.997, 2, 97, lower.tail = FALSE) ## [1] 2.716911e-67 som er veldig liten. Altså bekrefter analysen at det er sammenheng mellom forklaringsvariablene og responsvariabelen. Hva ville du tenkt var galt om vi ikke fikk denne konklusjonen fra F-testen, gitt den modellen vi har brukt til å generere dataene? Hint: Det er faktisk en reell sammenheng her. 4.4.2.1 Oppgave 2 Denne oppgaven følger samme premiss som oppgave 1, men handler om kolinearitet. Last ned datasettene 1 og 2, hvor y er responsvariabel og forklaringsvariable er navngitt x. Finn kolinearitet Hva må man passe dersom man skal lage en regresjonsmodell for responsen Y? Løsning Vi kan begynne med å plotte forholdet mellom de ulike variablene: pairs(y ~ ., data = df_colin1) corrplot(cor(df_colin1), method = &quot;number&quot;) Her ser det ut som vi har et forhold mellom \\((X_2, X_3)\\). Videre ser det ut som at alle tre forklaringsvariable har en sammenheng med responsvariablen \\(Y\\). reg_colin1 &lt;- lm(y ~ ., data = df_colin1) stargazer(reg_colin1, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 1.348*** ## (0.053) ## ## x2 -0.695*** ## (0.064) ## ## x3 -0.208** ## (0.102) ## ## Constant -0.095 ## (0.097) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.916 ## Adjusted R2 0.913 ## Residual Std. Error 0.960 (df = 96) ## F Statistic 346.946*** (df = 3; 96) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 \\(X_1\\) og \\(X_2\\) kommer ut som signifikante, og \\(X_3\\) ved lavere signifikansnivå. Vi kan sjekke variance inflation factor (VIF) vif(reg_colin1) ## x1 x2 x3 ## 1.018945 1.785963 1.774712 som oppgir at estimeringen i seg selv ikke burde være noe problem. Kommentarer: Det reelle forholdet mellom variablene i dette eksempelet er \\[\\begin{align} X_1 &amp;\\sim N(0, 2) \\\\ X_2 &amp;\\sim N(0, 2) \\\\ X_3 &amp;= 0.5 X_2 + \\epsilon_3 \\quad \\epsilon_3 \\sim N(0,1) \\\\ Y &amp;= 1.3 X_1 - 0.8 X_2 + \\epsilon, \\quad \\epsilon \\sim N(0,1), \\end{align}\\] og vi kan tenke oss at dette representerer det kausale forholdet mellom variablene. Siden vi har generert dataene kan vi si det. Det er videre interessant å merke seg i dette eksempelet at \\(X_3\\) kommer ut med en signifikant koeffisient selv om den rent kausalt ikke har noen sammenheng med \\(Y\\). \\(X_3\\) er kun relatert til \\(X_2\\). Dersom man ønsker å gjøre prediksjon kan det være forklaringskraft i \\(X_3\\) for å gjøre prediksjon av \\(Y\\), men i så tilfelle antas det at forholdet mellom \\((X_2, X_3)\\) også vedvarer i fremtiden. Dersom man skal gjøre inferens om en tenkt kausal sammenheng er det mulig å gå på en blemme i et tilfelle som dette. Hvis man ønsker å forklare noe og det er sammenheng mellom to forklaringsvariable kan det fra økonomisk prespektiv være interessant å spørre seg om man bryr seg om \\(X_3\\) i det hele tatt og skal ta den ut. Kanksje \\(X_3\\) er vanskelig å samle inn data om eller den åpenbart ikke har sammenheng med Y. Om den åpenbart ikke skal ha sammenheng med \\(Y\\) kan støyleddet \\(\\epsilon_3\\) også skape unødig støy i prediksjonene. Plott forholdet mellom de ulike variablene: pairs(y ~ ., data = df_colin2) corrplot(cor(df_colin2), method = &quot;number&quot;) Her ser vi et tydelig forhold mellom parene \\((X_3, X_4)\\), \\((X_4, X_5)\\), \\((X_3, X_5)\\) og \\((X_2, X_5)\\). Ellers ser det kun ut som det er et tydelig forhold mellom \\(Y\\) og \\(X_1, X_2\\). Med så mange forhold kan det være snakk om multikolinearitet, som ikke er så lett å se utfra 2D-plott. reg_colin2 &lt;- lm(y ~ ., data = df_colin2) stargazer(reg_colin2, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## y ## ----------------------------------------------- ## x1 2.049*** ## (0.053) ## ## x2 -1.179*** ## (0.111) ## ## x3 -0.198* ## (0.114) ## ## x4 0.101** ## (0.050) ## ## x5 0.092 ## (0.097) ## ## Constant -0.018 ## (0.095) ## ## ----------------------------------------------- ## Observations 100 ## R2 0.963 ## Adjusted R2 0.961 ## Residual Std. Error 0.932 (df = 94) ## F Statistic 486.175*** (df = 5; 94) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Vi ser på VIF for om det kan være problemer med estimering grunnet kolinearitet stargazer(vif(reg_colin2), type = &quot;text&quot;, title = &quot;VIF&quot;) ## ## VIF ## ============================= ## x1 x2 x3 x4 x5 ## ----------------------------- ## 1.062 5.731 5.517 1.662 7.769 ## ----------------------------- hvor vi ser at \\(X_5\\) nærmer seg problematisk område for å få en overdreven varians i estimatene. Dersom målet er å estimere sammenheng mellom alle forklaringsvariable og \\(Y\\) vil dette kunne gå gjennom siden VIF ikke er altfor høy. Hvis det derimot er snakk om modellbygging bør man tenke gjennom om noen av variablene ikke er så viktige og bør tas ut. Kommentarer: Det reelle forholdet mellom variablene i dette eksempelet er \\[\\begin{align} X_1 &amp;\\sim N(0, 2) \\\\ X_2 &amp;\\sim N(0, 2) \\\\ X_3 &amp;\\sim N(0, 2) \\\\ X_4 &amp;= X_3 + \\epsilon_4, \\quad \\epsilon_4 \\sim N(0,2) \\\\ X_5 &amp;= X_2 + X_3 + \\epsilon_5, \\quad \\epsilon_5 \\sim N(0,1) \\\\ Y &amp;= 2 X_1 - X_2 + \\epsilon, \\quad \\epsilon \\sim N(0,1) \\end{align}\\] Her kan vi igjen merke oss at variablene \\(X_3, X_4, X_5\\) kommer inn i regresjonsmodellen med noen koeffisienter som er svakt signifikante. I dette tilfellet trenger ikke dette ha så mye å si. Likevel, ettersom det ikke er noen direkte sammenheng mellom \\(X_3, X_4, X_5\\) ville det vært ideelt å ikke ha dem med i regresjonsmodellen for å unngå feilestimering av koeffisientene til \\(X_1\\) og \\(X_2\\). Fra anvendt statistikk perspektiv bør man tenke gjennom om det gir mening at \\(X_3, X_4, X_5\\) forklarer \\(Y\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
