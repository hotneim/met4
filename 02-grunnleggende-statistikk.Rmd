
# Grunnleggende statistikk

I denne modulen introduserer vi en del grunnleggende statistiske begreper. Mye vil oppleves som repetisjon, mens noe vil være nytt. Noe er veldig praktisk ved at vi kan bruke det direkte i eksempler, mens andre ting er mer teoretisk av natur. Felles for det vi skal se på her er at vi kommer til å bruke mange av begrepene vi lærer senere i kurset.

I videoforelesningene går vi gjennom noen slides, og vi skriver et R-skript. Du kan laste disse ned ved å klikke på lenkene under:

[Slides til "Grunnleggende statistikk"](script-slides/grunnleggende-stat/grunnleggende-stat-slides.html)

[R-script til "Grunnleggende statistikk"](script-slides/grunnleggende-stat/grunnleggende-stat-script.R)

**TIPS:** Hvis du ønsker å laste ned lysbildene som PDF trykker du på linken over, velger "Skriv ut", og så skriver du ut som PDF. Før du gjør det bør du scrolle gjennom alle sidene slik at ligningene vises korrekt.

## Deskriptiv statistikk

### Videoforelesninger

<div style='padding:74.79% 0 0 0;position:relative;'><iframe src='https://vimeo.com/showcase/7748078/embed' allowfullscreen frameborder='0' style='position:absolute;top:0;left:0;width:100%;height:100%;'></iframe></div>

### Kommentarer

Deskriptiv statistikk handler ikke om analyse eller regning, men om å presentere kompleks informasjon på en effektiv måte. Det er altså noe ganske annet enn det vi ellers snakker om i kurset, men det er likevel et av de nyttigste læringspunktet vi har. Hvem kan ikke regne med å måtte presentere tall og resultater i løpet av sin karriere? Eller selge inn forslag og planer for overordnede i håp om å bli lyttet til? Det kan være direkte avgjørende for din egen gjennomslagskraft at du er i stand til å produsere overbevisende tabeller og figurer i slike situasjoner, og det er det dette temaet handler om. 

I læreboken er det kapitlene **2--4** som behandler deskriptiv statistikk, men det er veldig Excel-fokusert, som ikke er så relevant for oss. Det er likevel ikke dumt å lese gjennom stoffet for å se hva det går i, og legg spesielt merke til følgende punkter:

- Ulike datatyper i avsnitt **2-1**.
- **3-4**: The art and science of graphical presentations. Hva er det som gjør en grafisk illustrasjon god? Prøv å ta inn over dere all informasjonen som vi lett kan lese ut av bildet på side 75 om Napoleons felttog mot Moskva. Her presenteres informasjon om tid, antall, geografi og temperatur på en helt eksepsjonelt effektiv måte! Videre er det noen grelle eksempler på hvordan vi kan bruke grafiske virkemidler til å gi skjeve fremstillinger. I videoforelesningen gir vi flere eksempler på dette. 
- Kapittel **4** går litt mer i dybden om numeriske deskriptive teknikker, som gjennomsnitt, median, standardavvik, korrelasjon, osv. Dette skal være dekket greit i forelesningen, men boken går litt lenger.

Det kan være en fin øvelse å kikke på eksemplene i læreboken og forsøke å gjenskape noen av Excel-figurene i R. Se på eksempel 3.2, der man skal lage to histogrammer over historiske avkastninger for to ulike investeringsstrategier. Vi leser inn datasettet (last ned fra Canvas) som under og kikker på det:

```{r, echo = FALSE}
library(readxl)
returns <- read_xlsx("datasett/Xm03-02.xlsx")
```

```{r, eval = FALSE}
library(readxl)
returns <- read_xlsx("Xm03-02.xlsx")
returns
```

```{r, echo = FALSE}
returns
```

Hver stategi har sin kolonne. Merk at variabelnavnene har mellomrom i seg, noe som er upraktisk når vi jobber med et seriøst programmeringsspråk. En god vane er å rett og slett gi dem nye navn, ved f.eks. å kjøre `colnames(returns) <- c("returnA", "returnB")`, eller så må vi alltid referere til variabelnavnene ved å bruke slike "backticks" som vi ser under.

Vi kan lage to enkle histogrammer slik vi gjorde det i forelesningen:

```{r to-histogrammer, fig.show = "hold", fig.cap="To histogrammer"}
library(ggplot2)
ggplot(returns, aes(x = `Return A`)) +
        geom_histogram(bins = 10)
ggplot(returns, aes(x = `Return B`)) +
        geom_histogram(bins = 10)
```

Her er noen kontrollspørsmål som du kan prøve deg på:

1. Hva er forskjellen på deskriptiv statistikk og statistisk inferens?
1. Deskriptiv statistikk kan gjøres grafisk eller numerisk, eventuelt som tabeller av ulike numeriske mål. Nevn noen fordeler og ulemper man må veie mot hverandre når vi skal velge mellom grafisk og numerisk deskriptiv statistikk.

## Utvalg og estimering

> You can, for example, never foretell what any one man will do, but you can say with presicion what an average number will be up to. Individuals vary, but percentages remain constant. So says the statistician.

`r tufte::quote_footer('--- Sherlock Holmes')`

### Videoforelesninger

<div style='padding:74.93% 0 0 0;position:relative;'><iframe src='https://vimeo.com/showcase/7774368/embed' allowfullscreen frameborder='0' style='position:absolute;top:0;left:0;width:100%;height:100%;'></iframe></div>

### Kommentarer

I videoforelesningene over går vi gjennom noen sentrale begreper i statistikk. Noen av dem skal vi bruke mye i fortsettelsen, mens andre er ment for å gi dere et solid teoretisk fundament når vi etter hvert skal begi oss ut på anvendt statistikk.

Vi startet med å sette opp en liten agenda. Som et første steg kan du kikke på, og notere ned noen setninger til, disse punktene og se om du har fått med deg hva de betyr:

- Samplingfordelinger
- Forventning/varians
- Sentralgrenseteoremet
- Hva er samplingfordelingen til et gjennomsnitt?
- Hva er samplingfordelingen til en andel?
- Forventningsrett
- Konsistens

I Boken er det kapittel **9** (*Sampling distributions*) og **10** (*Introduction to estimation*) som gjelder. Kapittel **6**--**8** omhandler stoff skal skal være greit dekket i MET2 (Sannsynlighet, fordelinger, stokastiske variable, osv.), men det kan være nyttig å skumme gjennom likevel hvis disse begrepene ligger langt bak i bevissheten din. 

Kapittel **9** starter med å diskutere samplingfordelingen til et gjennomsnitt. Dette er nyttig lesestoff, men de viktigste punktene er som følger:

- Dersom observasjonene $X_1, X_2, \ldots, X_n$ er normalfordelt, er også gjennomsnittet $\overline X = \frac{1}{n}\sum_{i=1}^n X_i$ normalfordelt.
- Dersom E$(X_i) = \mu$ og Var$(X_i) = \sigma^2$ for alle $i = 1,\ldots,n$, er E$(\overline X)=\mu$ og Var$(\overline X) = \sigma^2/n$. Dette regnet vi ut formelt.
- Dersom $n$ er *stor*, er $\overline X$ tilnærmet normalfordelt, uavhengig av fordelingen til den enkelte $X_i$. Dette følger av **sentralgrensesetningen**. 

Dette står i en boks på slutten av seksjon **9-1a**. Hvor stor må $n$ være for at denne tilnærmingen er god nok? Det finnes ikke et entydig svar på, men når vi passerer 50-100 observasjoner kan vi i våre MET4-problemer gjerne si at $n$ er «stor nok». I **9-1b** og **9-1c** brukes sentralgrenseteoremet til å regne på normalsannsynligheter i MET2-stil. I **9-1d** er det noen Excel-instruksjoner som du kan hoppe over hvis du vil.

Tekstboksen i **9-2c** oppsummerer det vi fant ut om samplingfordelingen til en observert andel. I seksjon **9-3** snakkes det om samplingfordelingen til *differansen av to gjennomsnitt*. Vi gikk ikke gjennom det eksplisitt i forelesningen, men det er ikke noe substansielt nytt her. Vi skal bruke dette reultatet i neste modul når vi skal sammenligne to gjennomsnitt. I seksjon **9-4** får vi forklart hva vi skal bruke samplingfordelinger til fremover. Bør leses.

Kapittel **10** omhandler *estimering*, dvs hvordan vi bruker data til å «gjette» på verdien til en ukjent parameter. Vi forsøkte i forelesningen å gi litt intuisjon til begrepene

- forventningsrett estimator,
- variansen til en estimator, og
- konsistens.

Vi kan lage et *punktestimat* av en forventningsverdi ved å ta gjennomsnittet av observasjoner, og vi kan lage et *konfidensintervall* ved å følge oppskriften i boksen på s. 316 (i 11. utgave).

I eksempel 10.1 har vi 25 observasjoner fra en normalfordeling. Oppgaven er å estimere forventningsverdien med et tilhørende 95% konfidensuntervall. Pass på at du forstår den manuelle utregningen. I stedet for å bruke Excel (eller taste alle disse tallene inn på en kalkulator) kan du skrive et lite R-script som gjør det samme:

```{r}
# Vi skriver inn datasettet i en vektor
demand <- c(235, 374, 309, 499, 253, 
            421, 361, 514, 462, 369,
            394, 439, 348, 344, 330,
            261, 374, 302, 466, 535,
            386, 316, 296, 332, 334)

# Vi trenger 4 verdier for å regne ut konfidensintervallet:
gj.snitt <- mean(demand)     # Regner ut gjennomsnittet
z <- 1.96                    # Denne finner vi i tabellen
sigma <- 75                  # Oppgitt i oppgaven
n <- length(demand)          # Antall observasjoner

# Vårt estimat av forventningsverdien er bare gjennomsnittet. 
# Regner ut nedre og øvre grense i konfidensintervallet (LCL, UCL):
LCL <- gj.snitt - z*sigma/sqrt(n)
UCL <- gj.snitt + z*sigma/sqrt(n)

# Samler de tre tallene i en vektor og skriver ut:
c(LCL, gj.snitt, UCL)
```
 
 I seksjon **10-2a** forsøker boken å forklare fortolkningen av et konfidensintervall. Hovedpoengene her er at:
 
 - Et 95%-konfidensintervall skal *ikke* tolkes som «sannsynligheten for at den sanne parameterverdien ligger i intervallet er 95%».
 - Den korrekte tolkningen er: «Dersom vi hadde hatt tilgang til å trekke nye utvalg fra populasjonen med like mange observasjoner og bruker dem til å regne ut nye konfidensintervaller, vil 95 av 100 intervaller inneholde den sanne parameterverdien».
 
Forskjellen på disse formuleringene er meget subtil, så subtil faktisk at det ikke er åpenbart at det er særlig god pedagogikk å peke på den. Problemet med den første formuleringen er at vi der kan få inntrykk av at det er den sanne parameterverdien som er stokastisk og avhengig av datasettet vi observerer, mens det strengt tatt er grensene til konfidensintervallet som er tilfeldige, og altså avhengige av datasettet. Det kommer klarere frem i den andre formuleringen.
 
Bredden til et konfidensintervall er altså et uttrykk for *usikkerhet*, eller motsatt: *presisjon*.  

Seksjon **10-2b** og **10-2c** kan skummes raskt gjennom. Seksjon **10-3** handler om at vi først bestemmer oss for et presisjonsnivå (dvs bredde på konfiensintervallet) $B$, og så regner ut hvor mange observasjoner vi trenger for å oppnå det. Vi kommer frem til en formelen

$$n = \left(\frac{z_{\alpha/2}\sigma}{B}\right)^2,$$
men problemet i praksis er at vi gjerne ikke kjenner $\sigma$, og vi kan heller ikke estimere den fordi vi ikke har samlet inn data enda. Løsningen er at vi enten på bruke fornuften, eller eventuelt et tidligere estimat av $\sigma$ dersom det er tilgjengelig.

Når du har vært gjennom dette stoffet skal du forhåpentligvis være i stand til å diskutere følgende spørsmål med f.eks. en medstudent:

1. Hva er en samplingfordeling?
2. Hva sier sentralgrenseteoremet?
3. Hva mener en statistiker når hen sier at "gjennomsnittet konvergerer som $1/\sqrt{n}$"?
4. Hva er samplingfordelingen til et gjennomsnitt?
5. Hva er samplingfordelingen til en andel?
6. Hva vil det si at et estimator er forventningsrett?
7. Hva vil det si at et estimator er konsistent?

### Ekstra øving i R

Som demonstrert i forelesningen kan vi i R simulere standard normalfordelte observasjoner (dvs normalfordelte observasjoner med $\mu = 0$ og $\sigma^2 = 1$) med kommandoen `rnorm(n)`, der `n` er antallet observasjoner vi ønsker. For eksempel kan vi kjøre følgende kode for å generere 10 observasjoner (du vil helt sikkert få andre verdier):

```{r}
n <- 10
rnorm(n)
```

Ved å skrive `mean(dnorm(n))` i stedet regner vi ut gjennomsnittet av observasjonene direkte. 

La oss gjøre dette 100 ganger og notere ned gjennomsnittet hver gang. I stedet for å gjøre det manuelt, kan vi skrive et lite program som gjør dette for oss ved å bruke en for-løkke. Det er ikke nødvendig (eller pensum) å forstå akkurat hvordan dette fungerer, men dersom du kjører følgende linjer vil du få en ny vektor `gj.snitt` som inneholder 100 slike gjennomsnitt:

```{r}
gj.snitt <- rep(NA, 100)
for(i in 1:100) {
    gj.snitt[i] <- mean(rnorm(n))
}
```

Skriv ut denne vektoren og kontroller at det ser korrekt ut. Vi husker at funksjonen `sd()` regner ut standardavviket til en vektor. Hvilket tall forventer du å få ut dersom du nå kjører `sd(gj.snitt)` i konsollen? Stemmer det? 

<details><summary>Hint</summary>

Standardavviket til de enkelte observasjonene er $\sigma = 1$, og standardavviket til et gjennomsnitt bestående av 10 observasjoner er $\sigma/\sqrt{n} = 1/\sqrt{10} \approx 0.32$. Med andre ord skal det *empiriske* standardavviket `sd(gj.snitt)` være omtrent lik 0.32, pluss/minus en estimeringsfeil.

</details>

Du kan gjerne regne ut 1000 gjennomsnitt i stedet for 100 ved å erstatte erstatte `100` med `1000` på to steder i koden over. Stemmer det bedre da?

<details><summary>Hint</summary>

```{r, eval = F}
gj.snitt <- rep(NA, 1000)            # Lager en tom vektor med 1000 plasser
for(i in 1:1000) {                   # Fyller hver plass med et gjennomsnitt av
    gj.snitt[i] <- mean(rnorm(n))    # 10 standard normalfordelte observasjoner.
}
```

</details>

Prøv å forklare.

<details><summary>Svar</summary>

Dette er ganske enkelt, men også litt vanskelig på en *inception*-aktig måte. På samme måte som at gjennomsnittet blir en mer og mer presis estimator for forventningsverdien når vi øker antall observasjoner (målt ved at standardavviket $\sigma/\sqrt{n}$ blir mindre når antall obserasjoner $n$ blir større), blir det empiriske standardavviket en mer og mer presis estimator av det sanne standardavviket når vi øker antall observasjoner. Altså; det empiriske standardavviket har *også* et standardavvik som går mot null som $1/\sqrt{n}$ `r knitr::asis_output("\U1F635")` 

</details>

## Oppgaver

### Standard oppgaver


1. Beskriv kort forskjellen mellom deskriptiv statistikk og statistisk inferens. 

<details><summary>Løsning</summary>

>Beskrivende statistikk bearbeider og presenterer data for å belyse faktiske forhold. Statistisk inferens, også kalt slutningsstatistikk, er å gjøre slutninger om populasjonen basert på det vi observerer i utvalget. 

</details>

2. Du skal spille kron og mynt. Motspilleren din er eieren av mynten, og hun påstår at den er rettferdig. Det vil si at om man flipper mynten svært mange ganger vil den gi et likt antall kron som mynt. 

    a.	Beskriv et eksperiment som tester denne påstanden. 
    b.	Hva er populasjonen i eksperimentet?
    c.	Hva er utvalget?
    d.	Hva er parameteren? 
    e.	Hva er estimatoren? 
    f.	Beskriv kort hvordan statistisk inferens kan bli brukt til å teste påstanden til motspilleren din. 

<details><summary>Løsning</summary>

>a.	Kast mynten ett gitt antall ganger, for eksempel 100 ganger. Registrer hvor mange ganger den viser mynt og hvor mange ganger den viser kron. 
b.	Populasjonen er det teoretiske resultatet av å kaste mynten uendelig mange ganger og registrere hvor mange ganger den viser mynt og hvor mange            ganger den gir kron. 
c.	Utvalget er antall mynt og kron i eksperimentet. 
d.	Parameteren er andelen mynt (eller kron) i populasjonen.
e.	Estimatoren er den registrerte andelen mynt (eller kron) i eksperimentet.
f.	Estimatoren kan brukes til å vurdere om mynten faktisk er rettferdig. I neste modul av kurset skal dere lære om hypotesetesting og om hvilken               hypotesetest man kan utføre for å avgjøre om man kan forkaste nullhypotesen om at mynten er rettferdig. 

</details>

3. En fabrikant av maskindeler påstår at mindre enn 15% av produktene er defekte. Når 1000 deler ble trukket fra en stor produksjon, var 12% defekte. 

    a.	Hva er populasjonen i dette tilfellet? 
    b.	Hva er utvalget? 
    c.	Hva er parameteren? 
    d.	Hva er estimatoren? 
    e.	Forklar kort hvordan estimatoren kan bli brukt til å gjøre inferens om parameteren for å undersøke påstanden om at 15% av produktene er defekte.

<details><summary>Løsning</summary>

>a.	Populasjonen er alle de aktuelle maskindelene som lages av fabrikanten. 
b.	Utvalget er de 1000 delene som ble trukket fra en stor produksjon. 
c.	Parameteren er andelen defekte maskindeler i alle de aktuelle maskindelene som produseres. 15% er den påståtte parameter. 
d.	Estimatoren er andelen defekte maskindeler i utvalget. 12% er den observerte estimatoren. 
e.	Vi kan estimere at andelen defekte maskindeler i populasjonen er 12%. Ved å bruke statistisk inferens kan vi undersøke om vi har nok statistisk bevis til å avvise påstanden om at den sanne andelen defekte maskindeler er 15%. 


</details>


4. Avgjør om de følgende datasettene inneholder forholdsdata, intervalldata, ordinale data eller dikotome og nominale data. Avgjør også hvilke regneoperasjoner som er tillatt. 

    a.	Antall mil en gruppe joggere løper hver uke 
    b.	Starlønnen til nyutdannede fra masterprogrammet til NHH 
    c.	Månedene et selskaps ansatte velger å ta ut ferie
    d.	Bokstavkarakterene til studentene i MET4
    e.	De daglige aksjeprisene til Tesla
    f.	Klesstørrelser (S, M, L)
    g.	Antallet Toyota som er importert månedlig til USA de siste 5 årene
    h.	Stillingene i Kjernestyret i NHHS (Leder, Prosjektansvarlig, Internansvarlig osv)

<details><summary>Løsning</summary>

>  a.	Intervalldata: =, ≠, <, >, +, -
    b.	Intervalldata: =, ≠, <, >, +, -
    c.	Nominale data: =, ≠
    d.	Ordinale data: =, ≠, <, >
    e.	Intervalldata: =, ≠, <, >, +, -
    f.	Ordinale data: =, ≠, <, >
    g.	Intervalldata: =, ≠, <, >, +, -
    h.	Nominale data: =, ≠


</details>

5. Skattebetalere som fyller ut egenmeldingen sin blir spurt de følgende spørsmålene. Hvilken type data utgjør svarene og hvilke regneoperasjoner er tillatt?
    a.	Er det første gang du fyller ut egenmeldingen din?
    b.	Hvor lang tid brukte du på å fylle ut egenmeldingen? 
    c.	Hvor enkelt/vanskelig var det å fylle ut egenmeldingen? (Veldig enkelt, nokså enkelt, hverken enkelt eller vanskelig, nokså vanskelig, veldig vanskelig) 

<details><summary>Løsning</summary>

 >  a.	Nominale data: =, ≠
   b.	Intervalldata: =, ≠, <, >, +, -
   c.	Ordinale data: =, ≠, <, >

</details>

6. Du får oppgitt at gjennomsnittlig startlønn for NHH studenter etter endt utdanning er 560 000kr. Median startlønnen er 520 000kr. Hva forteller disse tallene deg? 

<details><summary>Løsning</summary>

>Selv om den gjennomsnittlige startlønnen er 560 000kr, så får halvparten av arbeidstakerne 520 000kr eller mindre. Det forteller også at det er noen arbeidstakere som drar gjennomsnittet opp. 

</details>


7. En videregående skole rapporterer at gjennomsnittsfraværet i antall dager i løpet av ett skoleår er 10 dager. Medianen er 3,5 dager. Hva forteller de to målene oss, og hvilke av de to beskriver best fraværet? Hvorfor? 

<details><summary>Løsning</summary>

>Det gjennomsnittlige fraværet er høyere enn medianen. Dette forteller oss at noen elever har høyt fravær som drar gjennomsnittet opp. Disse elevene er ikke representative for det store flertallet. Halvparten av elevene har 3.5 eller mindre dagers fravær. Derfor er medianen et mer robust sentermål, som ikke blir påvirket av store uteliggere.   

</details>

8. Du skal ta en språktest før utveksling og kan velge mellom to ulike språkkurs som skal forberede deg til testen. Begge kursene har rapportert kvartilene til testresultatene oppgitt i antall poeng studentene deres fikk på testene: 

| Kvartil	| Kurs nummer 1	| Kurs nummer 2 |
|:----------:|:------:|:------:|
|Første kvartil	 |  230	| 225 |
| Andre kvartil  |	240 |	235 |
| Tredje kvartil | 	250 |	270 |

Hvordan tolker du kvartilene og hvilket kurs ville du ha valgt? 

<details><summary>Løsning</summary>

>Kvartilene kan fortelle oss noe om nivået og variasjonen i poengsummene til studentene som har tatt de to kursene. Vi ser at kurs nummer 1 har en symmetrisk fordeling med en median som er høyere enn kurs nummer 2. På en annen side har poengfordelingen til kurs nummer 2 en lengre høyre hale hvor tredje kvantilen tilsier at hele 25 % vil få 270 poeng eller mer. Det tilsvarende tallet for kurs nummer én er 250. Samlet sett er det kanskje "tryggere" med kurs nummer 1, mens det potensielle læringsutbytte har "større tak" i kurs nummer to.

</details>



9. MET4 ble et år rettet av to sensorer og et boxplot av poengene de ga sine respektive kandidater var som følger:

```{r, warning=F, echo=F, message=F}
library(ggplot2)
library(tidyverse)
set.seed(123)
n <- 200
Sensor1 <- rnorm(n, mean = 60, sd = 20)
Sensor2 <- c(rnorm(n*(2/4), mean = 40, sd = 20), rnorm(n*(2/4), mean = 85, sd = 20))

df <- data.frame(poeng = c(Sensor1, Sensor2), sensor =factor( rep(c("Sensor1", "Sensor2"), each = n)))

df %>% 
  filter(poeng > 0, poeng < 100) %>% 
ggplot() +
  geom_boxplot(aes(y = poeng, x = sensor, color = sensor))

```

Hva kan du si karakterfordelingen til de to sensorene?

<details><summary>Løsning</summary>
 
>	De to sensorene har begge en median rundt 60 poeng, så halvparten av alle studenter får under 60 poeng og den andre halvdelen får over 60 poeng. Men Sensor 2 har større spredning i sine poeng og vil følgelig fordele studentene ut mer på karakterskalaen sammenlignet med Sensor 1.  

</details>

10. Du er kvalitetsansvarlig for et produkt og samler inn data på levetiden i måneder til produktet. Et histogram over disse dataene ser ut som følger:

```{r, warning=F, echo=F, message=F}
library(ggplot2)
set.seed(12345)
levetid <- c(rep(1, 40), rep(2,30), rep(3, 25), rep(4, 20), rep(5, 12), 
             rep(6, 10),rep(7, 5), sample(seq(8:100), size = 200, replace = T), rnorm(400, mean = 120, sd = 20))

ggplot(data = data.frame(levetid = levetid)) +
  geom_histogram(aes(x = levetid)) + 
  xlab("levetid i antall måneder") +
  ylab("antall")
```

Hva sier histogrammet deg om produktet?

<details><summary>Løsning</summary>
 
>	Her ser vi en slags badekar-formet fordeling som ofte går igjen for levetiden til produkter. En rekke produkter går i stykker de første månedene. Dette kan være pga av produksjonsfeil som da slår ut tidlig i bruksperioden. Resten av produktene har en mer normalfordelt levetid med et gjennomsnitt rundt 120 måneder.  

</details>



11. Kovariansen til to variabler fra et utvalg har blitt beregnet til -150. Videre får du vite at empirisk standardavviket til den ene variabelen er 12 og 16 for den andre. Regn ut (utvalgs-) korrelasjonskoeffisienten $r$ og bruk denne til å beskrive sammenhengen mellom variablene. 

<details><summary>Løsning</summary>
 
>	Utvalgs korrelasjonskoeffisient er definert som utvalgskovariansen mellom to variabler delt på de empiriske standardavvikene til variablene. 

$$r=s_{xy}/(s_x s_y )=-150/(16\cdot12)=-0,7813$$

>Korrelasjonskoeffisienten er alltid mellom -1 og 1 og måler graden av lineær sammenheng mellom variablene. Når korrelasjonskoeffisienten er -1 er det en perfekt negativ lineær sammenheng, og når den er 1 vil det si at det er en perfekt positiv lineær sammenheng mellom variablene. Når korrelasjonskoeffisienten er 0 vil det si at ikke er noen lineær sammenheng mellom variablene, men vi kan allikevel ikke si at de er uavhengige. Korrelasjonskoeffisienten -0,78 indikerer en moderat til sterk negativ lineær sammenheng mellom variablene. 

</details>

12. Betrakt spredningsplottene for datasett a-d under, og par hvert datasett med en av de følgende korrelasjonene:

$$r_1 = -0.37,\quad r_2 = 0.81,\quad r_3 = -0.02,\quad r_4 = -0.96 $$


```{r, warning=F, echo=F, message=F}
set.seed(124)
n <- 150
x1 <- rnorm(n)
y1 <- x1 + rnorm(n, sd = 0.7)
y2 <- -0.5*x1 + rnorm(n, sd = 1)
y3 <- -0.7*x1 + rnorm(n, sd = 0.2)
y4 <- x1^2 + rnorm(n, sd = 0.5)

df <- data.frame(x = rep(x1, 4), y = c(y1, y2, y3, y4), figur = factor(rep(c("a", "b", "c", "d"), each = n)))
ggplot(df) +
  geom_point(aes(x = x, y = y)) +
  facet_wrap(~figur, scales = "free_y")

# cor(x1,y1)
# cor(x1, y2)
# cor(x1, y3)
# cor(x1, y4)

```

<details><summary>Løsning</summary>
 
>	Datasett a: $r_2 = 0.81$, Datasett b: $r_1 = -0.37$, Dasett c: $r_4 = -0.96$, Datasett d: $r_3 = -0.02$

> Datasett d er et eksempel på at korrelasjonen mellom to variabler kan være svært nær 0, selv om det er en tydelig (ikke-lineær) sammenheng mellom dem. Det som skjer når vi regner ut $r$ i dette tilfellet er at den negative avhengigheten vi ser til venstre i figuren kanselleres av den positive avhengigheten til høyre i figuren. 

</details>

13. En normalfordelt populasjon har gjennomsnitt 40 og standardavvik 12. 

  a.  Hva sier sentralgrenseteoremet om gjennomsnittet av et utvalg på 100 observasjoner fra denne populasjonen? 
  b.  Dersom populasjonen ikke var normalfordelt, hvordan endrer dette svaret ditt i a?
  
<details><summary>Løsning</summary>

> a. Dersom populasjonen er normalfordelt så vil også gjennomsnittet av et utvalg av populasjonen være normalfordelt. Gjennomsnittet vil ha forventning $40$ og ha standardavvik $12/\sqrt{100} = 1.2$.
 b. Sentralgrenseteoremet sier at fordelingen til gjennomsnittet vil for alle praktiske formål nærme seg normalfordelingen når antall observasjoner øker, uavhengig av fordelingen til populasjonen. Dette endrer altså ikke konklusjonen om fordelingen til utvalget. 
 
</details>  


14. La $X_1, X_2,\dots X_n$ være utfallet av en rekke kast med en tilfeldig terning.

  a. Hva er $P(X_1 = 1)$ og $P(X_1 = 6)$?
  b. Hva er $P(\overline{X} = 1)$ og $P(\overline{X} = 6)$ dersom $n = 2$?
  c. Hva er $P(\overline{X} = 1)$ og $P(\overline{X} = 6)$ dersom $n = 5$?

<details><summary>Løsning</summary>
 
>	a. $P(X_1 = 1) = P(X_2 = 6) = 1/6$
  b. Det er bare en måte at gjennomsnittet av to kast kan bli $1$ og det er at begge kastene blir $1$: $P(\overline{X} = 1) = P(X_1 = 1, X_2 = 1) = P(X_1 = 1)P(X_2 = 1) = 1/36$, og tilsvarende for $P(\overline{X} = 6)$.
  c.  Det er bare en måte at gjennomsnittet av 5 kast kan bli $1$ og det er at alle fem kastene blir $1$:$P(\overline{X} = 1) = P(X_1 = 1)P(X_2 = 1)P(X_3 = 1)P(X_4 = 1)P(X_5 = 1) = 0.00013$, og tilsvarende for $P(\overline{X} = 6)$.

> Denne oppgaven illustrerer hvordan gjennomsnittet, uavhengig av fordelingen til $X$ som i dette tilfellet er uniformt fordelt på ${1,2,3,4,5,6}$, er normalfordelt $\overline{X}\sim N(\mu, \sigma^2/n)$ og beveger seg mot forventningen til $X$ (som i dette tilfellet er $\mu = 3.5$). Det at variansen til gjennomsnittet $\sigma^2/n$ blir mindre når $n$ øker ser vi er svært logisk her: Det er værre for gjennomsnittet til terningkastene å oppnå "ekstreme" verdier som $\overline{X} = 1$ eller $\overline{X} = 6$ siden dette innebærer at alle kastene må enten være 1 eller 6. Derimot er det en hel rekke kombinasjoner av kast som gir et gjennomsnitt nær $3.5$, så slike verdier av gjennomsnittet er mye mer sannsynlig.  

>Under ser du et histogram over gjennomsnittene dersom vi gjentar terningkasteksperimentet med henholdsvis $n=1,2, 5$ kast 2000 ganger. Allerede for $n=5$ ser vi at de fleste gjennomsnitt ligger nær det sanne gjennomsnittet på 3.5.  

```{r, echo = F}
set.seed(123)
n <- 5000
X1 <- sample(seq(6), size = n, replace = T)
X2 <- rep(NA, n)
X3 <- rep(NA, n)
for(i in 1:n){
  foo1 <- sample(seq(6), 2)
  foo2 <- sample(seq(6), 5)
  X2[i] <- mean(foo1)
  X3[i] <- mean(foo2)
}

df <- data.frame(means = c(X1, X2, X3), 
                 kast = rep(c("n = 1", "n = 2", "n = 5"), each = n))

ggplot(df) + 
  geom_histogram(aes(x = means), bins = 15) +
  facet_grid(~kast)

```


</details>

15. La $\hat{p}=X/n$ være den estimerte suksessannsynligheten i et binomisk eksperiment $X$ med $n=300$ forsøk og (populasjons) suksessannsynlighet $p$.

a.	Hva er sannsynligheten for at $\hat{p}$ er høyere enn 60%  dersom $p = 0.5$? 
b.	Gjenta oppgave a med $p = 0.55$.
c.	Gjenta oppgave a med $p = 0.60$.


<details><summary>Løsning</summary>

> Her bruker vi sentralgrenseteoremet som sier at $\hat{p}\sim N(p, p(1-p))$:

>	a.
$$P(\hat{p} > 0.60) = P\left(\frac{\hat{p} - p}{\sqrt{p(1 - p)/n}} > \frac{0.60 - 0.5}{\sqrt{0.5(1 - 0.5)/300}}\right) = P(Z > 3.46)\approx 0$$.

>	b. 
\begin{equation*}
\begin{split}
P(\hat{p} > 0.60) &= P\left(\frac{\hat{p} - p}{\sqrt{p(1 - p)/n}} > \frac{0.60 - 0.55}{\sqrt{0.55(1 - 0.55)/300}}\right)\\
&= P(Z > 1.74)= 1 - P(Z < 1.74) = 1 - 0.9591 = 0.0409.
\end{split}
\end{equation*}

>c.
\begin{equation*}
\begin{split}
P(\hat{p} > 0.60) &= P\left(\frac{\hat{p} - p}{\sqrt{p(1 - p)/n}} > \frac{0.60 - 0.60}{\sqrt{0.60(1 - 0.60)/300}}\right)\\
&= P(Z > 0)= 0.5\quad\text{(p.g.a. $Z$'s symmetri rundt 0)}.
\end{split}
\end{equation*}

> Vi ser at for $n=300$ vil $\hat{p}$ sin normalfordeling være svært konsentrert rundt $p$ (variansen er $p(1-p)/n$) og at en overstigelse på 0.05 eller mer fra den sanne suksessannsynligheten (som i oppgave a.) er svært usannsynlig.

</details>

16. Anta at vi har to normalfordelte populasjoner hvor observasjoner fra populasjon 1 følger en $N(40, 6^2)$ og observasjoner fra populasjon 2 følger en $N(38, 8^2)$. Dersom man trekker 25 tilfeldige observasjoner fra hver populasjon, hva er sannsynligheten for at gjennomsnittet til trekningen fra populasjon 1 er større enn gjennomsnittet til trekningen fra populasjon 2?


<details><summary>Løsning</summary>

> Vi vet da at $\overline{X}_1 - \overline{X}_2 > 0$ og utnytter at denne størrelsen er normalfordelt:
\begin{equation*}
\begin{split}
P(\overline{X}_1 - \overline{X}_2 > 0) &= P\left(\frac{\overline{X}_1 - \overline{X}_2 - (40 - 38)}{\sqrt{\frac{6^2}{25} + \frac{8^2}{25}}} > \frac{0 - (40 - 38)}{\sqrt{\frac{6^2}{25} + \frac{8^2}{25}}} \right)\\
&= P(Z > -1) = 1 - P(Z < -1) = 1 - 0.1587 = 0.8413
\end{split}
\end{equation*}
 
</details> 

17. Produsenten av en kaviar forteller deg at hver tube er reklamert til å veie 65 gram, men at maskinen som produserer kaviaren gir en vekt som er en normalfordelt med gjennomsnitt lik 65,5 gram og standardavvik 3.6 gram. La oss si at du trekker et tilfeldig utvalg av 40 tuber for å undersøke denne påstanden og at det tilfeldige utvalget har en gjennomsnittlig vekt lavere enn 64. 

a. Hva er sannsynligheten for dette utfallet?
b. Kommenter påstanden til produsenten.

<details><summary>Løsning</summary>

> a. $$P(\overline{X} < 64) = P\left(\frac{\overline{X} - 65.5}{3.6/\sqrt{40}} < \frac{64 - 65.5}{3.6/\sqrt{40}}\right) = P(Z < - 2.63) = 0.004$$
b. Det er svært usannsynlig at vi observerer et gjennomsnitt mindre enn 64 dersom det er sant at maskinen produserer tuber som er normalfordelt med gjennomsnitt 65.5 og standardavvik 3.6. Så påstanden er nok feil. Enten produserer maskinen vekter som er mindre, eller så er standardavviket større.

</details>

18. En selger av robotgressklippere påstår at bare 4% av produktene må på service innen det første året etter installasjon. Du skal undersøke om påstanden stemmer og spør et tilfeldig utvalg av 100 husholdninger som har kjøpt robotgressklippere om de hadde den på service det første året. 

a. Hva vil du si om selgerens påstand dersom mer enn 6% svarer at de har hatt behov for service?
b. Anta nå at du istedet spurte 400 hustander. Hva vil du si om selgerens påstand dersom mer enn 6% svarer at de har hatt behov for service?

<details><summary>Løsning</summary>


> a. Da har vi altså en estimert service sannsynlighet på $\hat{p} = 0.06$ eller mer. For å finne ut om dette avviket fra 0.04 skyldes naturlig variasjon eller om den faktiske service raten er større enn 0.04 regner vi ut sannsynligheten for dette utfallet:

>
\begin{equation*}
\begin{split}
P(\hat{p}>0.06) &= P\left(\frac{\hat{p} - 0.04}{\sqrt{0.04(1-0.04)/100}}> \frac{0.06 - 0.04}{\sqrt{0.04(1-0.04)/100}}\right)\\
&= P(Z > 1.02) = 1 - P(Z < 1.02) = 1 - 0.8561 = 0.1539 
\end{split}
\end{equation*}

>Det er altså ikke så usannsynlig at $\hat{p} = 0.06$ eller mer. Vi kan se for oss at vi gjentar den samme spørreundersøkelsen mange ganger. Da kan vi forvente at rundt 15 % av undersøkelsene gi en $\hat{p} = 0.06$ ved ren tilfeldighet selv om populasjonsverdien er $p=0.04$. Selgerens påstand er ikke urimelig ved et slikt resultat.

>b. Sannsynligheten for et slikt utfall er da
\begin{equation*}
\begin{split}
P(\hat{p}>0.06) &= P\left(\frac{\hat{p} - 0.04}{\sqrt{0.04(1-0.04)/300}}> \frac{0.06 - 0.04}{\sqrt{0.04(1-0.04)/300}}\right)\\
&= P(Z > 2.04) = 1 - P(Z < 2.04) = 1 - 0.9793 = 0.02 
\end{split}
\end{equation*}

> Nå har vi spurt flere husstander og vi ser at den samme estimerte service sannsynligheten (6 %) nå er mye mer usannsynlig å få dersom det faktisk er slik at bare 4 % trenger service. Selgerens påstand ville altså vært mer tvilsom ved et slik resultat.

</details>


19. Forventningsrette estimatorer.
a. Hva vil det si at en estimator er forventningsrett?
b. Vis at hvis $X_1, X_2,\dots X_n$ er observasjoner fra en populasjon der $E(X_i) = \mu$ så er gjennomsnittet $\overline{X}$ en forventningsrett estimator av $\mu$.
c. Vis at hvis $X$ er et binomisk forsøk med $n$ forsøk og suksessannsynlighet $p$ så er $\hat{p} = X/n$ en forventningsett estimator av $p$. 

<details><summary>Løsning</summary>

> a. Hvis $\hat{\theta}$ er en estimator av $\theta$ så er $\hat{\theta}$ en forventningsrett estimator dersom $E(\hat{\theta}) = \theta$.  
b.
\begin{equation*}
\begin{split}
E(\overline{X}) &= E\left(\frac{1}{n}(X_1 + X_2 + .. + X_n)\right) = \frac{1}{n}(E(X_1) + E(X_2) + ... + E(X_n))\\
&= \frac{1}{n}(\mu + \mu + ... + \mu) = \frac{1}{n}\cdot n\mu = \mu
\end{split}
\end{equation*}
c. Vi husker at en binomisk variabel $X$ har forventning $np$. Altså er 
$$E\left(\frac{X}{n}\right) = \frac{1}{n}E(X) = \frac{1}{n}\cdot np = p$$
</details>

20. Konsistente estimatorer.
a. Hva vil det si at en estimator er konsistent?
b. Vis at hvis $X_1, X_2,\dots X_n$ er observasjoner fra en populasjon der $\text{Var}(X_i) = \sigma^2$ så er $\text{Var}(\overline{X}) = \sigma^2/n$. Bruk dette resultatet sammen med resultatet i 19 b til å forklare at $\overline{X}$ er en konsistent estimator av $\mu$.  
c. Vis at hvis $X$ er et binomisk forsøk med $n$ forsøk og suksessannsynlighet $p$ så er $\text{Var}(\hat{p}) = p(1-p)/n$. Bruk dette resultatet sammen med resultatet fra 19 c til å forklare at $\hat{p}$ er en konsistent estimator av $p$. 

<details><summary>Løsning</summary>

> a. En estimator er konsistent hvis forskjellen mellom estimatoren og parameteren blir mindre når størrelsen på utvalget øker.  
b.  
\begin{equation*}
\begin{split}
\text{Var}(\overline{X}) &= \text{Var}\left(\frac{1}{n}(X_1 + X_2 + .. + X_n)\right) = \left(\frac{1}{n}\right)^2(\text{Var}(X_1) + \text{Var}(X_2) + \dots + \text{Var}(X_n))\\
&= \left(\frac{1}{n}\right)^2(\sigma^2 + \sigma^2 + ... + \sigma^2) = \left(\frac{1}{n}\right)^2\cdot n\sigma^2 = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
Vi vet fra før at $E(\overline{X}) = \mu$. Vi ser av utrykket over at variansen til $\overline{X}$ avtar når $n$ øker altså vil forskjellen mellom $\overline{X}$ og $\mu$ bli mindre jo større utvalg vi får. 
c. Vi husker at en binomisk variabel har varians $\text{Var}(X) = np(1 - p)$. Altså er 
$$\text{Var}(\hat{p}) = \text{Var}\left(\frac{X}{n}\right) = \left(\frac{1}{n}\right)^2 \cdot\text{Var}(X) = \left(\frac{1}{n}\right)^2\cdot np(1 - p) = \frac{p(1 - p)}{n}$$
Igjen ser vi at variansen avtar når $n$ øker altså vil forskjellen mellom $\hat{p}$ og $p$ bli mindre jo større utvalg vi får.

</details>

### Nøtter



Nøtt 1. Korrelasjon er enten knyttet til et bestemt utvalg, eller en populasjon/modell, men vi sier sjelden "utvalgskorrelasjon" eller "populasjonskorrelasjon" og vi må ofte bare forstå dette ut fra situasjonen eller notasjonen ($r$ versus $\rho$). Men utvalgskorrelasjonen  

$$r = s_{xy}/(s_xs_y)=\frac{\frac{1}{n-1}\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\frac{1}{n-1}\sum_{i = 1}^n(x_i - \overline{x})^2\frac{1}{n-1}\sum_{i = 1}^n(y_i - \overline{y})^2}}$$

et altså en  *estimator* av populasjonskorrelasjonen 

$$\rho = \frac{\text{cov}(X,Y)}{\sigma_x\sigma_y} = \frac{E\left[(X - E(X))(Y - E(Y))\right]}{\sigma_x\sigma_y}$$.

Det betyr i praksis at $r$ kommer med en usikkerhet siden den er basert på data. Det kan vises at dersom den simultane fordelingen til $X$ og $Y$ er en såkalt *bivariat normalfordeling* så vil vi for store utvalg ha at samplingfordelingen til $r$ er tilnærmet normalfordelt med forventning $\rho$ og standardavvik $\frac{(1 - \rho^2)}{\sqrt{n}}$:

$$r\sim N\left(\rho, \frac{(1 - \rho^2)^2}{n}\right)$$

Anta at $X$ og $Y$ er bivariat normalfordelt.

a. Forklar at $r$ er en konsistent estimator av $\rho$.

b. Hva kjennetegner populasjoner hvor $\rho$ kan estimeres med stor sikkerhet ved hjelp av $r$ fra et utvalg?

c. Beskriv en test av $H_0: \rho = 0$, mot $H_1: \rho \neq 0$.


<details><summary>Løsning</summary>
 
>	a. Vi ser at når $n$ blir stor går standardavviket til $r$ mot null, og $r$ vil derfor nærme seg den sanne korrelasjonen $\rho$ i populasjonen.

>b. Vi ser også at standardavviket til $r$ er mindre jo nærmere $\rho$ er $\pm 1$. Utregninger av $r$ basert på utvalg fra 
populasjoner med sterk positiv eller negativ lineær avhengighet ($\rho$ nær $\pm 1$) kommer derfor med mindre usikkerhet.  

>c. En test observator som vil være tilnærmet normalfordelt er da: 

>$$Z^* = \frac{r - 0}{\frac{(1 - r^2)}{\sqrt{n}}}$$
og ved et $5\%$ signifikansnivå forkaster vi $H_0$ dersom $|Z^*|>1.96$. Merk at sammenlignet med det sanne standardavviket har vi her erstattet $\rho$ med $r$. Dette er litt som når vi erstatter $\sigma$ med $s$ i den vanlige $Z$ observatoren ($T$ observatoren). Pga av sentralgrenseteoremet går dette fint for store utvalg.
>Obs: Det kommer mer om hypotesetesting i neste modul, så her foregriper vi begivenhetene litt.


</details>


Nøtt 2. Anta at vi har følgende deterministiske sammenheng mellom variablene $X$ og $Y$:

$$Y = X^2$$

og at $X$ er normalfordelt med forventning $0$. Bruk definisjonen på kovarians til å vise at $\text{cov}(X,Y)=0$ og at korrelasjonen derfor også vil være $0$ mellom disse variablene. Hint: Hvis $X$ er symmetrisk fordelt og har forventning 0, så er $E(X^3) = 0$.

Hva sier dette resultatet oss om korrelasjon? 

<details><summary>Løsning</summary>
 
>	Bruk definisjonen, erstatt $Y$ med $X^2$ og utnytt at $E(X) = E(X^3) = 0$: 

\begin{equation*}
\begin{split}
\text{cov}(X,Y) &= E(XY) - E(X)E(Y)\\
&= E(X^3)- E(X)E(X^2)= 0 + 0*E(X^2)=0
\end{split}
\end{equation*}

>  Dette er et ekstremt eksempel der avhengigheten mellom $X$ og $Y$ er deterministisk (Vet du $X$ så vet du $Y$ med 100 % sikkerhet), men korrelasjonen $\rho = \text{cov}(X,Y)/(\sigma_X\sigma_Y)$ mellom variablene er allikevel 0. Altså er det fullt mulig at $X$ og $Y$ er avhengige selv om korrelasjonen mellom $X$ og $Y$ er null.

</details>

Nøtt 3. Rulett er et sjansespill der et ruletthjul har 37 nummererte lommer der en kule kan lande. 18 av disse lommene er røde, 18 er svarte og 1 er grønn (iallefall i Europa). Hvis du f.eks satser 1 krone på rød og rød intreffer så vinner vi kronen vår tilbake og 1 krone til (1 krone i gevinst), mens hvis svart eller grønn intreffer så taper vi hele kronen (-1 krone i gevinst). 

a.  Du satser 1 krone på rød.  Hva er forventet gevinst? Hva er variansen til denne gevinsten? Hva er sannsynligheten for en positiv gevinst?
b. Du fortsetter denne strategien $n = 50$ ganger. Bruk sentralgrenseteoremet til å svare på spørsmålene i a. for den gjennomsnittlige gevinsten. 
c. Finnes det en (teoretisk) satsingsstrategi for å sikre positiv gevinst uansett?

<details><summary>Løsning</summary>
>a. La $X$ være gevinsten. Dette er en diskret variabel som bare kan ta to utfall, og den har følgende sannsynlighetsfordeling: 

|   |   |   |
|---|---|---|
|$x_i$   | 1  |  -1 |
|P(X = x_i)   | 18/37  |  19/37 | 

> For diskret fordelinger som dette har vi at
$$\mu = E(X) = \sum_{i = 1}^2 x_i P(X = x_i) = 1\cdot(18/37) + (-1)\cdot(19/37) = -0.027$$
For å finne variansen finner vi først 
$$E(X^2) = \sum_{i = 1}^2 x_{i}^2 P(X = x_i) = 1^2\cdot(18/37) + (-1)^2\cdot(19/37) = 1$$
og bruker regneregelen
$$\sigma^2 = \text{Var}(X) = E(X^2) - (E(X))^2 = 1 - (-0.027)^2 = 0.999$$
Sannsynligheten for positiv gevinst svarer til sannsynligheten for at ballen treffer rød, altså $18/37 = 0.4864$.

>b. Sentralgrenseteoremet sier at når vi spiller et relativt stort antall runder så er $\overline{X}$ normalfordelt med forventning $E(\overline{X}) = \mu = -0.027$ og varians $\text{Var}(\overline{X})=\sigma^2/n = 0.999/50 = 0.02$. Sannsynligheten for at den gjennomsnittlige gevinsten er positiv blir altså
\begin{equation*}
\begin{split}
P(\overline{X}>0) &= P\left(\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} > \frac{0 - \mu}{\sigma/\sqrt{n}}\right) = P\left(Z > \frac{0 - (-0.027)}{\sqrt{0.02}}\right)\\
&= P(Z>0.19) = 1 - P(Z < 0.19) = 0.4246
\end{split}
\end{equation*}
Kasinoer vet med andre ord å utnytte sentralgrenseteoremet. Har du noen penger (som du har råd til å tape) så sett hele beløpet på en farge og spill 1 gang, framfor å plassere småbeløp over lang tid.  

>c. Det finnes noen strategier som kan sikre positiv gevinst, men som i praksis krever svært mye egenkapital og at det ikke er noe tak for hvor mye du kan satse ved bordet. Den såkalte *Martingal* strategien går ut på at du dobler innsatsen etter hvert tap og gir deg ved første gevinst. Gevinsten vil da svare til det første beløpet du satset.  
</details>

## Relevante R-kommandoer {#relevant-r-grunnleggende}

```{r, eval = FALSE, echo = FALSE}
# Lage testfilene:

library(tidyverse)
library(readxl)
library(openxlsx)

set.seed(1)

data <- tibble(variabel1 = rnorm(20),
               variabel2 = rnorm(20) + 1)

write.xlsx(x = data, file = "datasett/data.xlsx")
write.csv(data, file = "datasett/data.csv", 
          sep = ",", 
          quote = FALSE, 
          row.names = FALSE)

```


Under følger en liste over hvilke oppgaver du skal klare i R fra denne modulen. Vår policy fra og med **vårsemesteret 2022** er at R-kommandoene under er *tilstrekkelige* for å løse oppgavene i datalabber og hjemmeeksamen i MET4. Det er med andre ord ikke nødvendig å lære seg teknikker utover det som er listet opp eksplisitt i listen under. Eventuelle nye teknikker som trengs for å løse en bestemt oppgave vil bli oppgitt og forklart dersom det er nødvendig. Det antas i tillegg at du kan den grunnleggende R-syntaksen som er dekket under [Introduksjon til R].

### Antakelser om datasett {-}

Det antas at du har kontroll på hvilken mappesti R bruker, og at du kan forandre den dersom nødvendig.

I denne modulen kan du anta at alle datasett består av $p$ kolonner (variabler) med variabelnavn og $n$ rader (observasjoner). Datasettene blir gitt enten som Excel-filer (`.xls` elle `.xlsx`) eller som `.csv`-filer. Dersom du ønsker å prøve på kommandoene selv, kan du laste ned følgende eksempelfiler: [data.xls](datasett/data.xls), [data.xlsx](datasett/data.xlsx), [data.csv](datasett/data.csv).

Excel-filer kan leses inn i R ved hjelp av `read_excel()` som ligger i `readxl`-pakken:

```{r, eval = FALSE}
library(readxl)

data <- read_excel("data.xls")
# ...eller dersom det er en .xlsx-fil:
data <- read_excel("data.xlsx")
```

`.csv`-filer kan lastes inn i R ved hjelp av `read_csv()` som ligger i `readr`-pakken:

```{r, eval = FALSE}
library(readr)
data <- read_csv("data.csv")
```

"csv" er en forkortelse for "comma separated values", og grunnen til det kan du se ved å åpne opp `data.csv` i en ren tekst-editor. Da ser du at verdiene på rekken er separert med et komma. Av og til har disse filene litt andre formater, så vi må kunne håndtere følgende varianter:

- Dersom det ikke er komma som er brukt til å separere verdiene (det kan du alltid sjekke ved å se på filen i en tekst-editor), så kan du bruke funksjonen `read_delim` i stedet for `read_csv`, og bruke et argument for å spesifisere hvilket tegn som er brukt. Dersom det for eksempel er brukt semikolon ";", kan du lese datasettet ved å bruke `read_delim("data.xlsx", delim = ";")`.
- Dersom komma er brukt for å skille desimaler fra heltall, som er vanlig i europeiske datasett, kan du bruke følgende kommando (der vi fortsatt antar at semikolon blir brukt til å skille mellom kolonner): `read_delim("data.xlsx", delim = ";", locale = locale(decimal_mark = ",")`.
- Dersom en bestemt tekst-streng blir brukt til å indikere manglende verdier i datasettet kan du bruke argumentet `na` i `read_csv()` og `read_delim()` til å spesifisere denne tekst-strengen, slik at den blir tolket riktig ved innlesingen som manglende verdi. Dette argumentet fungerer også i `read_excel()`.

### Numerisk deskriptiv statistikk {-}

Vi bruker eksempeldatasettene [data_survey.csv](datasett/data_survey.csv) og [data_fishermen_mercury.csv](datasett/data_fishermen_mercury.csv) som eksempler. Du skal kunne bruke følgende funksjoner for å regne ut numerisk deskriptiv statistikk:

```{r, eval = FALSE}
survey    <- read_csv("data_survey.csv")
fishermen <- read_csv("data_fishermen_mercury.csv")

mean(survey$age)                          # Gjennomsnitt
median(survey$age)                        # Median

table(survey$party)                       # Kan lese av typetallet manuelt.
which.max(table(survey$party))            # Eller bruke funksjonen which.max()

sd(fishermen$total_mercury)               # Standardavvik
quantile(fishermen$total_mercury, 0.20)   # Kvantiler
quantile(fishermen$total_mercury)         # Kvartilene

cor(fishermen$height, fishermen$weight)   # Korrelasjonskoeffisient
table(survey$religious)                   # Frekvenstabell 
survey %>%                                # Krysstabell av to variabler (krever
  select(party, religious) %>%            # pipe-operatoren og select()-funks.  
  table                                   # fra dplyr-pakken)

```

### Grafisk deskriptiv statistikk med `ggplot2` {-}

Du skal kunne lage følgende plott ved hjelp av `ggplot2`-pakken;

```{r, eval = FALSE}
library(ggplot2)

# Boksplott
fishermen %>% 
  ggplot +
  geom_boxplot(aes(y = total_mercury)) 

# Deler opp i grupper basert på en annen variabel
fishermen %>% 
  ggplot +
  geom_boxplot(aes(y = total_mercury, x = as.factor(fisherman))) 

# Søylediagram
survey %>% 
  ggplot +
  geom_bar(aes(x = as.factor(religious)))

# Histogram
survey %>% 
  ggplot + 
  geom_histogram(aes(x = age))

# Spredningsplott
fishermen %>% 
  ggplot + 
  geom_point(aes(x = height, y = weight))
```

Du skal kunne grunnleggende bearbeiding av `ggplot`-figurer:

```{r, eval = FALSE}
# En bearbeidet variant
fishermen %>% 
  ggplot + 
  geom_point(aes(x = height, y = weight)) +          # Dette er selve figuren
  xlab("Høyde") +                                    # Tittel x-akse
  ylab("Vekt") +                                     # Tittel y-akse
  ggtitle("Sammenheng mellom høyde og vekt") +       # Tittel plott
  theme_classic()                                    # Ryddig layout
```



<!-- I tabellen under finner du noen oppgaver som du kan bryne deg på for å sjekke forståelsen din og trene på metodene som vi har gått gjennom i denne modulen. Vi peker også på noen tidligere eksamensoppgaver som er relevante til denne tematikken, du finner oppgavene under seksjon \@ref(skoleeksamen). -->

<!-- Har du en eldre utgave av boken kan du laste ned [dette dokumentet](oppgaver/Recommended excercises.doc) for en oversikt over oppgavenummer tilbake til 7. utgave. -->

<!-- **På Canvas finner du en .zip-fil som inneholder alle datasettene, samt løsningsforslag til oppgavene i læreboken.** -->

<!-- ```{r, echo = FALSE, message = FALSE} -->

<!-- library(dplyr) -->
<!-- options(knitr.kable.NA = '') -->

<!-- "oppgaver/oppgaver-grunnleggende-statistikk.xlsx" %>%  -->
<!--     readxl::read_excel() %>%  -->
<!--     kableExtra::kbl() %>%  -->
<!--     kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed")) -->

<!-- ``` -->

