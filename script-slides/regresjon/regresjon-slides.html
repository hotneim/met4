<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MODUL 3 - REGRESJON</title>
    <meta charset="utf-8" />
    <meta name="author" content="H√•kon Otneim &amp; Geir Drage Berentsen" />
    <script src="libs/header-attrs-2.5/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: middle, left, inverse, title-slide

# MODUL 3 - REGRESJON
## MET4
### H√•kon Otneim &amp; Geir Drage Berentsen

---



class: center, middle, inverse

# Del I: ENKEL REGRESJON

---

## Innledning

- Ofte √∏nsker vi √• modellere variasjon (respons) i en m√•levariabel med variasjon i andre variabler (forklaringsvariabler) uten √• m√•tte begrense oss til kategorivariable

- Eksempel 1: `\(Y\)` er salg og `\(X\)` reklame:

`$$\textrm{E}(Y) = \beta_0 + \beta_1X$$`

- Eksempel 2: `\(Y\)` er produksjon, `\(L\)` er arbeidskraft og `\(K\)` er kapital  

`$$\textrm{E}(Y) = \beta_0 + \beta_1L + \beta_2K$$`

- Med utgangspunkt i data √∏nsker vi √• estimere de ukjente parameterene `\(\beta_0, \beta_1\)` og `\(\beta_2\)`, lage konfidensintervaller for estimatene og teste hypoteser om parameterverdiene
- Basert p√• estimatene kan vi ogs√• predikere `\(Y\)` for hypotetiske verdier av forklaringsvariablene og konstruere prediksjonsintervaller
- Vi skal n√• ta for oss minste kvadraters regresjonsanalyse (‚ÄúOLS‚Äù) med √©n forklaringsvariabel, s√•kalt enkel regresjon

---

## Et datasett

* For bruktbilforhandlere er ‚Äúlistepris‚Äù et viktig verkt√∏y for verdibestemmelsen av biler
* I USA utgis m√•nedlig en s√•kalt Red Book med gjennomsnittlig omsetningspris for ulike bilmodeller
* N√•r en spesifikk bil skal verdibestemmes m√• man imidlertid justere for kj√∏relengde
* **En forhandler √∏nsket √• gj√∏r en n√¶rmere analyse av sammenhengen mellom kj√∏relengde og oppn√•dd pris**
* **Han plukket tilfeldig ut 100 Toyota Camry i topp stand og med en bestemt utstyrspakke, som hadde v√¶rt solgt p√• bilauksjoner den siste m√•neden**
* Pris og kj√∏relengde ble notert (i datasettet Xm16-02)

---

## Formell statistisk analyse

- Det ser ut til √• v√¶re en line√¶r tendens i sammenhengen mellom pris og kj√∏relengde
- Hva er forventet pris for en gitt kj√∏relengde?
- F√∏lgende modell for sammenhengen synes rimelig
`\begin{align*}
&amp;\textrm{E(Pris)} = \beta_0 + \beta_1\cdot\textrm{miles} \\
&amp; \Rightarrow \textrm{Pris} =  \beta_0 + \beta_1\cdot\textrm{miles} + \epsilon, \,\,\, \textrm{ der } \,\,\, \textrm{E}(\epsilon) = 0
\end{align*}`
- Koeffisientene `\(\beta_0\)` og `\(\beta_1\)` kan bestemmes ved √• gj√∏re en regresjonsanalyse basert p√• *minste kvadraters metode*

---

## Den enkle line√¶re regresjonsmodellen

`$$\textrm{E}(Y|X) = \beta_0 + \beta_1X$$`
Gitt `\(n\)` sammenh√∏rende observasjonssett
`$$(Y_i, X_i), \,\,\, i = 1,\ldots,n$$`
kan vi skrive
`$$Y_i = \beta_0 + \beta_1X_i + \epsilon_i, \,\,\, i = 1,\ldots,n$$`

- Feilleddene `\(\epsilon_i\)` er stokastiske variable som antas innbyrdes uavhengige, med `\(\textrm{E}(\epsilon_i)= 0\)` og konstant varians `\(\sigma^2_{\epsilon}\)` (dermed er ogs√• `\(Y_i\)` stokastisk med betinget varians `\(\sigma_{\epsilon}^2\)`)
- `\(X\)` er deterministisk, evt. stokastiske og uavhengige av `\(\epsilon\)`
- Dersom vi i tillegg antar at `\(\epsilon_i\)`'ene er normalfordelte, kan vi gj√∏re inferens.

---

## Den enkle line√¶re regresjonsmodellen


- Vi √∏nsker √• estimere de ukjente parametrene `\(\beta_0\)` og `\(\beta_1\)` basert p√• observasjoner av `\(Y\)` som varierer tilfeldig omkring sin forventning, `\(\textrm{E}(Y|X)\)`, med `\(\epsilon\)` som et stokastisk feilledd
- *Grafisk illustrasjon av enkel regresjon (√©n forklaringsvariabel)*. `\(\epsilon\)` er stokastisk. Linjen `\(\textrm{E}(Y|X)\)`, bestemt av `\(\beta_0\)` og `\(\beta_1\)` er derfor ukjent:

&lt;img src="forelesning10_reg1.jpg" width="500" style="display: block; margin: auto;" /&gt;

---

## Minste kvadraters metode

* Regresjonen `\(\textrm{E}(Y|X)\)` bestemmes ved √• legge ‚Äúlinjen‚Äù slik at kvadratsummen av forskjellene mellom observert verdi og beregnet forventingsverdi for de n observasjonen er minst mulig, dvs. ved √• minimere ‚ÄúSum of Squared Errors‚Äù mhp. `\(\beta_0\)` og `\(\beta_1\)`:
`\begin{align*}
\textrm{SSE}(\beta_0,\beta_1) &amp;= \sum_{i=1}^n\left(Y_i - \widehat Y_i\right)^2 = \sum_{i=1}^n \left(Y_i - [\beta_0 + \beta_1X_i]\right)^2 \\
&amp;= \sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1X_i\right)^2
\end{align*}`
* Dette kan gj√∏res ved √• derivere uttrykket mhp. `\(\beta_0\)` og `\(\beta_1\)` hver for seg og sette de deriverte lik null. Vi f√•r da 2 ligninger til bestemmelse av de to estimatene

---

## Minste kvadraters metode

Legger linjen slik at summen av de hvite arealene minimeres

&lt;img src="forelesning10_reg3.jpg" width="400" style="display: block; margin: auto;" /&gt;

---

## Minste kvadraters metode

* L√∏sningen p√• minimeringsproblemet er
`\begin{align*}
\widehat \beta_1 &amp;= \frac{\sum_{i=1}^n \left(X_i - \overline X\right)(Y_i - \overline Y)}{\sum_{i=1}^n\left(X_i - \overline X\right)^2} = \frac{\textrm{Cov}(X,Y)}{\textrm{Var}(X)} \\
\widehat \beta_0 &amp;= \overline Y - \widehat\beta_1\overline X
\end{align*}`
* Dette regner vi vanligvis ikke p√• for h√•nd! Vi legger dataene inn i R eller tilsvarende programmer og lar datamaskinen gj√∏re beregningene!

---

## La oss repetere forutsetningene for OLS

i. `\(\textrm{E}{(\epsilon) = 0}\)`
  - Tiln√¶rmet trivielt n√•r modellen har et konstantledd

ii. `\({\sigma_{\epsilon}}\)` er konstant for alle `\({X}\)`
  - Brudd p√• denne forutsetningen kalles heteroskedastisitet

iii. `\({\epsilon_i}\)` er uavhengig av `\({\epsilon_j}\)` for alle `\(i\)` og `\(j\)`
  - Et vanlig brudd p√• denne forutsetningen er autokorrelasjon

iv. Dersom `\(X\)`-variabelen ikke er deterministisk, men stokastisk, m√• den v√¶re uavhengig av `\(\epsilon\)`
  - Brudd p√• denne forutsetningen er sv√¶rt alvorlig i mange sammenhenger, fordi det skaper forventningsskjeve estimater. Avhengighet mellom `\(X\)` og `\(\epsilon\)` kalles endogenitet. Det er et sentralt tema i √∏konometri, men er ikke behandlet i boken.


---

## En forutsetning til 

v. Tilleggsforrutsetning: Normalitet
  - Vanligvis er vi ikke forn√∏yde med bare √• beregne `\(\widehat\beta_0\)` og `\(\widehat\beta_1\)` (som er punktestimater for de ukjente parameterne `\(\beta_0\)` og `\(\beta_1\)`)
  - Vi √∏nsker ogs√• √• kunne teste hypoteser og lage konfidensintervaller basert p√• modellen
  - *Da m√• vi gj√∏re en tilleggsantagelse om at* `\(\epsilon\)` *er normalfordelt*

---

## Aktuelle problemstillinger for oss

Den estimerte regresjonsligningen 

`$$\widehat Y = \widehat\beta_0 + \widehat\beta_1 X$$` 

kan brukes til √•

I. Teste forklaringsvariabelens betydning

II. Estimere `\(\textrm{E}(Y)\)` ( `\(\widehat{Y}\)` med konfidensintervall ) for gitte `\(X\)`-verdier
  - Estimert `\(\textrm{E}(Y)\)` er en usikker st√∏rrelse fordi `\(\beta\)`'ene er usikre
  
III. Predikere ny `\(Y\)` ( `\(\widehat{Y}\)` med prediksjonsintervall ) for gitte `\(X\)`-verdier
  - Faktiske nye observasjoner av `\(Y\)` vil avvike fra estimert forventet verdi av to grunner:
    - Avvik mellom estimert og sann `\(\textrm{E}(Y)\)` (usikre `\(\beta\)`'er som over)
    - De stokastiske feilleddene `\(\epsilon\)`
    
IV. Vurdere om den line√¶re regresjonsmodellen passer til datasettet (diagnose).

---

## I. Teste hypoteser om stigningstallet

Hvis det er tilstrekkelig mange observasjoner og det ikke er noen sammenheng mellom `\(X\)` og `\(Y\)` vil vi estimere en flat regresjonslinje.

&lt;img src="regresjon-slides_files/figure-html/unnamed-chunk-3-1.png" width="720" /&gt;

---

## I. Teste  hypoteser om stigningstallet

* Med et begrenset antall observasjoner vil vi kunne f√• tilfeldige avvik fra null selv om det ikke er noen sann sammenheng mellom `\(X\)` og `\(Y\)`
* Vi har derfor behov for √• kunne teste hypoteser om st√∏rrelsen p√• de sanne koeffisientene ( `\(\beta\)`‚Äôene ) med utgangspunkt i de estimerte verdiene ( `\(\widehat\beta\)`‚Äôene )
* Vi √∏nsker vanligvis √• teste nullhypotesen
`$$H_0: \beta_1 = 0 \,\,\, \textrm{ mot } H_A:\beta_1\neq 0$$`
* En egnet testobservator er 
`$$T = \frac{\widehat \beta_1 - \textrm{E}(\widehat \beta_1)}{S(\widehat\beta_1)} = \frac{\widehat\beta_1 - \beta_1}{S(\widehat\beta_1)} = \frac{\widehat\beta_1}{S(\widehat\beta_1)}$$`
* Testobservatoren er `\(t\)`-fordelt med `\(n-2\)` frihetsgrader under `\(H_0\)`

---

## I. Teste  hypoteser om stigningstallet

* For √• utf√∏re testen trenger man standardavviket til `\(\widehat\beta_1\)`
* Dette er
`$$S(\widehat\beta_1) = S_{\widehat\beta_1} = \frac{S_{\epsilon}}{\sqrt{(n-1)S_X^2}}$$`
* Vi ser at standardavviket
    + √∏ker med variansen til `\(\epsilon\)`
    + avtar med antall observasjoner, `\(n\)`
    + avtar med variasjonen til forklaringsvariabelen, `\(X\)`
* Regresjonsprogrammer oppgir typisk T-verdi for hver estimert koeffisient, og tilh√∏rende P-verdi for tosidig test (Hvis ensidig test er det relevante m√• oppgitt P-verdi halveres) 
* En kan ogs√• bruke T-test til √• teste mot andre verdier enn null

---

## Konfidensintervall for `\(\beta\)`-estimatene

* N√•r vi har estimert en effekt som er signifikant forskjellig fra null vil vi v√¶re interessert i √• beregne et konfidensintervall for st√∏rrelsen p√• effekten
* Det er enkelt n√•r vi kjenner standardavviket til koeffisienten
`$$\beta_1 \in \left[\widehat\beta_1 - k\cdot S\left(\widehat\beta_1\right), \widehat\beta_1 + k\cdot S\left(\widehat\beta_1\right)\right]$$`
der `\(k = t_{1-\alpha/2}\)` hentes fra `\(T\)`-tabell med `\(n-2\)` frihetsgrader
* Med 95% prediksjonsintervall og stor `\(n\)` blir `\(k=1.96\)`
* I bileksempelet er `\(n = 100\)`, og `\(t_{0.975, 98} = 1.98\)`, s√• et 95% konfidensintervall for `\(\beta_1\)` er

`$$\big[0.067 \pm 1.98\cdot0.005\big] = \big[0.0571, 0.0769\big]$$`

- Vi kan se at `\(\beta_1\)` ikke er signifikant forskjellig fra 0 p√• 5% signifikansniv√•, ved at et 95% konfidensintervall ikke inneholder null.

---

## Aktuelle problemstillinger for oss

Den estimerte regresjonsligningen 

`$$\widehat Y = \widehat\beta_0 + \widehat\beta_1 X$$` 

kan brukes til √•

I. Teste forklaringsvariabelens betydning

II. Estimere `\(\textrm{E}(Y)\)` ( `\(\widehat{Y}\)` med konfidensintervall ) for gitte `\(X\)`-verdier
  - Estimert `\(\textrm{E}(Y)\)` er en usikker st√∏rrelse fordi `\(\beta\)`'ene er usikre
  
III. Predikere ny `\(Y\)` ( `\(\widehat{Y}\)` med prediksjonsintervall ) for gitte `\(X\)`-verdier
  - Faktiske nye observasjoner av `\(Y\)` vil avvike fra estimert forventet verdi av to grunner:
    - Avvik mellom estimert og sann `\(\textrm{E}(Y)\)` (usikre `\(\beta\)`'er som over)
    - De stokastiske feilleddene `\(\epsilon\)`
    
IV. Vurdere om den line√¶re regresjonsmodellen passer til datasettet (diagnose).


---

## II. Konfidensintervall for `\(\textrm{E}(Y)\)`

* Et 95% konfidensintervall for `\(\textrm{E}(Y|X)\)` konstrueres som 
`$$\widehat Y \pm t_{\alpha/2, n-2}\cdot S\left(\widehat Y\right)$$`
der `\(t_{\alpha/2, n-2}\)` er kritisk grense fra en `\(t\)`-fordeling.
* Formelen for `\(S\left(\widehat Y\right)\)` er
`$$S\left(\widehat Y\right) = S_{\epsilon}\sqrt{\frac{1}{n} + \frac{(X-\overline X)^2}{(n-1)S_X^2}}$$`
der de to leddene har med usikkerhet i hhv. konstantleddet og stigningstallet √• gj√∏re

---

## II. Konfidensintervall for `\(\textrm{E}(Y)\)`

* Dette ser vi enklere ved √• skrive uttrykket om til
`$$S\left(\widehat Y\right) = \sqrt{\frac{S_{\epsilon}^2}{n} + \left[S(\widehat\beta_1)\right]^2\cdot\left(X-\overline X\right)^2}$$`
* Merk at f√∏rste leddet er identisk med uttrykket for variansen til et gjennomsnitt i m√•lemodellen, og at usikkerheten til det estimerte stigningstallet ( `\(\widehat\beta_1\)` ) gj√∏r at konfidensintervallet til `\(\widehat Y\)` vider seg ut jo lengre `\(X\)`-verdiene ligger fra sitt gjennomsnitt.
* Vi kan tenke p√• `\(S(\widehat Y)\)` som usikkerheten til v√•rt estimat for gjennomsnittet av mange `\(Y\)`-verdier for gitt `\(X\)`

---

## Aktuelle problemstillinger for oss

Den estimerte regresjonsligningen 

`$$\widehat Y = \widehat\beta_0 + \widehat\beta_1 X$$` 

kan brukes til √•

I. Teste forklaringsvariabelens betydning

II. Estimere `\(\textrm{E}(Y)\)` ( `\(\widehat{Y}\)` med konfidensintervall ) for gitte `\(X\)`-verdier
  - Estimert `\(\textrm{E}(Y)\)` er en usikker st√∏rrelse fordi `\(\beta\)`'ene er usikre
  
III. Predikere ny `\(Y\)` ( `\(\widehat{Y}\)` med prediksjonsintervall ) for gitte `\(X\)`-verdier
  - Faktiske nye observasjoner av `\(Y\)` vil avvike fra estimert forventet verdi av to grunner:
    - Avvik mellom estimert og sann `\(\textrm{E}(Y)\)` (usikre `\(\beta\)`'er som over)
    - De stokastiske feilleddene `\(\epsilon\)`
    
IV. Vurdere om den line√¶re regresjonsmodellen passer til datasettet (diagnose).

---

## III. Prediksjonsintervall for ny `\(Y\)`

* Et intervall som med gitt sannsynlighet fanger opp enkeltst√•ende nye verdier av `\(Y\)` for gitte verdier av `\(X\)` kalles prediksjonsintervall
* Dette vil v√¶re videre enn konfidensintervallet til `\(\textrm{E}(Y)\)`, siden det omfatter b√•de usikkerhet i estimeringen av `\(\textrm{E}(Y)\)` og at realisert `\(Y\)` varierer omkring `\(\textrm{E}(Y)\)`
* For √• beregne et prediksjonsintervall trenger vi √• kjenne standardavviket til avviket mellom en ny `\(Y\)`-verdi og dens estimerte forventning: `\(S\left(Y-\widehat Y\right)\)`

---

## III. Prediksjonsintervall for ny `\(Y\)`

* Generelt har vi at
`\begin{align*}
S\left(Y-\widehat Y\right) &amp;= \sqrt{S_{\epsilon}^2 + S\left(\widehat Y\right)^2} \\
&amp;= S_{\epsilon}\sqrt{1 + \frac{1}{n} + \frac{\left(X - \overline X\right)^2}{(n-1)S_X^2}}
\end{align*}`
* Et 95% konfidensintervall til `\(Y\)` er for stor `\(n\)`
`$$\widehat Y \pm 1.96\cdot S\left(Y-\widehat Y\right)$$`

---

## En liten oppsummering av standardavvikene



|                                   |                                                     |
|-----------------------------------|-----------------------------------------------------|
| `\(S_{\epsilon}\)`                  | Estimert standardavvik til feilleddet               |
| `\(S\left(\widehat\beta_1\right)\)` | Estimert standardavvik til stigningstallet          |
|                                   | (regresjonskoeffisienten til `\(X\)`)                 |
| `\(S\left(\widehat Y\right)\)`      | Estimert standardavvik til estimert forventet `\(Y\)` |
| `\(S\left(Y-\widehat Y\right)\)`    | Estimert standardavvik til prediksjonsfeilen        |



Merk: dersom man bare skriver `\(S\)` mener man vanligvis `\(S_{\epsilon}\)`

---

## Aktuelle problemstillinger for oss

Den estimerte regresjonsligningen 

`$$\widehat Y = \widehat\beta_0 + \widehat\beta_1 X$$` 

kan brukes til √•

I. Teste forklaringsvariabelens betydning

II. Estimere `\(\textrm{E}(Y)\)` ( `\(\widehat{Y}\)` med konfidensintervall ) for gitte `\(X\)`-verdier
  - Estimert `\(\textrm{E}(Y)\)` er en usikker st√∏rrelse fordi `\(\beta\)`'ene er usikre
  
III. Predikere ny `\(Y\)` ( `\(\widehat{Y}\)` med prediksjonsintervall ) for gitte `\(X\)`-verdier
  - Faktiske nye observasjoner av `\(Y\)` vil avvike fra estimert forventet verdi av to grunner:
    - Avvik mellom estimert og sann `\(\textrm{E}(Y)\)` (usikre `\(\beta\)`'er som over)
    - De stokastiske feilleddene `\(\epsilon\)`
    
IV. Vurdere om den line√¶re regresjonsmodellen passer til datasettet (diagnose).

---

## Aktuelle problemstillinger for oss

Den estimerte regresjonsligningen 

`$$\widehat Y = \widehat\beta_0 + \widehat\beta_1 X$$` 

kan brukes til √•

I. Teste forklaringsvariabelens betydning

II. Estimere `\(\textrm{E}(Y)\)` ( `\(\widehat{Y}\)` med konfidensintervall ) for gitte `\(X\)`-verdier
  - Estimert `\(\textrm{E}(Y)\)` er en usikker st√∏rrelse fordi `\(\beta\)`'ene er usikre
  
III. Predikere ny `\(Y\)` ( `\(\widehat{Y}\)` med prediksjonsintervall ) for gitte `\(X\)`-verdier
  - Faktiske nye observasjoner av `\(Y\)` vil avvike fra estimert forventet verdi av to grunner:
    - Avvik mellom estimert og sann `\(\textrm{E}(Y)\)` (usikre `\(\beta\)`'er som over)
    - De stokastiske feilleddene `\(\epsilon\)`
    
IV. Vurdere om den line√¶re regresjonsmodellen passer til datasettet (diagnose).

---

## Diagnose: Er modellen god?

&lt;img src="regresjon-slides_files/figure-html/unnamed-chunk-4-1.png" width="648" /&gt;


&lt;img src="regresjon-slides_files/figure-html/unnamed-chunk-5-1.png" width="648" /&gt;

---

## Diagnose: Er modellen god?

De "beregnede" verdiene
`$$\widehat Y = \widehat\beta_0 + \widehat\beta_1X$$`
ligger p√• regresjonslinjen, og forskjellene
`$$e_i = Y_i - \widehat Y_i$$`
kalles *residualer* (eller restledd eller feilledd). 



*Residualene* `\(e_i\)` *kan betraktes som estimater for de sanne feilleddene* `\(\epsilon_i\)`. *De kan brukes til √•*

- Teste hvor godt regresjonslinjen f√∏yer dataene
- Teste forutsetningene vi har gjort om `\(\epsilon\)`

---

## Diagnose: Er modellen god?

&lt;img src="tabell.png" width="800" style="display: block; margin: auto;" /&gt;

---

## Standardavviket til residualene, `\(S_{\epsilon}\)`
* Hvis modellen er god b√∏r observasjonene ‚Äúi gjennomsnitt‚Äù ligge n√¶r regresjonslinjen
* Et mulig m√•l for dette er standardavviket til feilleddet, `\(\sigma_{\epsilon}\)`
* `\(\sigma_{\epsilon}\)` er en ukjent parameter, men vi kan estimere den basert p√• de beregnede residualene `\(e_i\)`:
`$$S_{\epsilon} = \sqrt{\frac{\sum_{i=1}^n\left(Y_i - \widehat Y_i\right)^2}{n-2}} = \sqrt{\frac{\sum_{i=1}^ne_i^2}{n-2}} = \sqrt{\frac{\textrm{SSE}}{n-2}}$$`
* **Hva er** `\({S_{\epsilon}}\)` **i bileksempelet?** 

---

##  Forklaringsgraden, `\(R^2\)`

* Et annet og enda mer brukt m√•l p√• hvor godt modellen passer til dataene er forklaringsgraden, `\(R^2\)`, ogs√• kalt determinasjonskoeffisienten 
* `\(R^2\)` er et m√•l p√• hvor stor andel av variasjonen i `\(Y\)` modellen forklarer
* Et m√•l p√• den totale variasjonen i Y er ‚ÄúTotal Sum of Squares‚Äù
`$$\textrm{SS(Total)} = \sum_{i=1}^n\left(Y_i - \overline Y\right)^2$$`

---

## Forklaringsgraden, `\(R^2\)`
* Den variasjonen i `\(Y\)` som modellen *ikke* kan forklare er "sum of Sqared Errors"
`$$\textrm{SSE} = \sum_{i=1}^n\left(Y_i - \widehat Y_i\right)^2 = \sum_{i=1}^ne_i^2$$`
* Forklaringsgraden til modellen kan uttrykkes
`$$R^2 = \frac{\textrm{SS(Total)} - \textrm{SSE}}{\textrm{SS(Total)}} = \frac{\textrm{forklart variasjon}}{\textrm{total variasjon}}$$`
* En kan vise at `\(R^2\)` er kvadratet av korrelasjonskoeffisienten, `\(r\)`, mellom `\(X\)` og `\(Y\)`, derav navnet
* **Hva er** `\({R^2}\)` **i bileksempelet?** 

---

## Heteroskedastisitet

- Feilleddene skal ha lik varians, uavhengig av `\(X\)`
- I s√• fall forventer vi at et plott av feilleddene mot `\(X\)`-verdiene, evt. de predikerte `\(Y\)`-verdiene, ikke skal vise noe spesielt m√∏nster
- Ikke sjelden kan variansen til feilleddene √∏ke med √∏kende verdier av `\(X\)` eller `\(\widehat Y\)`
- Dette kalles heteroskedastisitet
- **Da er ikke lenger minste kvadraters metode den ‚Äúbeste‚Äù estimatoren og den statistiske inferensen (hypotesetester, konfidens- og prediksjonsintervaller) er ikke lenger korrekt, men estimatene er fortsatt forventningsrette**

---

## Typisk bilde av heteroskedastisitet

&lt;img src="forelesning10_reg4.jpg" width="800" style="display: block; margin: auto;" /&gt;

---

## Tydelig bilde av heteroskedastisitet

Resultat mot investert kapital for 250 store vestlandsbedrifter

&lt;img src="forelesning10_reg5.jpg" width="600" style="display: block; margin: auto;" /&gt;

---

## Innbyrdes uavhengige feilledd

- Avhengighet mellom feilleddene til ulike observasjoner kan oppst√• av forskjellige grunner
- Den vanligste √•rsaken er autokorrelasjon
- Autokorrelasjon kan oppst√• i tidsseriedata og inneb√¶rer at feilleddet p√• tidspunkt `\(t\)` er  positivt eller negativt korrelert med feilleddet p√• tidspunkt `\(t+1\)`
- Lik konkjunktursituasjon i to p√•f√∏lgende perioder kan for eksempel gi positiv autokorrelasjon dersom v√•r modell ikke kontrollerer for konjunkturer 
- **Med autokorrelerte feilledd er ikke lenger minste kvadraters metode den ‚Äúbeste‚Äù estimatoren og inferens er ikke gyldig, men estimatene er fortsatt forventningsrette**

---

## Typisk bilde av tidsserie med negativ autokorrelasjon


&lt;img src="forelesning10_reg8.jpg" width="600" style="display: block; margin: auto;" /&gt;

---

## Typisk bilde av tidsserie med positiv autokorrelasjon

&lt;img src="forelesning10_reg7.jpg" width="600" style="display: block; margin: auto;" /&gt;

---

## Typisk bilde av tidsserie uten autokorrelasjon

&lt;img src="forelesning10_reg6.jpg" width="600" style="display: block; margin: auto;" /&gt;

---

## Eksempel fra eksamen V17

&lt;img src="regresjon-slides_files/figure-html/unnamed-chunk-12-1.png" width="576" style="display: block; margin: auto;" /&gt;

---

## Spesielt innflytelsesrike observasjoner 

- Vi √∏nsker √• se n√¶rmere p√• observasjoner som har stor innflytelse p√• hvor regresjonslinjen legges
- Dette vil typisk v√¶re avvikende observasjoner (‚Äúoutliers‚Äù)
- **Dersom slike observasjoner skyldes feil i dataene, eller er atypiske er de sv√¶rt skadelige for analysen**
- **Dersom det ikke er noe galt med disse observasjonene er de sv√¶rt verdifulle for analysen**
- Er du i tvil b√∏r du vurdere √• rapportere resultater med og uten slike observasjoner
- R kan plukke dem ut for oss



---

## Vi tar bort den ekstreme observasjonen

&lt;img src="regresjon-slides_files/figure-html/unnamed-chunk-13-1.png" width="576" style="display: block; margin: auto;" /&gt;

---

## Identifiserer innflytelsesrike observasjoner

- Cooks avstand (Cook's distance) er et mye brukt m√•l p√• en observasjons innflytelse.
- Cooks avstand til observasjon `\(i\)` er et m√•l p√• hvor mye regresjonslinjen forandrer seg dersom man fjerner observasjon `\(i\)`. (Vi g√•r ikke inn i detaljer her)
- Vi finner Cooks avstand (samt flere andre m√•l p√• innflytelse) ved hjelp av funksjonen `influence.measures()` i R.

---

## Oppsummering: Konsekvensen av brudd p√• forutsetningene som OLS bygger p√•

* Ikke konstant varians:
    + Vanlig minste kvadraters metode gir forventingsrette estimat, men det er ikke lenger den mest ‚Äúeffektive‚Äù estimatoren, og inferensen er ikke gyldig
* Ikke uavhengige feilledd:
    + Vanlig minste kvadraters metode gir forventingsrette estimat, men det er ikke lenger den mest ‚Äúeffektive‚Äù estimatoren, og inferensen er ikke gyldig
* Ikke normalfordelte feilledd:
    + Inferens er ikke gyldig i sm√• utvalg, dvs. at hypotesetester, konfidensintervaller etc. kan bli feil. Hvis avvikene fra normalitet ikke er for sterke g√•r inferens bra i store utvalg, og normalitet er heller ikke en forutsetning for at miste kvadraters metode skal v√¶re en ‚Äúeffektiv‚Äù estimator
* Ikke uavhengighet mellom forklaringsvariablene og feilleddet:
    + **F√•r forventningsskjeve estimater!!**
    + **Men, metoden kan fremdeles fungere for prediksjonsform√•l**

---

class: center, middle, inverse

# Del II: MULTIPPEL REGRESJON

---

## Multippel regresjon

Multippel regresjon er regresjon med flere forklaringsvariabler. Dengenerelle line√¶re regresjonsmodellen er

`$$\textrm{E}(Y|X) = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \cdots \beta_k X_k,$$`

der de `\(k\)` forklaringsvariablene antas line√¶rt uavhengige.

Gitt `\(n\)` sammenh√∏rende observasjonssett

`$$(Y_i, X_{i1}, X_{i2}, \ldots, X_{ik}), \qquad i = 1,2,\ldots,n,$$`

kan vi skrive 

`$$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_kX_{ik} + \epsilon_i, \qquad i = 1,2,\ldots,n.$$`

Koeffisientene estimeres fortsatt ved kjelp av minste kvadraters metode, dvs `\(\widehat \beta_0, \widehat\beta_1, \ldots, \widehat\beta_k\)` velges slik at de kvadrerte observerte residualene under minimeres:

`$$\textrm{SSE}(\beta_0, \beta_1, \ldots, \beta_k) = \sum_{i=1}^n\left(Y_i - \widehat Y\right)^2$$`

---

## Grafisk illustrasjon av regresjon med to forklaringsvariabler

&lt;img src="forelesning11_reg31.jpg" width="600" style="display: block; margin: auto;" /&gt;

---

## Multippel regresjon

* Alt vi har l√¶rt om den enkle regresjonsmodellen lar seg generalisere til en situasjon med flere variabler, men
    + formlene blir kompliserte og krever bruk av matrisenotasjon
    + antall frihetsgrader blir `\((n-k-1)\)`, der `\(k\)` er antall forklaringsvariabler
    + ‚Äúregresjonslinjen‚Äù blir et plan i tilfellet med to forklaringsvariabler og generelt et hyperplan som ikke lar seg illustrerer grafisk
Nye tema:

1. Justert `\(R^2\)`
2. Multikolinearitet
3. F-test

---

## I - Justert `\(R^2\)`

* `\(R^2\)` er det mest brukte (og misbrukte?) m√•let p√• forklaringskraft
* Jo flere forklaringsvariabler vi legger til modellen desto h√∏yere `\(R^2\)`
* `\(R^2\)` √∏ker selv om nye variabler ikke har reell forklaringskraft
    + med `\(k = (n-1)\)` vil all variasjon bli forklart dvs. `\(R^2=1\)`, idet vi da kan legge et ‚Äúplan‚Äù gjennom alle punktene.
    + jfr. figur med `\(n=2\)` og `\(k=1\)`
* En modell med flere forklarende variabler og h√∏yere `\(R^2\)` beh√∏ver ikke ha bedre prediksjonsevne, idet en da m√• estimere flere parametere som det hefter usikkerhet ved.
* **Forklaringsgraden til modellen b√∏r derfor korrigeres for antall forklaringsvariabler (frihetsgrader) dersom vi skal sammenligne modeller med ulikt antall forklaringsvariabler.**
* I regresjons-utskrifter oppgis det en justert `\(R^2\)` som tar hensyn til dette
    + Merk at vi kun kan sammenligne modeller med samme `\(Y\)`-variabel

---
    
## I - Justert `\(R^2\)`

* **Formelen for** `\({R^2}\)` **er**
`$$R^2 = \frac{\textrm{forklart variasjon}}{\textrm{total variasjon}} = \frac{\textrm{SS(Total)} - \textrm{SSE}}{\textrm{SS(Total)}} \\
= 1-\frac{\textrm{SSE}}{\textrm{SS(Total)}}$$`
- **Formelen for justert** `\({R^2}\)` **er**
`$$R^2_{\textrm{justert}} = 1-\frac{\textrm{MSE}}{\textrm{MS(Total)}} = 1-\frac{\textrm{SSE}/(n-k-1)}{\textrm{SS(Total)}/(n-1)}$$`
* **Men vi kan ikke bruke** `\({R^2}\)` **til √• sammenligne modeller med forskjellige responsvariabler!!**

---

## II - Multikolinearitet

* Multikollinearitet har vi n√•r noen av forklaringsvariablene samvarierer sterkt, dvs. at en regresjon av en forklaringsvariabel mot de √∏vrige forklaringsvariablene f√•r h√∏y `\(R^2\)`

---

## Perfekt multikolinearitet

* *Perfekt* multikollinearitet er et brudd p√• forutsetningen for minste kvadraters metode om at forklaringsvariablene skal v√¶re line√¶rt uavhengige
* Anta at du skal estimere
`$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon$$`
* Anta videre at i datasettet er alltid `\(x_2\)` lik `\(2x_1\)`
* Det er da √•penbart at man ikke kan f√• ut selvstendige estimat for b√•de `\(\beta_1\)` og `\(\beta_2\)`

---

## Tiln√¶rmet multikolinearitet

* Dersom `\(x_2\approx2x_1\)` har vi ikke formelt brudd p√• forutsetningene for miste kvadraters metode og kan regne ut selvstendige estimater for `\(\beta_1\)` og `\(\beta_2\)`
* Disse estimatene vil imidlerid bli sv√¶rt usikre. N√•r `\(x_2\)` og `\(2x_1\)` beveger seg nesten i takt er det vanskelig √• skille de to effektene fra hverandre.
    + Typisk vil `\(\beta_1\)` og `\(\beta_2\)` v√¶re ikke-signifikante hver for seg mens de sammen er sterkt signfikante (krever F-test, se senere)
    + Det reflekterer at vi kan f√• et presist estimat for `\(\beta_1+2\beta_2\)`, men vi vet ikke hvor mye hver av variablene bidrar med. 
    + Sm√• endringer i datamaterialet kan da gi store utslag i `\(\beta_1\)` og `\(\beta_2\)`, men p√• en slik m√•te at `\(\beta_1+2\beta_2\)` endres lite
    + For prediksjonsform√•l er dette uproblematisk hvis `\(x_1\)` og `\(x_2\)` alltid er sterkt korrelerte
    
---

## Multikolinearitet er ikke n√∏dvendigvis et problem

* I en generell multippel regresjon er
`$$S(\widehat\beta_j) = \frac{S_{\epsilon}}{\sqrt{(n-1)S_{X_j}^2}}\times\underbrace{\frac{1}{\sqrt{1-R_j^2}}}_{\text{Variance inflation} \atop\text{factor (VIF)}}$$`
der `\(R_j^2\)` er `\(R^2\)` fra en regresjon av `\(X_j\)` mot de andre forklaringsvariablene
* Parameterestimatet vil alts√• ha lav presisjon hvis 
    + `\(S_{\epsilon}\)` er h√∏y (mye st√∏y)
    + Vi har f√• observasjoner, `\(n\)`
    + Det er lite variasjon i `\(X_j\)` **eller**
    + `\(R_j^2\)` er h√∏y (multikolinearitet)
* Multikollinearitet er alts√• verken n√∏dvendig eller tilstrekkelig betingelse for lav presisjon 
* Vi kan bruke funksjonen `vif()` i R til √• regne ut VIF.

---

## Mulige l√∏sninger hvis multikolinearitet er et problem

* Det finnes ingen statistisk teknikk som kan l√∏se problemet
* Mulige ‚Äúl√∏sninger‚Äù er da
    + Tenke grundigere gjennom hvilke sp√∏rsm√•l vi kan svare p√• (har for eksempel ‚Äú `\(\beta_1+2\beta_2\)` ‚Äù en selvstendig tolkning?)
    + Fjerne en av de sterkt korrelerte variablene, hvis den ikke er viktig i modellen. Det vil redusere variansen til de gjenv√¶rende parametrene, i bytte mot at de blir skjeve/f√•r en annen tolkning.

---

## F-test i multippel regresjon

- Vi husker tilbake til problemet om multippel testing.
- Vi m√• tenke p√• det samme n√•r vi leser en regresjonstabell:
- Det at vi ikke forkaster hypotesene `\(\beta_1 = 0\)` og `\(\beta_2 = 0\)` hver for seg betyr **ikke** at vi ikke kan forkaste hypotesen `\(\beta_1 = \beta_2 = 0\)`.
- Til det trenger vi en egen test, og det er vanlig i regresjonsutskrifter  √• rapportere resultatet for testen
`$$\beta_1 = \beta_2 = \cdots = \beta_k = 0.$$`
- Dette er en `\(F\)`-test (vi g√•r ikke inn i detaljer).

---

## Tolking av parametrene i multippel regresjon

- **Koeffisienten til en gitt variabel angir *den forventede* endringen i Y for √©n enhet endring i denne X-verdien, gitt at alle de andre X-verdiene holdes uforandret**
* MEN HUSK: Dersom en inkludert forklaringsvariabel er korrelert med en relevant *utelatt* forklaringsvariabel, vil den inkluderte variabelen delvis plukke opp effekten av den utelatte variabelen
    + Den inkluderte variabelen er da endogen og koeffisienten kan ikke tolkes kausalt!
* **Statistisk signifikans versus √∏konomisk signifikans**
    + At en effekt er statistisk signifikant betyr ikke n√∏dvendigvis at den er viktig. Med nok data vil selv den mest ubetydelige gjennomsnittsforskjell fremst√• som statistisk signifikant 
    + Tilsvarende, en effekt kan godt v√¶re reell og viktig selv om den ikke framst√•r som statistisk signifikant. Problemet kan v√¶re h√∏y varians og for f√• observasjoner
* **Tolkning av konstantleddet**
    + Konstantleddet m√• tolkes med stor varsomhet dersom vi ikke har observasjoner i n√¶rheten av `\(X_i=0\)` for alle `\(i\)`
    + Gjelder b√•de multippel og enkel regresjon

---

class: center, middle, inverse

# Del III: MODELLBYGGING

---

## Eksempel fra eksamen V17

* Hvert √•r gjennomf√∏res nasjonale pr√∏ver i lesing, regning og engelsk, for 5., 8. og 9. trinn.
* Vi har data fra en slik pr√∏ve, der vi har registrert gjennomsnittlig score p√• lesepr√∏ven for hver enkelt kommune.
* Vi har i tillegg en del ekstra informasjon fra hver kommune.
* Kan vi forklare resultat p√• lesepr√∏ven med noen av de andre variablene?

---

## F-test

* En vanlig ¬´regresjonsfelle¬ª er √• pr√∏ve og feile helt til man f√•r en regresjonsmodell som *ser bra ut*.
* Men husk at enhver stjerne (eller mangel p√• stjerne) er resultatet av en statistisk test.
* `F-statistic` er testobservatoren i f√∏lgende test:

`\begin{align*}
&amp;H_0: \beta_1 = \cdots = \beta_k = 0 \\
&amp;H_A: \textrm{Minst en koeffisient er forskjellig fra null.}
\end{align*}`

* Observatoren er `\(F\)`-fordelt under nullhypotesen, og testen er n√¶rt beslektet med variansanalyse (som ikke i seg selv er pendum i MET4).

---

## F-test

* Kan brukes til √• teste signifikans av *grupper av variable*:


```r
reg_liten &lt;- lm(lesing ~ log(folketall), data = skoledata)
reg_stor  &lt;- lm(lesing ~ log(folketall) + 
                             driftsutgifter + 
                             nynorsk, data = skoledata) 

anova(reg_liten, reg_stor)
```

```{}
Analysis of Variance Table

Model 1: lesing ~ log(folketall)
Model 2: lesing ~ log(folketall) + driftsutgifter + nynorsk
  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
1    383 1989.7                           
2    381 1972.2  2    17.494 1.6898 0.1859
```

---

## Modellbygging

* Vi har ofte data p√• mange variable i et datasett, og stiller oss f√∏lgende sp√∏rsm√•l:
    + Hvilke variable skal v√¶re med?
    + Hvor mange variable skal v√¶re med?
    + Skal noen av variablene transformeres? Hvis ja, hvordan? Polynom, log, ...?
    + Hvilke kriterier skal vi bruke for √• vurdere de ulike modellene?
* Disse sp√∏rsm√•lene har ikke matematiske eller universelle svar. Vi kan l√¶re mye av √• pr√∏ve ut forskjellige varianter.
    + Men vi trenger noen strategier for √• angripe datasettet.

---

## Antall sider vs. antall poeng v/ MET4 V17

&lt;img src="forelesning12_page1.jpg" width="500" style="display: block; margin: auto;" /&gt;

---

## Antall sider vs. antall poeng v/ MET4 V17


```{}
Call:
lm(formula = poeng ~ pagelength, data = eks)

Residuals:
   Min     1Q Median     3Q    Max 
-49.16 -12.24   2.40  10.26  35.50 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  37.0576     3.8743   9.565  &lt; 2e-16 ***
pagelength    2.0967     0.2957   7.092 1.99e-11 ***
---
Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1

Residual standard error: 15.93 on 210 degrees of freedom
Multiple R-squared:  0.1932,	Adjusted R-squared:  0.1894 
F-statistic: 50.29 on 1 and 210 DF,  p-value: 1.988e-11
```

---

## Antall sider vs. antall poeng v/ MET4 V17

&lt;img src="forelesning12_page2.jpg" width="500" style="display: block; margin: auto;" /&gt;

---

## Antall sider vs. antall poeng v/ MET4 V17

```{}
Call:
lm(formula = poeng ~ pagelength + square, data = eks2)

Residuals:
    Min      1Q  Median      3Q     Max 
-46.853 -10.885   1.584  10.383  37.814 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   9.3918    10.3177   0.910  0.36373    
pagelength    6.6348     1.5995   4.148 4.88e-05 ***
square       -0.1711     0.0593  -2.885  0.00432 ** 
---
Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1

Residual standard error: 15.66 on 209 degrees of freedom
Multiple R-squared:  0.2241,	Adjusted R-squared:  0.2167 
F-statistic: 30.19 on 2 and 209 DF,  p-value: 3.044e-12
```

---

## Antall sider vs. antall poeng v/ MET4 V17

&lt;img src="forelesning12_page3.jpg" width="500" style="display: block; margin: auto;" /&gt;

---

## Polynomtransformasjoner

- Vi kan bruke line√¶r regresjon til √• tilpasse polynommodeller, f.eks.
`$$\textrm{E}(Y|X) = \beta_0 + \beta_1X + \beta_2X^2.$$`
- I praksis gj√∏r vi det ved √• legge til `\(X^2\)` som en ny kolonne, og bruke den som forklaringsvariabel p√• vanlig m√•te.
- Kan brukes til √• fange opp ikke-line√¶re sammenhenger.
    
.small[Eksempel: Forsikringsselskap √∏nsker √• modellere *forventet* skadeutbetaling for kundene sine slik at de kan beregne riktig pris p√• forsikrking. Ved bilforsikring er *alder* en viktig faktor, der unge sj√•f√∏rer gj√∏r mer skade, men at forventet skadeutbetaling g√•r ned med alderen. En fin oppgave for line√¶r regresjon! Problemet er bare at skadefrekvensen g√•r litt opp for eldre igjen. Det kan enkelt fanges opp med et svakt positivt andregradsledd for alder i regresjonen.]

- **Gir ikke dette mulikolinearitet?** Nei, fordi det handler om *line√¶r* avhengighet mellom forklaringsvariablene; `\(X\)` og `\(X^2\)` er ikke line√¶rt avhengige.
- **Hva med fortolkningen?** Vi kan *ikke* lenger tolke `\(\beta_1\)` som "en enhets √∏kning i `\(X\)` henger sammen med `\(\beta_1\)` enhets √∏kning i `\(Y\)`".
- **Forsiktig!!** Polynomer, spesielt n√•r vi √∏ker graden, kan gi veldig store utslag. Bruk med forsiktighet, og v√¶r ekstra forsiktig med √• ekstrapolere.

---

## Funksjonsform og transformasjoner

* Heteroskedastisitet, ikke-normalfordelte feilledd, avvikende observasjoner og andre problemer kan v√¶re en konsekvens av feil funksjonsform.
* Minste kvadraters metode kan brukes p√• en langt videre klasse av modeller enn den line√¶re og polynomiske som vi har sett s√• langt, f.eks

`$$\textrm{E}(Y|X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3\frac{\sqrt{X_1}}{X_2} + \beta_4\cos(X_1) + \beta_5\textrm{e}^{X_1X_2^3}$$` 

* Mange funksjoner som er *ikke-line√¶re i parametrene* kan transformeres til uttrykk som er *line√¶re i parametrene*.
    + Alle modeller som er line√¶re i parametrene kan estimeres med minste kvadraters metode.
    + En del standard transformasjoner er derfor nyttig √• kjenne til.
    + En annen interessant retning (som ikke vi skal g√•) er *ikke-line√¶r* regresjon. Der trenger vi ikke bry oss om funksjonsform i hele tatt.

---
    
## Dummyvariabler (ogs√• kalt indikatorvariabler)

* En dummyvariabel er en variabel som er enten null eller √©n, og brukes vanligvis til √• skille ut ulike kategorier som kvinner, ulike yrkesgrupper osv.
* Dummyvariabler har mange anvendelser, men brukes s√¶rlig til √• ‚Äúutvide‚Äù funksjonsformen slik at ulike grupper kan ulikt gjennomsnitt og/eller ulik respons
    + Vi skal her ta for oss dummyvariabler p√• h√∏yresiden, men man kan ogs√• ha en dummyvariabel som venstresidevariabel. Dette behandles under temaet kategorisk (logistisk) regresjon
    
---
    
## Ulikt konstantledd

* Hvis datasettet best√•r av observasjoner fra to grupper som kan forventes √• ha ulikt konstantledd, kan dette modelleres ved √• inkludere en dummyvariabel for den ene gruppen, for eksempel
`$$\ln W = \beta_0 + \beta_1D_{\textrm{kvinne}} + \beta_2\textrm{edu} + \epsilon$$`
* For hvert utdanningsniv√• tillater vi n√• kvinner √• ha en annen gjennomsnittsl√∏nn enn menn. Vi kan teste denne hypotesen ved √• se om `\(\beta_1\)` er forskjellig fra null
* Merk at for √• unng√• perfekt kollinearitet (*dummy variable trap*) m√• da √©n n√¶ring utelates, jfr. at vi ikke har dummy b√•de for menn og kvinner. Den utelatte kategorien ‚Äúfanges opp‚Äù av `\(\beta_0\)` og de andre n√¶ringene m√•les i forhold til denne.

---

## Ulikt konstantledd

&lt;img src="forelesning12_mb5.jpg" width="500" style="display: block; margin: auto;" /&gt;

---

## Ulikt konstantledd


- Spesifikasjonen kan lett utvides til en situasjon med mange kategorier, for eksempel testing av l√∏nnsforskjeller mellom n√¶ringer:

`$$\ln W = \beta_0 + \sum_{i=1}^{n-1}\beta_iD_i^{\textrm{n√¶ring}} + \beta_n\textrm{edu} + \epsilon$$`

&lt;img src="forelesning12_mb6.jpg" width="500" style="display: block; margin: auto;" /&gt;

---

## Ulikt konstantledd

- Merk at for √• unng√• perfekt kollinearitet (*dummy variable trap*) m√• da √©n n√¶ring utelates, jfr. at vi ikke har dummy b√•de for menn og kvinner. Den utelatte kategorien ‚Äúfanges opp‚Äù av `\(\beta_0\)` og de andre n√¶ringene m√•les i forhold til denne.

---

## Ulike stigningstall

* Dummyvariabler kan ogs√• brukes til √• la to grupper ha ulikt stigningstall
* Hvis vi vil teste om menn og kvinner har ulik avkastning p√• utdanning kan vi bruke spesifikasjonen

`$$\ln W = \beta_0 + \beta_1\textrm{edu} + \beta_2\textrm{edu}\cdot D_{\textrm{kvinne}} + \epsilon$$`

&lt;img src="forelesning12_mb7.jpg" width="400" style="display: block; margin: auto;" /&gt;

---

## Ulikt konstantledd *og* ulikt stigningstall

- Anta at kvinner har h√∏yere avkastning p√• utdanning p√• enn menn som vist i figuren, men at de samtidig lavere gjennomsnittsl√∏nn for gitt utdanning. Da vil vi neppe f√• estimert positiv `\(\beta_2\)`. Derfor b√∏r vi tillate b√•de konstantleddet og stigningstallet √• v√¶re forskjellig, dvs. estimere:

`$$\ln W = \beta_0 + \beta_1D_{\textrm{kvinne}} + \beta_2\textrm{edu} + \beta_3\textrm{edu}\cdot D_{\textrm{kvinne}} + \epsilon$$`

&lt;img src="forelesning12_mb8.jpg" width="400" style="display: block; margin: auto;" /&gt;

---

## Variabelutvelgelse og faremomenter

* √ònsker en enkel modell som f√•r fram de viktigste trekkene ved virkeligheten
* Ideelt sett b√∏r modellutformingen ha et teoretisk fundament, men teorien strekker sjelden til i praksis, og et element av pr√∏ving og feiling er vanskelig √• unng√•
* To tiln√¶rminger:
    + Begynn enkelt og legg til signifikante variabler (forward inclusion)
    + Begynn med rik modell og fjern ikke-signifikante variabler som ikke har et sterkt teoretisk fundament (backward exclusion)
    + ‚ÄúFull stepwise‚Äù: Automatisk metode basert p√• F-tester (Ch. 18.5)
* Ved √• pr√∏ve og feile risikerer en √• konstruere en modell som f√∏yer seg etter de tilfeldige feilleddene i utvalget og som ser god ut uten √• fange opp noe allmenngyldig
* Ved √• inkludere mange variabler risiker en √• f√• d√•rligere prediksjoner fordi prediksjonene da er basert p√• mange usikre koeffisienter
* üíÄ  üíÄ **DOKUMENTER!** üíÄ  üíÄ
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"slideNumberFormat": " "
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
